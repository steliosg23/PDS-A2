{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steliosg23/PDS-A2/blob/main/Finetuned_PubMedBERT%2BNN_BenchmarksPDS_FHD_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6e4pRWUUGFk"
      },
      "source": [
        "## Assignement 2\n",
        "### Food Hazard Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9of3N4dpUGF-"
      },
      "source": [
        "# Benchmarks - Advanced Model: PubMedBERT\n",
        "\n",
        "In this task, we aim to classify food safety-related incidents based on two distinct types of input data: short texts (title) and long texts (text).\n",
        "\n",
        "Using Advanced Model: PubMedBERT  \n",
        "\n",
        "\n",
        "For each of these input types, we perform the following two subtasks:\n",
        "\n",
        "**Subtasks (Performed Separately for  title and text):**\n",
        "\n",
        "**Subtask 1:**\n",
        "\n",
        "- Classify hazard-category (general hazard type).\n",
        "\n",
        "- Classify product-category (general product type).\n",
        "\n",
        "**Subtask 2:**\n",
        "\n",
        "- Classify hazard (specific hazard).\n",
        "- Classify product (specific product).\n",
        "\n",
        "We use all features (year, month, day, country, and the text feature) as input.\n",
        "\n",
        "Thus, we treat title and text as two distinct data sources, with each undergoing its own preprocessing, model training, and evaluation for all four targets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the file on Google Drive\n",
        "train_path = '/content/drive/MyDrive/Data/incidents_train.csv'\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(train_path)\n",
        "df = df.drop(columns=['Unnamed: 0'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4g33xroGg4P",
        "outputId": "d493ae12-1ec9-4706-d72b-3a12e27e8481"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n",
        "\n",
        "# Hyperparameters configuration\n",
        "config = {\n",
        "    'max_len': 128,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 2e-5,\n",
        "    'epochs': 5,\n",
        "    'model_name': \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "}\n",
        "\n",
        "# Set device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Custom Dataset for Text Data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Function to clean text (title or text) and remove stopwords\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Load tokenizer for Microsoft PubMedBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "df['title'] = df['title'].apply(clean_text)\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Define relevant features and targets\n",
        "features = ['year', 'month', 'day', 'country']\n",
        "targets_subtask1 = ['hazard-category','product-category']\n",
        "targets_subtask2 = ['hazard','product']\n",
        "\n",
        "# Encode target labels to numeric values\n",
        "label_encoders = {}\n",
        "for target in targets_subtask1 + targets_subtask2:\n",
        "    le = LabelEncoder()\n",
        "    df[target] = le.fit_transform(df[target])\n",
        "    label_encoders[target] = le\n",
        "\n",
        "# Prepare data for both title and text\n",
        "def prepare_data(text_column):\n",
        "    X = df[features + [text_column]]\n",
        "    y_subtask1 = df[targets_subtask1]\n",
        "    y_subtask2 = df[targets_subtask2]\n",
        "\n",
        "    data_splits = {}\n",
        "    for target in targets_subtask1 + targets_subtask2:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, df[target], test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Reset indices to ensure matching\n",
        "        X_train = X_train.reset_index(drop=True)\n",
        "        y_train = y_train.reset_index(drop=True)\n",
        "        X_test = X_test.reset_index(drop=True)\n",
        "        y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "        data_splits[target] = (X_train, X_test, y_train, y_test)\n",
        "\n",
        "    return data_splits\n",
        "\n",
        "# Prepare data for title and text\n",
        "title_splits = prepare_data('title')\n",
        "text_splits = prepare_data('text')\n",
        "\n",
        "# Custom BERT model with additional layers\n",
        "class CustomBertModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(CustomBertModel, self).__init__()\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).bert\n",
        "        # Add extra layers after BERT for enhancement\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(512, 256)  # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(256, num_labels)  # Final layer for classification\n",
        "        self.dropout = nn.Dropout(0.3)  # Dropout layer to reduce overfitting\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # BERT outputs\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # Get the pooled output (embedding of [CLS] token)\n",
        "\n",
        "        # Pass through extra layers\n",
        "        x = self.dropout(self.fc1(pooled_output))\n",
        "        x = self.dropout(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Function to train and evaluate neural network models\n",
        "def train_and_evaluate_nn(data_splits, targets, model_type='title'):\n",
        "    f1_scores = []  # List to store F1 scores for each task\n",
        "\n",
        "    for target in targets:\n",
        "        print(f\"\\nStarting training for task: {target}\")  # Print task message\n",
        "\n",
        "        X_train, X_test, y_train, y_test = data_splits[target]\n",
        "\n",
        "        # Prepare text data using the tokenizer\n",
        "        if model_type == 'title':\n",
        "            texts_train = X_train['title'].values\n",
        "            texts_test = X_test['title'].values\n",
        "        else:\n",
        "            texts_train = X_train['text'].values\n",
        "            texts_test = X_test['text'].values\n",
        "\n",
        "        # Create DataLoader for training and testing\n",
        "        train_dataset = TextDataset(texts_train, y_train, tokenizer, config['max_len'])\n",
        "        test_dataset = TextDataset(texts_test, y_test, tokenizer, config['max_len'])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "        # Model setup\n",
        "        num_labels = len(label_encoders[target].classes_)\n",
        "        model = CustomBertModel(model_name=config['model_name'], num_labels=num_labels).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training process\n",
        "        model.train()\n",
        "        for epoch in range(config['epochs']):\n",
        "            print(f\"Epoch {epoch+1}/{config['epochs']} - Training: {target}\")\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", total=len(train_loader), leave=True)\n",
        "            for batch in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluation process\n",
        "        print(f\"Evaluating model for task: {target}\")\n",
        "        model.eval()\n",
        "        y_preds = []\n",
        "        y_true = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Evaluating\", total=len(test_loader), leave=True):\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                _, preds = torch.max(outputs, dim=1)\n",
        "                y_preds.extend(preds.cpu().numpy())\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Decode labels back to original categories using the label encoder\n",
        "        decoded_preds = label_encoders[target].inverse_transform(y_preds)\n",
        "        decoded_true = label_encoders[target].inverse_transform(y_true)\n",
        "\n",
        "        # Calculate F1 score for the task\n",
        "        f1 = f1_score(decoded_true, decoded_preds, average='weighted')\n",
        "        f1_scores.append(f1)\n",
        "        print(f\"F1-Score for {target}: {f1}\")\n",
        "\n",
        "        # Print classification report\n",
        "        print(f\"Classification Report for {target}:\\n\")\n",
        "        print(classification_report(decoded_true, decoded_preds, zero_division=0))\n",
        "\n",
        "    return f1_scores  # Return the list of F1 scores for plotting\n",
        "\n",
        "# Train and evaluate for both title and text\n",
        "print(\"\\nTraining and Evaluating for Title Tasks:\")\n",
        "title_f1_scores = train_and_evaluate_nn(title_splits, targets_subtask1 + targets_subtask2, model_type='title')\n",
        "\n",
        "print(\"\\nTraining and Evaluating for Text Tasks:\")\n",
        "text_f1_scores = train_and_evaluate_nn(text_splits, targets_subtask1 + targets_subtask2, model_type='text')\n",
        "\n",
        "# Create DataFrames for F1 scores for title and text\n",
        "f1_scores_title_df = pd.DataFrame({\n",
        "    'Task': targets_subtask1 + targets_subtask2,\n",
        "    'F1-Score': title_f1_scores\n",
        "})\n",
        "\n",
        "f1_scores_text_df = pd.DataFrame({\n",
        "    'Task': targets_subtask1 + targets_subtask2,\n",
        "    'F1-Score': text_f1_scores\n",
        "})\n",
        "\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "index_title = range(len(f1_scores_title_df))  # Position for Title-Focused\n",
        "index_text = [i + len(f1_scores_title_df) for i in range(len(f1_scores_text_df))]  # Position for Text-Focused\n",
        "\n",
        "# Plotting all Title-Focused F1-scores\n",
        "plt.bar(index_title, f1_scores_title_df['F1-Score'], label='Title-Focused')\n",
        "\n",
        "# Plotting all Text-Focused F1-scores (shifted on the x-axis after the Title-Focused bars)\n",
        "plt.bar(index_text, f1_scores_text_df['F1-Score'], label='Text-Focused')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Task')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.title('F1-Scores for Title-Focused and Text-Focused Classification')\n",
        "\n",
        "# Adjusting x-ticks to show all tasks\n",
        "plt.xticks(range(len(f1_scores_title_df) + len(f1_scores_text_df)),\n",
        "           list(f1_scores_title_df['Task']) + list(f1_scores_text_df['Task']),\n",
        "           rotation=45)\n",
        "\n",
        "# Adding legend\n",
        "plt.legend()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CWe2Kra5WAe",
        "outputId": "311273fd-603e-4755-898d-cccf36e6cb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Evaluating for Title Tasks:\n",
            "\n",
            "Starting training for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 255/255 [00:23<00:00, 11.07it/s, loss=0.606]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 255/255 [00:22<00:00, 11.50it/s, loss=0.961]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 255/255 [00:22<00:00, 11.55it/s, loss=0.0675]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 255/255 [00:22<00:00, 11.55it/s, loss=0.172]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 255/255 [00:22<00:00, 11.52it/s, loss=0.0163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 64/64 [00:01<00:00, 33.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard-category: 0.8343720161321142\n",
            "Classification Report for hazard-category:\n",
            "\n",
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                     allergens       0.87      0.92      0.89       377\n",
            "                    biological       0.92      0.91      0.91       339\n",
            "                      chemical       0.73      0.66      0.69        68\n",
            "food additives and flavourings       1.00      0.40      0.57         5\n",
            "                foreign bodies       0.70      0.86      0.77       111\n",
            "                         fraud       0.86      0.56      0.68        68\n",
            "                     migration       0.00      0.00      0.00         1\n",
            "          organoleptic aspects       0.33      0.30      0.32        10\n",
            "                  other hazard       0.54      0.52      0.53        27\n",
            "              packaging defect       0.60      0.27      0.38        11\n",
            "\n",
            "                      accuracy                           0.84      1017\n",
            "                     macro avg       0.65      0.54      0.57      1017\n",
            "                  weighted avg       0.84      0.84      0.83      1017\n",
            "\n",
            "\n",
            "Starting training for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 255/255 [00:22<00:00, 11.53it/s, loss=0.0945]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 255/255 [00:22<00:00, 11.51it/s, loss=0.0809]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 255/255 [00:22<00:00, 11.54it/s, loss=0.328]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 255/255 [00:22<00:00, 11.52it/s, loss=1.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 255/255 [00:22<00:00, 11.50it/s, loss=5.94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 64/64 [00:01<00:00, 33.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product-category: 0.7430015662205394\n",
            "Classification Report for product-category:\n",
            "\n",
            "                                                   precision    recall  f1-score   support\n",
            "\n",
            "                              alcoholic beverages       0.60      0.43      0.50         7\n",
            "                      cereals and bakery products       0.71      0.80      0.75       123\n",
            "     cocoa and cocoa preparations, coffee and tea       0.73      0.82      0.77        49\n",
            "                                    confectionery       0.64      0.40      0.49        40\n",
            "dietetic foods, food supplements, fortified foods       0.67      0.58      0.62        24\n",
            "                                    fats and oils       0.75      0.75      0.75         4\n",
            "                                   feed materials       0.00      0.00      0.00         3\n",
            "                           food contact materials       0.00      0.00      0.00         1\n",
            "                            fruits and vegetables       0.83      0.72      0.77       112\n",
            "                                 herbs and spices       0.40      0.62      0.49        16\n",
            "                            honey and royal jelly       0.00      0.00      0.00         1\n",
            "                                ices and desserts       0.89      0.88      0.88        56\n",
            "                     meat, egg and dairy products       0.89      0.90      0.89       282\n",
            "                          non-alcoholic beverages       0.85      0.71      0.77        31\n",
            "                     nuts, nut products and seeds       0.77      0.78      0.77        63\n",
            "                       other food product / mixed       0.00      0.00      0.00         9\n",
            "                                         pet feed       1.00      0.33      0.50         6\n",
            "                       prepared dishes and snacks       0.37      0.51      0.43        90\n",
            "                                          seafood       0.96      0.82      0.88        56\n",
            "             soups, broths, sauces and condiments       0.60      0.56      0.58        43\n",
            "                                sugars and syrups       0.00      0.00      0.00         1\n",
            "\n",
            "                                         accuracy                           0.74      1017\n",
            "                                        macro avg       0.55      0.51      0.52      1017\n",
            "                                     weighted avg       0.75      0.74      0.74      1017\n",
            "\n",
            "\n",
            "Starting training for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 255/255 [00:22<00:00, 11.52it/s, loss=4.36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 255/255 [00:22<00:00, 11.52it/s, loss=1.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3:  82%|████████▏ | 210/255 [00:18<00:03, 11.53it/s, loss=1.2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQFCK2AKS84s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steliosg23/PDS-A2/blob/main/Food_Hazard_Detection_PubMedBERT_Benchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6e4pRWUUGFk"
      },
      "source": [
        "## Assignement 2\n",
        "### Food Hazard Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9of3N4dpUGF-"
      },
      "source": [
        "# Benchmarks - Advanced Model: PubMedBERT\n",
        "\n",
        "In this task, we aim to classify food safety-related incidents based on two distinct types of input data: short texts (title) and long texts (text).\n",
        "\n",
        "Using Advanced Model: PubMedBERT  \n",
        "\n",
        "\n",
        "For each of these input types, we perform the following two subtasks:\n",
        "\n",
        "**Subtasks (Performed Separately for  title and text):**\n",
        "\n",
        "**Subtask 1:**\n",
        "\n",
        "- Classify hazard-category (general hazard type).\n",
        "\n",
        "- Classify product-category (general product type).\n",
        "\n",
        "**Subtask 2:**\n",
        "\n",
        "- Classify hazard (specific hazard).\n",
        "- Classify product (specific product).\n",
        "\n",
        "We use all features (year, month, day, country, and the text feature) as input.\n",
        "\n",
        "Thus, we treat title and text as two distinct data sources, with each undergoing its own preprocessing, model training, and evaluation for all four targets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the file on Google Drive\n",
        "train_path = '/content/drive/MyDrive/Data/incidents_train.csv'\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(train_path)\n",
        "df = df.drop(columns=['Unnamed: 0'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4g33xroGg4P",
        "outputId": "02d0ba2d-1684-4cc6-d775-f08f23fd59ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "mRFk2pk0H6P4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Device for GPU"
      ],
      "metadata": {
        "id": "LqlqRgiXIKOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained BERT Tokenizer and Model"
      ],
      "metadata": {
        "id": "79qymYJEINLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Text Cleaning Function"
      ],
      "metadata": {
        "id": "IoA53QC2IVz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Cleaning Function to Data"
      ],
      "metadata": {
        "id": "WXmQRg_4IZ_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Feature Columns and Targets"
      ],
      "metadata": {
        "id": "P7yfTEksIeCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for Title and Text Columns"
      ],
      "metadata": {
        "id": "-uhwRoleIg6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Dataset Class for BERT Input"
      ],
      "metadata": {
        "id": "a15ZyK3TInMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Neural Network Classifier"
      ],
      "metadata": {
        "id": "uNrOdpsnIqlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Model Dynamically Based on Target Classes"
      ],
      "metadata": {
        "id": "taC3m2ZnJWs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "y2wbSyDAJZ6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1-Score Calculation and Training Loop"
      ],
      "metadata": {
        "id": "iNm3fHjGJgj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Evaluate Model for Title and Text (Dynamic num_classes)"
      ],
      "metadata": {
        "id": "St1DkLtqJkZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot F1-Scores"
      ],
      "metadata": {
        "id": "wcBEB7bjJn-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "\n",
        "# Get the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean text (title or text) and remove stopwords\n",
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters (excluding spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra spaces\n",
        "    text = ' '.join(text.split())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "df['title'] = df['title'].apply(clean_text)\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Define relevant features and targets\n",
        "features = ['year', 'month', 'day', 'country']\n",
        "targets_subtask1 = []\n",
        "targets_subtask2 = ['hazard', 'product']\n",
        "\n",
        "# Function to prepare data for both title and text\n",
        "def prepare_data(text_column):\n",
        "    X = df[features + [text_column]]\n",
        "    y_subtask1 = df[targets_subtask1]\n",
        "    y_subtask2 = df[targets_subtask2]\n",
        "\n",
        "    # Initialize LabelEncoders for each target column\n",
        "    label_encoders = {}\n",
        "    for target in targets_subtask1 + targets_subtask2:\n",
        "        le = LabelEncoder()\n",
        "        df[target] = le.fit_transform(df[target])  # Convert categorical labels to integers\n",
        "        label_encoders[target] = le\n",
        "\n",
        "    # Splitting data for both tasks\n",
        "    data_splits = {}\n",
        "    for target in targets_subtask1 + targets_subtask2:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, df[target], test_size=0.2, random_state=42\n",
        "        )\n",
        "        data_splits[target] = (X_train, X_test, y_train, y_test)\n",
        "\n",
        "    return data_splits, label_encoders\n",
        "\n",
        "# Prepare data for title and text (with updated label encoding)\n",
        "title_splits, title_label_encoders = prepare_data('title')\n",
        "text_splits, text_label_encoders = prepare_data('text')\n",
        "\n",
        "# Initialize an empty DataFrame to store F1-scores for title\n",
        "f1_scores_title_df = pd.DataFrame(columns=['Task', 'F1-Score'])\n",
        "f1_scores_text_df = pd.DataFrame(columns=['Task', 'F1-Score'])\n",
        "\n",
        "# Define the custom dataset class for BERT\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        label = self.labels[item]  # Labels are already integers\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)  # labels as integers\n",
        "        }\n",
        "\n",
        "# Function to create a neural network model with BERT\n",
        "def build_bert_model(num_classes):\n",
        "    class BERTClassifier(nn.Module):\n",
        "        def __init__(self, pretrained_model_name, num_classes):\n",
        "            super(BERTClassifier, self).__init__()\n",
        "            self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
        "            self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "            self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            outputs = self.bert(input_ids, attention_mask=attention_mask)  # BERT forward pass\n",
        "            output = outputs[1]  # Get the pooled output (from [CLS] token)\n",
        "            output = self.dropout(output)  # Apply dropout\n",
        "            return self.fc(output)  # Final classification\n",
        "\n",
        "    model = BERTClassifier('bert-base-uncased', num_classes)\n",
        "    return model\n",
        "\n",
        "# Function to train and evaluate BERT model\n",
        "def train_and_evaluate_bert(data_splits, targets, text_column, hyperparameters):\n",
        "    global f1_scores_title_df, f1_scores_text_df\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    max_len = hyperparameters['max_len']  # Adjustable max length\n",
        "\n",
        "    for target in targets:\n",
        "        print(f\"\\nTraining for {target}...\")  # Mention the task being trained\n",
        "\n",
        "        X_train, X_test, y_train, y_test = data_splits[target]\n",
        "\n",
        "        train_dataset = TextDataset(X_train[text_column].values, y_train.values, tokenizer, max_len)\n",
        "        test_dataset = TextDataset(X_test[text_column].values, y_test.values, tokenizer, max_len)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], shuffle=False)\n",
        "\n",
        "        model = build_bert_model(num_classes=len(y_train.unique()))\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters['learning_rate'])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop with tqdm progress bar\n",
        "        model.train()\n",
        "        for epoch in range(hyperparameters['num_epochs']):  # Number of epochs can be customized\n",
        "            epoch_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", ncols=100, position=0, leave=True)\n",
        "            for batch in epoch_bar:\n",
        "                optimizer.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update tqdm progress bar with loss info\n",
        "                epoch_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                _, preds = torch.max(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate F1-Score\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        # Collect F1-score into DataFrame\n",
        "        if text_column == 'title':\n",
        "            f1_scores_title_df = pd.concat([f1_scores_title_df, pd.DataFrame({'Task': [f\"{target} (Title)\"], 'F1-Score': [f1]})], ignore_index=True)\n",
        "        else:\n",
        "            f1_scores_text_df = pd.concat([f1_scores_text_df, pd.DataFrame({'Task': [f\"{target} (Text)\"], 'F1-Score': [f1]})], ignore_index=True)\n",
        "\n",
        "        # Print the classification report\n",
        "        print(f\"\\nClassification Report for {target} ({text_column}):\")\n",
        "        print(classification_report(all_labels, all_preds, zero_division=0))  # Handle zero division gracefully\n",
        "\n",
        "# Define hyperparameters\n",
        "def get_hyperparameters():\n",
        "    return {\n",
        "        'learning_rate': 2e-5,\n",
        "        'batch_size': 16,\n",
        "        'num_epochs': 1,\n",
        "        'max_len': 128\n",
        "    }\n",
        "\n",
        "# Get hyperparameters\n",
        "hyperparameters = get_hyperparameters()\n",
        "\n",
        "# Train and evaluate BERT for title\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"BERT for Titles:\")\n",
        "train_and_evaluate_bert(title_splits, targets_subtask1 + targets_subtask2, text_column='title', hyperparameters=hyperparameters)\n",
        "\n",
        "# Train and evaluate BERT for text\n",
        "print(\"\\nBERT for Texts:\")\n",
        "train_and_evaluate_bert(text_splits, targets_subtask1 + targets_subtask2, text_column='text', hyperparameters=hyperparameters)\n",
        "\n",
        "# Print the collected F1-scores for title\n",
        "print(\"\\nCollected F1-Scores for Title-Focused Classification:\")\n",
        "print(f1_scores_title_df)\n",
        "\n",
        "# Print the collected F1-scores for text\n",
        "print(\"\\nCollected F1-Scores for Text-Focused Classification:\")\n",
        "print(f1_scores_text_df)\n",
        "\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting Title-Focused F1-scores\n",
        "plt.bar(f1_scores_title_df['Task'], f1_scores_title_df['F1-Score'], label='Title-Focused')\n",
        "\n",
        "# Plotting Text-Focused F1-scores\n",
        "plt.bar(f1_scores_text_df['Task'], f1_scores_text_df['F1-Score'], label='Text-Focused')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Task')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.title('F1-Scores for Title-Focused vs Text-Focused Classification with BERT')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DubLjVG-OXfG",
        "outputId": "0c61767e-8ef1-4555-8c90-4d8d50912c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT for Titles:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training for hazard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   9%|███▌                                      | 22/255 [00:10<01:18,  2.95it/s, loss=4.63]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnbACWoKdQLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
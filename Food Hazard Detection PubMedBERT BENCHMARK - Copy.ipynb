{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignement 2\n",
    "### Food Hazard Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Inspection\n",
    "First step is to load the data directly from github repository and take a look into an overview of the dataframe that we create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\steli\\OneDrive\\Desktop\\Stelios\\DSAUEB\\Trimester 1\\PDS\\A2\\PDS-A2\\Data\\incidents_train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initial inspection of the data\n",
    "data_overview = {\n",
    "    'Shape': df.shape,\n",
    "    'Columns': df.columns.tolist(),\n",
    "    'df Types': df.dtypes,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "}\n",
    "\n",
    "print(data_overview)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unnecessary Index Column\n",
    "Removing not meaningful columns such as index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary index column\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical summary of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical summary for numerical columns\n",
    "numerical_summary = df.describe()\n",
    "numerical_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset spans from 1994 to 2022, with most incidents occurring in recent years, particularly after 2015. The average incident year is 2016, indicating a focus on modern data, while monthly values center around June, hinting at potential mid-year peaks. Incident days average around the 15th, suggesting a balanced distribution across the month. Quartile analysis shows that most incidents (75%) occurred between 2015 and 2022, pointing to a recent and perhaps seasonal relevance in the data, which could be valuable for time-series and trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary for categorical columns\n",
    "categorical_summary = df.describe(include=['object'])\n",
    "categorical_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides a multi-dimensional view of food hazard incidents, primarily from the U.S. but covering nine countries overall. Analysis shows a high degree of uniqueness in incident titles and descriptions, reflecting varied hazard cases. Categories like allergens and Listeria monocytogenes are common, often associated with products such as meat, dairy, and eggs. The data also reveals that specific products, especially ice cream, appear frequently, which might indicate recurring risks in certain food types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of top 5 product categories per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Top 5 product categories per country\n",
    "top_5_per_country = (\n",
    "    df.groupby('country')['product-category']\n",
    "    .value_counts()\n",
    "    .groupby(level=0)\n",
    "    .nlargest(5)\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Plotting the map of top 5 product categories per country\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(\n",
    "    x='country', y='product-category', size='product-category',\n",
    "    sizes=(100, 500), data=top_5_per_country, hue='product-category', legend=False\n",
    ")\n",
    "plt.title('Top 5 Product Categories Per Country', fontsize=15)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Product Category', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bubble chart shows the top 5 product categories per country, with larger bubbles indicating more frequent categories. Common categories like \"meat, egg, and dairy products\" and \"cereals and bakery products\" appear across most countries. Some countries, like the UK and Singapore, show more diverse product preferences, while others, like the US, have a few dominant categories. This highlights regional variations in food product consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(\n",
    "    y='country', data=df, \n",
    "    order=df['country'].value_counts().index, \n",
    "    color=\"skyblue\", legend=False\n",
    ")\n",
    "plt.title('Country Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows the distribution of data across countries, with the United States (\"us\") having the highest count, followed by Australia (\"au\") and Canada (\"ca\"). Countries like the UK, Ireland (\"ie\"), and Hong Kong (\"hk\") also have notable counts, but much lower compared to the US. The other countries such as Singapore (\"sg\"), Scotland (\"scot\"), and North America (\"na\") have smaller counts. This distribution suggests that the dataset is heavily skewed toward the US, which may lead to biases if used for analysis or predictive modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hazard Category Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hazard Category Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(\n",
    "    y='hazard-category', data=df, \n",
    "    order=df['hazard-category'].value_counts().index, \n",
    "    color=\"lightgreen\", legend=False\n",
    ")\n",
    "plt.title('Hazard Category Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows the distribution of different hazard categories. \"Allergens\" and \"biological\" hazards have the highest counts, which dominate the dataset, while categories like \"migration\" and \"food additives and flavourings\" have much lower frequencies. This suggests that allergen and biological risks are the most prominent concerns in the data, which is useful for food safety analysis. However, the imbalance in hazard categories, with some hazards being more frequent than others, could lead to biased models or analyses that favor the more common hazards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Category Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Category Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(\n",
    "    y='product-category', data=df, \n",
    "    order=df['product-category'].value_counts().index, \n",
    "    color=\"salmon\", legend=False\n",
    ")\n",
    "plt.title('Product Category Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar chart visualizes the distribution of product categories, with the \"meat, egg and dairy products\" category showing the highest frequency, followed by \"cereals and bakery products\" and \"fruits and vegetables.\" Categories such as \"pet feed,\" \"honey and royal jelly,\" and \"sugars and syrups\" are represented with significantly lower counts, indicating an imbalance in the dataset.\n",
    "\n",
    "This imbalance suggests that certain product categories are overrepresented, which could lead to biased analyses or models that favor these dominant categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Hazards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Hazards\n",
    "top_20_hazards = df['hazard'].value_counts().nlargest(20)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    y=top_20_hazards.index, x=top_20_hazards.values, \n",
    "    color=\"orange\", legend=False\n",
    ")\n",
    "plt.title('Top 20 Hazards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart displays the distribution of the top 20 hazards in the dataset, with \"Listeria monocytogenes\" and \"Salmonella\" being the most prevalent, followed by \"milk and products thereof,\" \"Escherichia coli,\" and \"peanuts and products thereof.\"\n",
    "\n",
    "The chart highlights a potential focus area for food safety management in addressing microbial contamination and physical hazards in products, with overrepresentation of certain hazards that could influence the analysis or model outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Products\n",
    "top_20_products = df['product'].value_counts().nlargest(20)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    y=top_20_products.index, x=top_20_products.values, \n",
    "    color=\"purple\", legend=False\n",
    ")\n",
    "plt.title('Top 20 Products')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar chart shows the top 20 products in the dataset, with \"ice cream\" being the most frequently occurring product, followed by \"chicken-based products\" and \"cakes.\" Other products like \"cookies,\" \"cheese,\" and \"salads\" also have significant counts. The chart suggests that certain food items are overrepresented, potentially influencing any analysis or model developed from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['year'], bins=20, kde=True, color=\"lightcoral\")\n",
    "plt.title('Year Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows the distribution of data over the years. It indicates a sharp increase in the count of entries starting around 2010, with the highest peaks occurring in the years 2019 and 2020. This trend suggests a growing number of incidents, products, or reports over time, particularly in the more recent years. The relatively low counts before 2010 imply that data collection or reporting might have started later or that there were fewer occurrences in earlier years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud and Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine all titles into a single string\n",
    "title_text = ' '.join(df['title'].dropna())\n",
    "\n",
    "# Generate a word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(title_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud for Incident Titles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word cloud shows the most frequent terms associated with product recalls and incidents. Prominent words like \"recalled,\" \"due,\" \"health,\" and \"risk\" highlight the primary focus on safety concerns, particularly related to potential health risks such as \"allergen,\" \"salmonella,\" and \"Listeria monocytogenes.\" \"Brand\" and \"products\" are also frequently mentioned, suggesting that brand names and specific product recalls are central to the incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks - Advanced Model: PubMedBERT \n",
    "\n",
    "In this task, we aim to classify food safety-related incidents based on two distinct types of input data: short texts (title) and long texts (text). \n",
    "\n",
    "Using Advanced Model: PubMedBERT  \n",
    "\n",
    "\n",
    "For each of these input types, we perform the following two subtasks:\n",
    "\n",
    "**Subtasks (Performed Separately for  title and text):**\n",
    "\n",
    "**Subtask 1:**\n",
    "\n",
    "- Classify hazard-category (general hazard type).\n",
    "\n",
    "- Classify product-category (general product type).\n",
    "\n",
    "**Subtask 2:**\n",
    "\n",
    "- Classify hazard (specific hazard).\n",
    "- Classify product (specific product).\n",
    "\n",
    "We use all features (year, month, day, country, and the text feature) as input.\n",
    "\n",
    "Thus, we treat title and text as two distinct data sources, with each undergoing its own preprocessing, model training, and evaluation for all four targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "# Define relevant features and targets\n",
    "features = ['year', 'month', 'day', 'country']\n",
    "targets_subtask1 = ['hazard-category', 'product-category']\n",
    "targets_subtask2 = ['hazard', 'product']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Pipelines and Train Models (PubMedBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubMedBERT Classification Titles (Short Text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Function to prepare data for both title and text\n",
    "def prepare_data(text_column):\n",
    "    X = df[features + [text_column]]\n",
    "    y_subtask1 = df[targets_subtask1]\n",
    "    y_subtask2 = df[targets_subtask2]\n",
    "    \n",
    "    # Splitting data for both tasks\n",
    "    data_splits = {}\n",
    "    for target in targets_subtask1 + targets_subtask2:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, df[target], test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit the label encoder on the entire target column (train + test)\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(df[target])  # Fit on the entire column\n",
    "        y_test = le.transform(y_test)  # Transform both training and test\n",
    "\n",
    "        data_splits[target] = (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    return data_splits\n",
    "\n",
    "# Prepare data for title\n",
    "title_splits = prepare_data('title')\n",
    "\n",
    "# Custom Dataset for PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# Function to train and evaluate model for each target\n",
    "def train_and_evaluate_model(text_column, target):\n",
    "    # Load pretrained PubMedBERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "    \n",
    "    # Determine number of unique labels for the target\n",
    "    num_labels = len(set(df[target]))\n",
    "\n",
    "    # Load the pre-trained PubMedBERT model with dynamic number of labels\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    # Prepare the dataset for training and testing\n",
    "    X_train, X_test, y_train, y_test = title_splits[target][:4]\n",
    "    train_dataset = CustomDataset(X_train[text_column].values, y_train, tokenizer)\n",
    "    test_dataset = CustomDataset(X_test[text_column].values, y_test, tokenizer)\n",
    "    \n",
    "    # Define Trainer arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "    )\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: {'f1': f1_score(p.predictions.argmax(axis=1), p.label_ids, average='weighted')}\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Train and evaluate models for all subtasks (title-based classification)\n",
    "results_title = {}\n",
    "for target in targets_subtask1 + targets_subtask2:\n",
    "    results_title[target] = train_and_evaluate_model('title', target)\n",
    "\n",
    "# Print results for title-based classification\n",
    "for target in results_title:\n",
    "    print(f\"Results for {target} (Title-based):\")\n",
    "    print(results_title[target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubMedBERT Classification Texts (Long Text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steliosg23/PDS-A2/blob/main/Augmented%20Train%20Set%20Benchmark%20Models%20Finetuned%20PubMedBERT%20PDS%20A2%20Food%20Hazard_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpqOuXHKPREa"
      },
      "source": [
        "#Assignement 2\n",
        "## Food Hazard Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG2far4MTvkS"
      },
      "source": [
        "# Benchmarks - Advanced Model: PubMedBERT\n",
        "\n",
        "**PubMedBERT** is a **pretrained language model** based on BERT, specifically fine-tuned on **biomedical text** such as PubMed abstracts. This model is designed to capture the complexities of biomedical language, including specialized terminology and health-related concepts.\n",
        "\n",
        "### Benefits of Using Pretrained PubMedBERT for Food Hazard Detection:\n",
        "- **Domain Expertise**: Being pretrained on biomedical literature, PubMedBERT already understands key terms related to **food safety**, such as **toxins**, **bacteria**, and **allergens**, which makes it highly effective for detecting and classifying food hazards.\n",
        "  \n",
        "- **Faster and More Efficient**: Using a pretrained model significantly reduces training time compared to building a model from scratch. It has already learned general language patterns, so fine-tuning it on food hazard data requires less data and computational power while still achieving strong performance.\n",
        "\n",
        "- **Task-Specific Fine-Tuning**: By fine-tuning PubMedBERT on tasks like **hazard-category**, **product-category**, **hazard**, and **product** classification, it adapts to the specific requirements of food hazard detection. This enables accurate classification of food safety incidents and associated risks.\n",
        "\n",
        "### Task-Specific Fine-Tuning of PubMedBERT\n",
        "\n",
        "Fine-tuning PubMedBERT for food hazard detection involves adapting its pre-trained biomedical knowledge to specific classification tasks:\n",
        "- **Hazard-Category Classification**: Classifies types of hazards (e.g., microbiological, chemical).\n",
        "- **Product-Category Classification**: Identifies food product types (e.g., dairy, meat).\n",
        "- **Hazard Identification**: Detects specific hazards (e.g., Salmonella, pesticide).\n",
        "- **Product Identification**: Pinpoints exact products (e.g., milk, spinach).\n",
        "\n",
        "**Process**:\n",
        "1. Add task-specific classification heads.\n",
        "2. Fine-tune on labeled food hazard data using loss functions like cross-entropy.\n",
        "3. Optimize representations for accurate domain-specific predictions.\n",
        "\n",
        "**Benefits**:\n",
        "- Adapts to food safety terminology.\n",
        "- Improves classification accuracy.\n",
        "- Handles diverse food hazard scenarios effectively.\n",
        "\n",
        "## Fine-tuning Neural Network Models for Classification Tasks\n",
        "\n",
        "### Overview\n",
        "\n",
        "The `train_and_evaluate_nn` function performs **fine-tuning** of pre-trained transformer models for specific classification tasks. Fine-tuning involves adapting a pre-trained model to task-specific data to improve its performance on a given problem.\n",
        "\n",
        "### Fine-tuning Process\n",
        "\n",
        "1. **Pre-trained Model Initialization**:\n",
        "   - The function uses `AutoModelForSequenceClassification.from_pretrained` to load a pre-trained transformer model (e.g., BERT, RoBERTa, PubMedBERT).\n",
        "   - These models are pre-trained on large corpora to capture general language representations.\n",
        "\n",
        "2. **Task-specific Adjustment**:\n",
        "   - Fine-tuning modifies the model's weights using labeled datasets for specific tasks (e.g., hazard or product classification).\n",
        "   - A classification head with `num_labels` matching the task's categories is added.\n",
        "\n",
        "3. **Custom Data Preparation**:\n",
        "   - Text data (`title` or `text`) and labels are tokenized and loaded into `TextDataset`.\n",
        "   - DataLoaders provide batches for training and evaluation.\n",
        "\n",
        "4. **Training**:\n",
        "   - The model is fine-tuned for several epochs using task-specific data.\n",
        "   - Loss is computed with `CrossEntropyLoss`, and weights are updated via the `Adam` optimizer.\n",
        "\n",
        "5. **Evaluation**:\n",
        "   - Predictions are decoded and evaluated using metrics like the weighted **F1-score** and classification reports.\n",
        "\n",
        "\n",
        "### Outputs\n",
        "\n",
        "- Weighted **F1-scores** for each task.\n",
        "- Detailed **classification reports** for evaluation.\n",
        "\n",
        "\n",
        "### Use in Food Hazard Detection:\n",
        "1. **Hazard-category**: Classifies the **type of hazard** (e.g., biological, chemical).\n",
        "2. **Product-category**: Identifies the **type of product** (e.g., meat, dairy) linked to a hazard.\n",
        "3. **Hazard**: Detects **specific foodborne hazards** such as bacteria or contamination.\n",
        "4. **Product**: Classifies the **specific product** involved in the food safety incident.\n",
        "\n",
        "The **pretrained** nature of PubMedBERT allows for more efficient adaptation to the food hazard domain, enabling faster, more accurate identification of food safety risks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS3IfUfUPUYy"
      },
      "source": [
        "\n",
        "# Method\n",
        "In this task, we aim to classify food safety-related incidents based on two distinct types of input data: short texts (title) and long texts (text).\n",
        "\n",
        "Using Advanced Model: PubMedBERT  \n",
        "\n",
        "\n",
        "For each of these input types, we perform the following two subtasks:\n",
        "\n",
        "**Subtasks (Performed Separately for  title and text):**\n",
        "\n",
        "**Subtask 1:**\n",
        "\n",
        "- Classify hazard-category (general hazard type).\n",
        "\n",
        "- Classify product-category (general product type).\n",
        "\n",
        "**Subtask 2:**\n",
        "\n",
        "- Classify hazard (specific hazard).\n",
        "- Classify product (specific product).\n",
        "\n",
        "We use all features (year, month, day, country, and the text feature) as input.\n",
        "\n",
        "Thus, we treat title and text as two distinct data sources, with each undergoing its own preprocessing, model training, and evaluation for all four targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYTL5Xs0PL1V"
      },
      "source": [
        "# Mount Google Drive and Load Dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pAjqx37bPD7o",
        "outputId": "4baf1a81-ec8e-4111-a443-4550bb626fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       year  month  day country  \\\n",
              "0      2008     11    7      au   \n",
              "1      2011      7   11      au   \n",
              "2      2012      2   21      au   \n",
              "3      2012     12    4      us   \n",
              "4      2014      4   10      au   \n",
              "...     ...    ...  ...     ...   \n",
              "11834  2016      9   29      au   \n",
              "11835  2016      8   18      au   \n",
              "11836  2018     11   30      us   \n",
              "11837  2005      9   27      au   \n",
              "11838  2007     11    1      au   \n",
              "\n",
              "                                                   title  \\\n",
              "0           Country Cuisine Pty Ltd—Malouf’s Spice Mezza   \n",
              "1      Haigh's Manufacturing Pty Ltd—Haigh’s Aprichoc...   \n",
              "2      Coles Supermarkets Limited—Coles Deli 200g Spi...   \n",
              "3      2012 - Price Chopper Supermarkets Recalls Cent...   \n",
              "4                                      Coles Easter Eggs   \n",
              "...                                                  ...   \n",
              "11834  Quality Bakers Australia Pty Limited — Various...   \n",
              "11835  Gluten Free Bakehouse Pty Ltd — Various Zehnde...   \n",
              "11836  Tres Hermanos Bakery Issues Allergy Alert on U...   \n",
              "11837  Gold Coast Bakery Queensland Pty Ltd—Vogels—Fr...   \n",
              "11838  Woolworths Limited—Home brand white sliced san...   \n",
              "\n",
              "                                                    text hazard-category  \\\n",
              "0      PRA No. 2008/10424 Date Published Nov 7, 2008 ...       allergens   \n",
              "1      PRA No. 2011/12730 Publication Date Jul 11, 20...       allergens   \n",
              "2      PRA No. 2012/13032 Publication Date Feb 21, 20...       allergens   \n",
              "3      FOR IMMEDIATE RELEASE - October 21, 2012 - (Sc...       allergens   \n",
              "4      Coles Supermarkets Australia Pty Ltd recalled ...       allergens   \n",
              "...                                                  ...             ...   \n",
              "11834  PRA No. 2016/15657 Date published 29 Sep 2016 ...  foreign bodies   \n",
              "11835  PRA No. 2016/15603 Date published 18 Aug 2016 ...       allergens   \n",
              "11836  Wyoming, MI - Tres Hermanos Bakery of Wyoming,...       allergens   \n",
              "11837  PRA No. 2005/8073 Date published 27 Sep 2005 P...  foreign bodies   \n",
              "11838  PRA No. 2007/9622 Date published 1 Nov 2007 Pr...  foreign bodies   \n",
              "\n",
              "                                   product-category  \\\n",
              "0                                  herbs and spices   \n",
              "1      cocoa and cocoa preparations, coffee and tea   \n",
              "2              soups, broths, sauces and condiments   \n",
              "3                       cereals and bakery products   \n",
              "4      cocoa and cocoa preparations, coffee and tea   \n",
              "...                                             ...   \n",
              "11834                   cereals and bakery products   \n",
              "11835                   cereals and bakery products   \n",
              "11836                   cereals and bakery products   \n",
              "11837                   cereals and bakery products   \n",
              "11838                   cereals and bakery products   \n",
              "\n",
              "                              hazard         product  \n",
              "0                               nuts       spice mix  \n",
              "1                               nuts       chocolate  \n",
              "2                               nuts           sauce  \n",
              "3                               nuts           cakes  \n",
              "4                               nuts  chocolate eggs  \n",
              "...                              ...             ...  \n",
              "11834                 metal fragment           bread  \n",
              "11835  soybeans and products thereof           bread  \n",
              "11836      milk and products thereof           bread  \n",
              "11837               plastic fragment           bread  \n",
              "11838                 glass fragment           bread  \n",
              "\n",
              "[11839 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28a10d7e-b37d-4101-af4c-22ea68be727d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>country</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>hazard-category</th>\n",
              "      <th>product-category</th>\n",
              "      <th>hazard</th>\n",
              "      <th>product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>au</td>\n",
              "      <td>Country Cuisine Pty Ltd—Malouf’s Spice Mezza</td>\n",
              "      <td>PRA No. 2008/10424 Date Published Nov 7, 2008 ...</td>\n",
              "      <td>allergens</td>\n",
              "      <td>herbs and spices</td>\n",
              "      <td>nuts</td>\n",
              "      <td>spice mix</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>au</td>\n",
              "      <td>Haigh's Manufacturing Pty Ltd—Haigh’s Aprichoc...</td>\n",
              "      <td>PRA No. 2011/12730 Publication Date Jul 11, 20...</td>\n",
              "      <td>allergens</td>\n",
              "      <td>cocoa and cocoa preparations, coffee and tea</td>\n",
              "      <td>nuts</td>\n",
              "      <td>chocolate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>au</td>\n",
              "      <td>Coles Supermarkets Limited—Coles Deli 200g Spi...</td>\n",
              "      <td>PRA No. 2012/13032 Publication Date Feb 21, 20...</td>\n",
              "      <td>allergens</td>\n",
              "      <td>soups, broths, sauces and condiments</td>\n",
              "      <td>nuts</td>\n",
              "      <td>sauce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>us</td>\n",
              "      <td>2012 - Price Chopper Supermarkets Recalls Cent...</td>\n",
              "      <td>FOR IMMEDIATE RELEASE - October 21, 2012 - (Sc...</td>\n",
              "      <td>allergens</td>\n",
              "      <td>cereals and bakery products</td>\n",
              "      <td>nuts</td>\n",
              "      <td>cakes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>au</td>\n",
              "      <td>Coles Easter Eggs</td>\n",
              "      <td>Coles Supermarkets Australia Pty Ltd recalled ...</td>\n",
              "      <td>allergens</td>\n",
              "      <td>cocoa and cocoa preparations, coffee and tea</td>\n",
              "      <td>nuts</td>\n",
              "      <td>chocolate eggs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11834</th>\n",
              "      <td>2016</td>\n",
              "      <td>9</td>\n",
              "      <td>29</td>\n",
              "      <td>au</td>\n",
              "      <td>Quality Bakers Australia Pty Limited — Various...</td>\n",
              "      <td>PRA No. 2016/15657 Date published 29 Sep 2016 ...</td>\n",
              "      <td>foreign bodies</td>\n",
              "      <td>cereals and bakery products</td>\n",
              "      <td>metal fragment</td>\n",
              "      <td>bread</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11835</th>\n",
              "      <td>2016</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>au</td>\n",
              "      <td>Gluten Free Bakehouse Pty Ltd — Various Zehnde...</td>\n",
              "      <td>PRA No. 2016/15603 Date published 18 Aug 2016 ...</td>\n",
              "      <td>allergens</td>\n",
              "      <td>cereals and bakery products</td>\n",
              "      <td>soybeans and products thereof</td>\n",
              "      <td>bread</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11836</th>\n",
              "      <td>2018</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>us</td>\n",
              "      <td>Tres Hermanos Bakery Issues Allergy Alert on U...</td>\n",
              "      <td>Wyoming, MI - Tres Hermanos Bakery of Wyoming,...</td>\n",
              "      <td>allergens</td>\n",
              "      <td>cereals and bakery products</td>\n",
              "      <td>milk and products thereof</td>\n",
              "      <td>bread</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11837</th>\n",
              "      <td>2005</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "      <td>au</td>\n",
              "      <td>Gold Coast Bakery Queensland Pty Ltd—Vogels—Fr...</td>\n",
              "      <td>PRA No. 2005/8073 Date published 27 Sep 2005 P...</td>\n",
              "      <td>foreign bodies</td>\n",
              "      <td>cereals and bakery products</td>\n",
              "      <td>plastic fragment</td>\n",
              "      <td>bread</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11838</th>\n",
              "      <td>2007</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>au</td>\n",
              "      <td>Woolworths Limited—Home brand white sliced san...</td>\n",
              "      <td>PRA No. 2007/9622 Date published 1 Nov 2007 Pr...</td>\n",
              "      <td>foreign bodies</td>\n",
              "      <td>cereals and bakery products</td>\n",
              "      <td>glass fragment</td>\n",
              "      <td>bread</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11839 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28a10d7e-b37d-4101-af4c-22ea68be727d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-28a10d7e-b37d-4101-af4c-22ea68be727d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-28a10d7e-b37d-4101-af4c-22ea68be727d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f116b260-02ee-4df9-ac83-193fb1c57891\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f116b260-02ee-4df9-ac83-193fb1c57891')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f116b260-02ee-4df9-ac83-193fb1c57891 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_19d37a45-b698-4ff2-a244-0509db973a3c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_19d37a45-b698-4ff2-a244-0509db973a3c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 11839,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1994,\n        \"max\": 2022,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          1996,\n          2010,\n          2013\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          6,\n          5,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          18,\n          3,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"scot\",\n          \"us\",\n          \"ie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3732,\n        \"samples\": [\n          \"Giant/Martin\\u2019s Alerts Customers to Voluntary Recall of Store Brand Frozen Whole Kernel Sweet Corn\",\n          \"Flowers Foods Issues Voluntary Recall on Certain Tastykake Multi-Pack Cupcakes Sold in Eight States Due to Possible Presence of Tiny Fragments of Metal Mesh Wire\",\n          \"2015 - Gilster - Mary Lee Corp. Issues an Allergen Alert for Undeclared Almonds in Market Pantry Honey & Oat Mixers Ready to Eat Cereal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3900,\n        \"samples\": [\n          \"The presence of a small battery makes this product dangerous to eat and poses a health risk. Product: Coop Hollow Milk Chocolate Bunny Package size: 120g Lot codes: All date codes No other Co-op products are known to be affected. The product below may contain a small battery that can be a choking risk and can cause burns in the event of ingestion. If you purchased the product do not eat it. Instead, return it to a Co-op store for a full refund, or contact their customer relationship team at 0800 0686 727. Co-op recalls the product above. Point-of-sale notices will be displayed in all retail stores that sell the product. These reviews explain to customers why the product is recalled and tell them what to do if they bought the product.\",\n          \"Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.\",\n          \"The company Double Cola recalls about 200 cases of its product Cherry SKI because they are mislabeled. Only Cherry SKI products with the new design are recalled, which have been delivered south of Illinois, Evansville, IN and Winchester, OH markets. No other products from the company Double Cola are recalled. New Cherry SKI cans are recalled because the nutritional facts are incorrect. The cans show \\\"Red #4\\\" under ingredients, which is incorrect. The ingredients must include \\\"Red #40\\\" and \\\"Yellow #5.\\\" These dyes can cause a reaction to any allergic person. Double Cola company has reported that no medical illness or harmful effects have resulted from this product so far. The affected products are coded A8023EV10:25 to A8023EV11:40. \\\"We have taken immediate action once we have known the situation and recalled the product\\\" Gina Dhanani, vice-president of SKI.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hazard-category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"organoleptic aspects\",\n          \"foreign bodies\",\n          \"packaging defect\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product-category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"herbs and spices\",\n          \"seafood\",\n          \"ices and desserts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hazard\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 128,\n        \"samples\": [\n          \"chemical\",\n          \"coliforms\",\n          \"hazelnut\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 523,\n        \"samples\": [\n          \"horseradish in jars\",\n          \"liquorice\",\n          \"bakery products\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the file on Google Drive\n",
        "train_path = '/content/drive/MyDrive/Data/augmented_incidents_train.csv'\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(train_path)\n",
        "\n",
        "# Keep only the specified columns\n",
        "columns_to_keep = ['year', 'month', 'day', 'country', 'title', 'text', 'hazard-category', 'product-category', 'hazard', 'product']\n",
        "df = df[columns_to_keep]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97WMs5Q0ODJj"
      },
      "source": [
        "# Import Required Libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_tnUGj2aOD40"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pmVvEpBOHkz"
      },
      "source": [
        "# Configure Hyperparameters\n",
        "\n",
        "This cell sets up the hyperparameters for model training, including:\n",
        "- `max_len`: The maximum length of the input sequences.\n",
        "- `batch_size`: The number of samples per batch.\n",
        "- `learning_rate`: The learning rate for the optimizer.\n",
        "- `epochs`: The number of training epochs.\n",
        "- `model_name`: The pre-trained model (PubMedBERT in this case) to use for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mRqpDhY5OF0r"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters configuration\n",
        "config = {\n",
        "    'max_len': 256,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 2e-5,\n",
        "    'epochs': 5,\n",
        "    'model_name': \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsuqdQcyOLFt"
      },
      "source": [
        "# Set Device for Training\n",
        "\n",
        "This cell checks whether a GPU is available and sets the device for training (either `cuda` for GPU or `cpu` for CPU).\n",
        "It prints the device being used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzgpGLbkOKCp",
        "outputId": "07d35115-c406-48ba-8e32-d5d8c7ede386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-A4hSxYOOdo"
      },
      "source": [
        "# Define Custom Dataset Class\n",
        "\n",
        "Here we define a custom PyTorch `Dataset` class to handle the text data. This class takes in the input texts, labels, tokenizer, and maximum sequence length, and implements methods to return tokenized inputs and labels in a batch.\n",
        "- The `__len__` method returns the number of samples in the dataset.\n",
        "- The `__getitem__` method returns tokenized input data (input ids and attention mask) and the corresponding label for a given index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WbqvZzJgOMmA"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset for Text Data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcQI95wcOT3C"
      },
      "source": [
        "# Text Preprocessing - Cleaning Function\n",
        "\n",
        "This function cleans the text by removing any non-alphanumeric characters (e.g., punctuation) and converts the text to lowercase.\n",
        "This helps standardize the text for further processing, such as tokenization and model input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KlHfuLKlOQmI"
      },
      "outputs": [],
      "source": [
        "# Function to clean text (title or text) and remove stopwords\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lX3m9kDOXB7"
      },
      "source": [
        "# Preprocessing the Text Data\n",
        "\n",
        "In this step, we apply the `clean_text` function to the `title` and `text` columns of the DataFrame to clean and preprocess the text data. This ensures that all the text data used for model input is in a consistent format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5f2YpcNOVs5",
        "outputId": "36d748c9-bf86-4127-a784-8697ac34d7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer for Microsoft PubMedBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "df['title'] = df['title'].apply(clean_text)\n",
        "df['text'] = df['text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Am4pzJqOdSV"
      },
      "source": [
        "# Defining Features and Targets\n",
        "\n",
        "In this cell, we define the features (columns) used for input to the model, which include `year`, `month`, `day`, and `country`. We also define the target variables for both subtasks, such as `hazard-category`, `product-category`, `hazard`, and `product`. These will be the labels we aim to predict.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xe4fnbojOY-v"
      },
      "outputs": [],
      "source": [
        "# Define relevant features and targets\n",
        "features = ['year', 'month', 'day', 'country']\n",
        "targets_subtask1 = ['hazard-category','product-category']\n",
        "targets_subtask2 = ['hazard','product']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydZ2LF0MOgBU"
      },
      "source": [
        "# Label Encoding for Targets\n",
        "\n",
        "Here, we encode the categorical target labels into numeric values using `LabelEncoder` from `sklearn`. This step is necessary for training the model, as models require numeric labels for classification tasks.\n",
        "For each target, a new `LabelEncoder` is created, and the target column is transformed into numeric labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b7B8EhUdOer2"
      },
      "outputs": [],
      "source": [
        "# Encode target labels to numeric values\n",
        "label_encoders = {}\n",
        "for target in targets_subtask1 + targets_subtask2:\n",
        "    le = LabelEncoder()\n",
        "    df[target] = le.fit_transform(df[target])\n",
        "    label_encoders[target] = le\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzBXkKWrOjiP"
      },
      "source": [
        "# Data Preparation for Training and Testing\n",
        "\n",
        "This function splits the data into training and testing sets for each target variable. It also ensures that the features and corresponding targets are aligned and reset the indices for consistency. The function returns a dictionary containing the splits for each target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8a9dTrQ9Oh7Q"
      },
      "outputs": [],
      "source": [
        "# Prepare data for both title and text\n",
        "def prepare_data(text_column):\n",
        "    X = df[features + [text_column]]\n",
        "    y_subtask1 = df[targets_subtask1]\n",
        "    y_subtask2 = df[targets_subtask2]\n",
        "\n",
        "    data_splits = {}\n",
        "    for target in targets_subtask1 + targets_subtask2:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, df[target], test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Reset indices to ensure matching\n",
        "        X_train = X_train.reset_index(drop=True)\n",
        "        y_train = y_train.reset_index(drop=True)\n",
        "        X_test = X_test.reset_index(drop=True)\n",
        "        y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "        data_splits[target] = (X_train, X_test, y_train, y_test)\n",
        "\n",
        "    return data_splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyR49XO_Op1Z"
      },
      "source": [
        "# Prepare Data for Title and Text Subtasks\n",
        "\n",
        "This step prepares separate data splits for the `title` and `text` columns. It uses the `prepare_data` function to generate data splits for both input types, which will be used to train and evaluate the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ffx_UagwOlb2"
      },
      "outputs": [],
      "source": [
        "# Prepare data for title and text\n",
        "title_splits = prepare_data('title')\n",
        "text_splits = prepare_data('text')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PhVjMyFOtDf"
      },
      "source": [
        "# Model Training and Evaluation\n",
        "\n",
        "- This function trains and evaluates the model for each target variable. It creates the DataLoader objects for the training and testing data, sets up the model, and runs the training loop. It also evaluates the model's performance by calculating the F1 score and printing the classification report.\n",
        "\n",
        "- The training loop updates the model's weights using backpropagation, and the evaluation phase computes the model's predictions and compares them with the true labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RQYXs45FOoPz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "# Simplified and enhanced training and evaluation function\n",
        "def train_and_evaluate_nn(data_splits, targets, model_type='text'):\n",
        "    f1_scores = []  # List to store F1 scores for each task\n",
        "\n",
        "    for target in targets:\n",
        "        print(f\"\\nStarting training for task: {target}\")\n",
        "\n",
        "        # Retrieve data splits\n",
        "        X_train, X_test, y_train, y_test = data_splits[target]\n",
        "\n",
        "        # Extract text columns based on model_type\n",
        "        if model_type == 'title':\n",
        "            texts_train = X_train['title'].values\n",
        "            texts_test = X_test['title'].values\n",
        "        else:\n",
        "            texts_train = X_train['text'].values\n",
        "            texts_test = X_test['text'].values\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = TextDataset(texts_train, y_train, tokenizer, config['max_len'])\n",
        "        test_dataset = TextDataset(texts_test, y_test, tokenizer, config['max_len'])\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "        # Determine the number of labels for classification\n",
        "        num_labels = len(label_encoders[target].classes_)\n",
        "\n",
        "        # Load pre-trained model\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            config['model_name'], num_labels=num_labels\n",
        "        ).to(device)\n",
        "\n",
        "        # Optimizer and scheduler\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "        scheduler = get_scheduler(\n",
        "            \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * config['epochs']\n",
        "        )\n",
        "\n",
        "        # Class-weighted loss\n",
        "        class_counts = np.bincount(y_train)\n",
        "        class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(config['epochs']):\n",
        "            print(f\"Epoch {epoch+1}/{config['epochs']} - Training: {target}\")\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=True)\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Get inputs and labels\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                # Forward pass and loss calculation\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            print(f\"Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        # Final evaluation on the test set\n",
        "        print(\"Final evaluation on test set...\")\n",
        "        model.eval()\n",
        "        test_preds, test_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Testing\", leave=True):\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                # Get predictions\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                _, preds = torch.max(outputs.logits, dim=1)\n",
        "                test_preds.extend(preds.cpu().numpy())\n",
        "                test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate F1 score\n",
        "        test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
        "        f1_scores.append(test_f1)\n",
        "        print(f\"Final Test MACRO F1 for {target}: {test_f1:.4f}\")\n",
        "\n",
        "        # Classification report\n",
        "        print(f\"Classification Report for {target}:\\n\")\n",
        "        print(classification_report(test_labels, test_preds, zero_division=0))\n",
        "\n",
        "    return f1_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUj0YrQTO0RT"
      },
      "source": [
        "# Train and Evaluate for Title and Text Subtasks\n",
        "\n",
        "This step trains and evaluates the model separately for the `title` and `text` features. It calls the `train_and_evaluate_nn` function for both types of input (title and text) and stores the F1 scores for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OsSoAzzgOyct",
        "outputId": "fb10e812-9367-45a4-a454-a67a709b30f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Evaluating for Title Tasks:\n",
            "\n",
            "Starting training for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:37<00:00,  6.09it/s, loss=0.197]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.0711\n",
            "Epoch 2/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:36<00:00,  6.16it/s, loss=0.0181]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2811\n",
            "Epoch 3/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:36<00:00,  6.16it/s, loss=0.00643]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1485\n",
            "Epoch 4/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:36<00:00,  6.16it/s, loss=1.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0859\n",
            "Epoch 5/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:36<00:00,  6.16it/s, loss=0.00175]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0597\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:08<00:00, 18.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for hazard-category: 0.9598\n",
            "Classification Report for hazard-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97       710\n",
            "           1       0.97      0.99      0.98       703\n",
            "           2       0.99      0.98      0.99       269\n",
            "           3       0.94      0.94      0.94        18\n",
            "           4       0.98      0.91      0.94       258\n",
            "           5       0.97      0.95      0.96       248\n",
            "           7       0.93      0.97      0.95        39\n",
            "           8       0.94      1.00      0.97        75\n",
            "           9       0.92      0.96      0.94        48\n",
            "\n",
            "    accuracy                           0.97      2368\n",
            "   macro avg       0.96      0.96      0.96      2368\n",
            "weighted avg       0.97      0.97      0.97      2368\n",
            "\n",
            "\n",
            "Starting training for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:36<00:00,  6.16it/s, loss=0.892]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.9701\n",
            "Epoch 2/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:36<00:00,  6.16it/s, loss=0.521]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5209\n",
            "Epoch 3/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=0.441]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1952\n",
            "Epoch 4/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:36<00:00,  6.16it/s, loss=0.0137]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1077\n",
            "Epoch 5/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=0.00984]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0627\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:08<00:00, 18.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for product-category: 0.9710\n",
            "Classification Report for product-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        51\n",
            "           1       0.90      0.94      0.92       254\n",
            "           2       0.94      0.89      0.92        74\n",
            "           3       0.93      0.98      0.95       103\n",
            "           4       0.98      0.90      0.94        69\n",
            "           5       1.00      0.97      0.98        29\n",
            "           6       1.00      1.00      1.00         6\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      1.00      1.00         4\n",
            "           9       0.98      0.99      0.98       281\n",
            "          10       0.99      0.97      0.98        78\n",
            "          11       1.00      1.00      1.00        10\n",
            "          12       0.95      0.93      0.94        42\n",
            "          13       0.98      0.97      0.98       610\n",
            "          14       0.99      0.99      0.99       116\n",
            "          15       0.96      0.95      0.96       108\n",
            "          16       0.95      1.00      0.98        42\n",
            "          17       1.00      1.00      1.00        13\n",
            "          18       0.92      0.87      0.90       178\n",
            "          19       1.00      1.00      1.00       143\n",
            "          20       0.93      0.99      0.96       142\n",
            "          21       1.00      1.00      1.00         7\n",
            "\n",
            "    accuracy                           0.96      2368\n",
            "   macro avg       0.97      0.97      0.97      2368\n",
            "weighted avg       0.96      0.96      0.96      2368\n",
            "\n",
            "\n",
            "Starting training for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=3.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.1107\n",
            "Epoch 2/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=1.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.5019\n",
            "Epoch 3/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5878\n",
            "Epoch 4/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=1.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1085\n",
            "Epoch 5/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=0.809]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8913\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:08<00:00, 18.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for hazard: 0.8381\n",
            "Classification Report for hazard:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       0.90      1.00      0.95         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "           3       1.00      1.00      1.00         5\n",
            "           4       0.33      0.67      0.44         9\n",
            "           5       0.83      0.83      0.83        12\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       0.88      1.00      0.93         7\n",
            "           8       1.00      0.88      0.93         8\n",
            "           9       0.67      0.67      0.67         9\n",
            "          10       1.00      1.00      1.00         3\n",
            "          11       1.00      0.64      0.78        11\n",
            "          12       0.77      1.00      0.87        10\n",
            "          13       0.71      1.00      0.83        12\n",
            "          14       0.82      0.88      0.85        16\n",
            "          15       1.00      1.00      1.00        10\n",
            "          16       0.62      1.00      0.76         8\n",
            "          17       0.83      0.66      0.73        58\n",
            "          18       0.67      1.00      0.80         4\n",
            "          19       0.60      1.00      0.75         6\n",
            "          20       0.38      1.00      0.55         3\n",
            "          21       1.00      1.00      1.00        11\n",
            "          22       1.00      0.94      0.97        17\n",
            "          23       1.00      1.00      1.00         3\n",
            "          24       0.60      1.00      0.75         6\n",
            "          25       1.00      1.00      1.00         2\n",
            "          26       0.67      1.00      0.80         4\n",
            "          27       0.88      1.00      0.93         7\n",
            "          28       0.92      0.92      0.92        13\n",
            "          29       0.89      1.00      0.94        16\n",
            "          30       1.00      1.00      1.00         9\n",
            "          31       0.93      1.00      0.96        13\n",
            "          32       0.83      1.00      0.91         5\n",
            "          33       0.86      1.00      0.92        12\n",
            "          34       0.92      0.69      0.79        80\n",
            "          35       1.00      0.86      0.92         7\n",
            "          36       0.92      0.71      0.81        84\n",
            "          37       0.90      1.00      0.95         9\n",
            "          38       0.50      0.73      0.59        15\n",
            "          39       0.48      0.73      0.58        22\n",
            "          40       0.87      0.81      0.84        32\n",
            "          41       1.00      1.00      1.00         8\n",
            "          42       0.40      0.86      0.55         7\n",
            "          43       1.00      0.94      0.97        16\n",
            "          44       1.00      1.00      1.00        10\n",
            "          45       0.83      0.94      0.88        16\n",
            "          46       1.00      1.00      1.00         6\n",
            "          47       0.58      0.88      0.70         8\n",
            "          48       1.00      1.00      1.00        11\n",
            "          49       0.89      0.80      0.84        10\n",
            "          50       0.80      1.00      0.89         8\n",
            "          51       1.00      0.77      0.87        13\n",
            "          52       0.94      0.84      0.89        38\n",
            "          53       0.70      0.50      0.58        14\n",
            "          54       0.72      0.95      0.82        22\n",
            "          55       0.84      0.77      0.81       220\n",
            "          56       0.55      1.00      0.71         6\n",
            "          57       0.84      0.65      0.73        55\n",
            "          58       0.84      0.94      0.89        17\n",
            "          59       0.98      0.65      0.78       203\n",
            "          60       0.57      0.80      0.67        10\n",
            "          61       0.50      0.69      0.58        13\n",
            "          62       0.82      1.00      0.90         9\n",
            "          63       1.00      1.00      1.00        10\n",
            "          64       0.91      1.00      0.95        21\n",
            "          65       0.60      0.94      0.73        16\n",
            "          66       1.00      1.00      1.00         6\n",
            "          67       0.73      1.00      0.84         8\n",
            "          68       1.00      1.00      1.00        12\n",
            "          69       1.00      1.00      1.00         5\n",
            "          70       0.38      0.21      0.27        14\n",
            "          71       1.00      1.00      1.00         9\n",
            "          72       1.00      1.00      1.00         6\n",
            "          73       1.00      0.26      0.41        43\n",
            "          74       0.92      1.00      0.96        11\n",
            "          75       0.67      1.00      0.80        10\n",
            "          76       1.00      1.00      1.00         9\n",
            "          77       0.40      0.86      0.55         7\n",
            "          78       1.00      0.57      0.73         7\n",
            "          79       1.00      0.57      0.73         7\n",
            "          80       0.43      0.75      0.55        12\n",
            "          81       0.43      1.00      0.60         3\n",
            "          82       0.58      1.00      0.74         7\n",
            "          83       0.78      1.00      0.88         7\n",
            "          84       0.93      1.00      0.96        13\n",
            "          85       0.75      0.79      0.77        63\n",
            "          86       0.89      0.89      0.89         9\n",
            "          87       1.00      1.00      1.00         6\n",
            "          88       0.70      1.00      0.82         7\n",
            "          89       1.00      0.89      0.94         9\n",
            "          90       0.77      0.73      0.75        83\n",
            "          91       1.00      1.00      1.00        12\n",
            "          92       0.62      1.00      0.76         8\n",
            "          93       0.50      1.00      0.67         6\n",
            "          94       1.00      0.73      0.85        15\n",
            "          95       0.75      1.00      0.86         6\n",
            "          96       1.00      1.00      1.00        15\n",
            "          97       0.60      0.75      0.67        16\n",
            "          98       0.82      0.78      0.80       202\n",
            "          99       0.94      0.84      0.89        19\n",
            "         100       0.58      0.65      0.61        65\n",
            "         101       0.80      1.00      0.89         4\n",
            "         102       0.92      0.80      0.86        15\n",
            "         103       1.00      1.00      1.00         4\n",
            "         104       0.64      1.00      0.78         7\n",
            "         105       0.79      1.00      0.88        11\n",
            "         106       0.78      1.00      0.88         7\n",
            "         107       0.35      1.00      0.52         7\n",
            "         108       0.67      0.80      0.73         5\n",
            "         109       0.89      0.56      0.68        45\n",
            "         110       1.00      1.00      1.00         5\n",
            "         111       0.35      1.00      0.52         6\n",
            "         112       1.00      1.00      1.00         3\n",
            "         113       1.00      1.00      1.00        12\n",
            "         114       0.75      1.00      0.86        12\n",
            "         115       0.70      1.00      0.82         7\n",
            "         116       1.00      1.00      1.00        17\n",
            "         117       0.80      1.00      0.89         4\n",
            "         118       0.83      1.00      0.91         5\n",
            "         119       1.00      1.00      1.00        27\n",
            "         120       0.86      1.00      0.92         6\n",
            "         121       0.61      0.69      0.65        16\n",
            "         122       0.40      1.00      0.57         8\n",
            "         123       1.00      1.00      1.00         3\n",
            "         124       0.75      1.00      0.86         6\n",
            "         125       1.00      1.00      1.00         4\n",
            "         126       0.79      1.00      0.88        15\n",
            "         127       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           0.80      2368\n",
            "   macro avg       0.81      0.90      0.84      2368\n",
            "weighted avg       0.84      0.80      0.80      2368\n",
            "\n",
            "\n",
            "Starting training for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-12-d56d50d070ea>:50: RuntimeWarning: divide by zero encountered in divide\n",
            "  class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=5.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 6.0634\n",
            "Epoch 2/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=5.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 5.3235\n",
            "Epoch 3/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=4.34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.7057\n",
            "Epoch 4/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=4.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.2676\n",
            "Epoch 5/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:36<00:00,  6.15it/s, loss=3.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.0223\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:08<00:00, 18.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for product: 0.6066\n",
            "Classification Report for product:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      1.00      0.55         3\n",
            "           2       0.00      0.00      0.00         6\n",
            "           4       1.00      0.50      0.67         6\n",
            "           6       1.00      0.11      0.20         9\n",
            "           7       0.57      1.00      0.73         4\n",
            "           8       1.00      1.00      1.00        12\n",
            "           9       1.00      1.00      1.00         6\n",
            "          10       0.86      1.00      0.92         6\n",
            "          11       1.00      1.00      1.00        11\n",
            "          12       0.54      1.00      0.70         7\n",
            "          13       0.33      1.00      0.50         4\n",
            "          14       1.00      0.40      0.57         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          17       0.33      0.33      0.33         3\n",
            "          18       1.00      0.83      0.91         6\n",
            "          19       0.80      1.00      0.89        16\n",
            "          20       1.00      1.00      1.00         1\n",
            "          21       1.00      0.33      0.50         3\n",
            "          22       0.50      0.60      0.55         5\n",
            "          23       0.73      1.00      0.84         8\n",
            "          24       1.00      1.00      1.00         2\n",
            "          25       0.50      0.20      0.29        10\n",
            "          26       0.89      1.00      0.94         8\n",
            "          27       0.00      0.00      0.00         2\n",
            "          29       0.00      0.00      0.00         1\n",
            "          31       0.58      1.00      0.74         7\n",
            "          32       0.20      0.50      0.29         2\n",
            "          33       1.00      1.00      1.00         7\n",
            "          34       0.00      0.00      0.00         4\n",
            "          35       1.00      1.00      1.00         5\n",
            "          36       0.40      1.00      0.57         4\n",
            "          38       0.00      0.00      0.00        11\n",
            "          39       0.70      1.00      0.82         7\n",
            "          40       0.00      0.00      0.00         6\n",
            "          41       1.00      1.00      1.00         1\n",
            "          42       0.08      1.00      0.15         2\n",
            "          43       0.39      0.90      0.55        10\n",
            "          44       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         3\n",
            "          46       1.00      1.00      1.00         6\n",
            "          47       0.00      0.00      0.00         1\n",
            "          48       1.00      1.00      1.00         2\n",
            "          49       0.22      0.29      0.25         7\n",
            "          50       1.00      1.00      1.00        12\n",
            "          52       0.00      0.00      0.00         1\n",
            "          53       0.80      0.67      0.73         6\n",
            "          54       1.00      1.00      1.00         1\n",
            "          55       0.78      1.00      0.88         7\n",
            "          56       0.50      0.33      0.40         3\n",
            "          57       1.00      0.25      0.40         4\n",
            "          58       0.00      0.00      0.00         5\n",
            "          59       1.00      0.25      0.40         4\n",
            "          60       0.31      0.80      0.44         5\n",
            "          62       1.00      0.57      0.73         7\n",
            "          63       0.00      0.00      0.00         3\n",
            "          64       0.83      1.00      0.91         5\n",
            "          65       1.00      1.00      1.00         2\n",
            "          66       0.46      1.00      0.63         6\n",
            "          67       0.27      1.00      0.42         4\n",
            "          68       0.00      0.00      0.00        19\n",
            "          69       0.00      0.00      0.00         3\n",
            "          71       1.00      1.00      1.00         6\n",
            "          72       0.00      0.00      0.00         1\n",
            "          73       0.33      1.00      0.50         3\n",
            "          74       1.00      1.00      1.00         6\n",
            "          75       0.00      0.00      0.00         1\n",
            "          77       0.50      0.50      0.50         2\n",
            "          78       0.67      0.50      0.57         4\n",
            "          79       0.58      1.00      0.74         7\n",
            "          80       0.67      0.33      0.44         6\n",
            "          81       1.00      0.33      0.50         3\n",
            "          82       0.00      0.00      0.00         4\n",
            "          84       1.00      0.25      0.40         8\n",
            "          85       0.00      0.00      0.00         8\n",
            "          86       1.00      1.00      1.00         8\n",
            "          87       0.00      0.00      0.00         4\n",
            "          88       1.00      0.60      0.75         5\n",
            "          89       0.25      1.00      0.40         2\n",
            "          90       1.00      1.00      1.00         2\n",
            "          91       0.00      0.00      0.00         9\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.12      0.04      0.06        23\n",
            "          94       1.00      0.07      0.12        15\n",
            "          95       1.00      1.00      1.00         8\n",
            "          96       0.57      1.00      0.73         8\n",
            "          97       0.26      1.00      0.42         5\n",
            "          98       0.77      1.00      0.87        10\n",
            "          99       1.00      1.00      1.00         3\n",
            "         100       0.00      0.00      0.00         8\n",
            "         101       0.25      0.40      0.31         5\n",
            "         104       0.83      1.00      0.91         5\n",
            "         107       1.00      0.20      0.33         5\n",
            "         109       1.00      1.00      1.00         5\n",
            "         110       0.30      1.00      0.47         7\n",
            "         111       0.00      0.00      0.00         0\n",
            "         112       0.00      0.00      0.00         4\n",
            "         113       1.00      0.27      0.43        11\n",
            "         114       1.00      0.78      0.88         9\n",
            "         116       1.00      0.75      0.86         4\n",
            "         117       0.00      0.00      0.00         1\n",
            "         118       0.00      0.00      0.00         1\n",
            "         119       1.00      1.00      1.00         1\n",
            "         120       1.00      0.12      0.22         8\n",
            "         121       0.00      0.00      0.00         1\n",
            "         122       0.75      1.00      0.86         6\n",
            "         123       1.00      0.78      0.88         9\n",
            "         124       1.00      1.00      1.00         5\n",
            "         126       1.00      1.00      1.00        10\n",
            "         127       1.00      1.00      1.00         4\n",
            "         129       0.62      1.00      0.76         8\n",
            "         130       0.50      0.50      0.50         2\n",
            "         131       1.00      1.00      1.00         3\n",
            "         132       0.45      1.00      0.62         5\n",
            "         133       0.88      1.00      0.93         7\n",
            "         134       0.67      1.00      0.80         2\n",
            "         135       1.00      1.00      1.00         6\n",
            "         136       1.00      0.17      0.29         6\n",
            "         138       0.00      0.00      0.00         1\n",
            "         139       1.00      1.00      1.00         3\n",
            "         141       1.00      0.17      0.29         6\n",
            "         142       0.00      0.00      0.00         5\n",
            "         143       0.78      1.00      0.88         7\n",
            "         145       0.50      1.00      0.67        11\n",
            "         146       0.00      0.00      0.00         8\n",
            "         147       1.00      0.50      0.67         4\n",
            "         148       0.00      0.00      0.00         7\n",
            "         150       1.00      0.33      0.50         3\n",
            "         151       1.00      1.00      1.00         4\n",
            "         152       0.00      0.00      0.00         4\n",
            "         153       0.00      0.00      0.00         1\n",
            "         154       0.75      1.00      0.86         3\n",
            "         155       1.00      0.67      0.80         9\n",
            "         156       0.86      1.00      0.92         6\n",
            "         158       1.00      1.00      1.00         5\n",
            "         159       0.50      1.00      0.67         4\n",
            "         160       1.00      0.88      0.93         8\n",
            "         162       0.50      1.00      0.67         4\n",
            "         163       1.00      1.00      1.00         2\n",
            "         164       0.50      0.12      0.20         8\n",
            "         166       1.00      0.22      0.36         9\n",
            "         168       0.00      0.00      0.00         1\n",
            "         169       1.00      0.56      0.72        16\n",
            "         170       0.67      1.00      0.80         2\n",
            "         171       0.00      0.00      0.00         6\n",
            "         172       1.00      1.00      1.00         1\n",
            "         173       1.00      0.71      0.83         7\n",
            "         174       0.71      1.00      0.83         5\n",
            "         176       0.00      0.00      0.00         1\n",
            "         177       0.80      0.67      0.73        12\n",
            "         178       0.80      1.00      0.89         4\n",
            "         180       1.00      1.00      1.00         6\n",
            "         181       0.79      1.00      0.88        11\n",
            "         182       1.00      0.75      0.86         4\n",
            "         183       1.00      1.00      1.00         4\n",
            "         184       0.83      1.00      0.91         5\n",
            "         187       0.50      1.00      0.67         1\n",
            "         189       1.00      0.57      0.73         7\n",
            "         190       0.00      0.00      0.00        12\n",
            "         192       1.00      0.70      0.82        10\n",
            "         193       1.00      1.00      1.00         5\n",
            "         194       1.00      1.00      1.00         3\n",
            "         195       1.00      1.00      1.00         8\n",
            "         197       1.00      1.00      1.00         6\n",
            "         198       0.67      1.00      0.80        10\n",
            "         199       0.60      0.75      0.67         4\n",
            "         200       1.00      0.67      0.80         6\n",
            "         201       0.45      0.83      0.59         6\n",
            "         202       0.33      1.00      0.50         3\n",
            "         204       1.00      1.00      1.00         1\n",
            "         206       1.00      1.00      1.00         7\n",
            "         207       1.00      0.14      0.25         7\n",
            "         208       1.00      0.50      0.67         6\n",
            "         209       0.00      0.00      0.00         1\n",
            "         210       1.00      1.00      1.00         3\n",
            "         211       0.00      0.00      0.00         1\n",
            "         212       0.71      0.83      0.77         6\n",
            "         213       0.28      1.00      0.43         5\n",
            "         215       1.00      1.00      1.00         5\n",
            "         216       0.83      0.83      0.83         6\n",
            "         217       0.67      1.00      0.80         4\n",
            "         218       0.00      0.00      0.00         6\n",
            "         219       1.00      0.88      0.93         8\n",
            "         220       0.90      0.90      0.90        10\n",
            "         221       1.00      1.00      1.00         3\n",
            "         223       0.62      1.00      0.77         5\n",
            "         224       0.92      1.00      0.96        11\n",
            "         225       1.00      1.00      1.00         7\n",
            "         226       0.00      0.00      0.00         1\n",
            "         227       0.00      0.00      0.00         1\n",
            "         228       1.00      0.50      0.67         6\n",
            "         229       0.00      0.00      0.00         4\n",
            "         230       0.40      0.40      0.40         5\n",
            "         231       0.30      1.00      0.47         7\n",
            "         232       1.00      1.00      1.00         3\n",
            "         233       0.67      1.00      0.80         8\n",
            "         234       0.00      0.00      0.00         0\n",
            "         236       0.00      0.00      0.00         2\n",
            "         237       1.00      1.00      1.00         2\n",
            "         238       0.86      0.75      0.80         8\n",
            "         239       0.86      1.00      0.92         6\n",
            "         240       1.00      1.00      1.00         5\n",
            "         241       1.00      0.25      0.40         4\n",
            "         242       0.78      1.00      0.88         7\n",
            "         243       0.33      1.00      0.50         2\n",
            "         244       0.50      1.00      0.67         5\n",
            "         245       0.75      1.00      0.86         6\n",
            "         247       1.00      0.60      0.75         5\n",
            "         248       0.33      1.00      0.50         4\n",
            "         249       1.00      0.60      0.75         5\n",
            "         250       1.00      1.00      1.00         1\n",
            "         251       1.00      1.00      1.00         4\n",
            "         253       1.00      1.00      1.00         4\n",
            "         255       1.00      1.00      1.00         9\n",
            "         256       0.00      0.00      0.00         1\n",
            "         257       0.78      1.00      0.88         7\n",
            "         258       0.00      0.00      0.00         9\n",
            "         259       0.48      1.00      0.65        10\n",
            "         260       1.00      1.00      1.00         4\n",
            "         261       1.00      0.86      0.92         7\n",
            "         262       1.00      1.00      1.00         7\n",
            "         263       1.00      0.20      0.33         5\n",
            "         264       1.00      1.00      1.00         2\n",
            "         265       0.00      0.00      0.00         0\n",
            "         266       1.00      0.57      0.73         7\n",
            "         267       0.83      0.83      0.83         6\n",
            "         268       0.67      1.00      0.80         2\n",
            "         270       1.00      1.00      1.00         7\n",
            "         271       0.88      0.76      0.81        29\n",
            "         273       1.00      1.00      1.00        11\n",
            "         274       0.40      1.00      0.57         4\n",
            "         275       1.00      0.50      0.67         4\n",
            "         276       1.00      0.43      0.60         7\n",
            "         277       0.65      0.87      0.74        15\n",
            "         278       1.00      1.00      1.00         7\n",
            "         279       1.00      1.00      1.00         4\n",
            "         280       0.82      1.00      0.90         9\n",
            "         281       1.00      0.40      0.57         5\n",
            "         282       0.50      0.80      0.62         5\n",
            "         283       1.00      1.00      1.00         1\n",
            "         284       1.00      1.00      1.00         4\n",
            "         285       0.33      1.00      0.50         4\n",
            "         287       1.00      1.00      1.00         2\n",
            "         288       0.47      1.00      0.64         7\n",
            "         289       1.00      1.00      1.00         2\n",
            "         290       1.00      1.00      1.00         2\n",
            "         291       0.57      1.00      0.73         4\n",
            "         292       0.75      1.00      0.86         3\n",
            "         293       0.75      1.00      0.86         6\n",
            "         294       0.50      1.00      0.67         1\n",
            "         295       0.00      0.00      0.00         1\n",
            "         296       1.00      0.50      0.67         8\n",
            "         297       0.75      1.00      0.86         3\n",
            "         298       1.00      0.33      0.50         3\n",
            "         299       0.00      0.00      0.00         1\n",
            "         300       1.00      0.67      0.80         6\n",
            "         301       0.50      0.33      0.40         6\n",
            "         303       0.11      1.00      0.20         3\n",
            "         304       0.71      1.00      0.83         5\n",
            "         305       1.00      0.36      0.53        14\n",
            "         307       0.60      0.43      0.50         7\n",
            "         308       0.25      0.57      0.35         7\n",
            "         309       0.00      0.00      0.00         2\n",
            "         310       1.00      0.60      0.75         5\n",
            "         311       0.40      1.00      0.57         4\n",
            "         312       1.00      1.00      1.00         2\n",
            "         313       0.89      1.00      0.94         8\n",
            "         314       0.56      1.00      0.71         5\n",
            "         316       1.00      0.50      0.67         8\n",
            "         318       0.86      1.00      0.92         6\n",
            "         320       0.42      1.00      0.59         5\n",
            "         321       1.00      0.20      0.33         5\n",
            "         322       1.00      0.33      0.50         3\n",
            "         323       1.00      1.00      1.00         7\n",
            "         324       0.10      1.00      0.18         1\n",
            "         325       0.67      1.00      0.80         4\n",
            "         326       1.00      1.00      1.00         4\n",
            "         327       0.00      0.00      0.00         1\n",
            "         328       1.00      1.00      1.00         5\n",
            "         329       0.80      1.00      0.89         4\n",
            "         330       0.29      1.00      0.44         4\n",
            "         331       0.50      0.67      0.57         6\n",
            "         335       1.00      1.00      1.00         5\n",
            "         336       0.00      0.00      0.00         1\n",
            "         337       0.20      1.00      0.33         1\n",
            "         338       0.82      1.00      0.90         9\n",
            "         339       1.00      1.00      1.00        10\n",
            "         340       0.62      1.00      0.77         5\n",
            "         341       0.00      0.00      0.00         0\n",
            "         342       1.00      1.00      1.00         2\n",
            "         343       1.00      1.00      1.00         3\n",
            "         346       1.00      0.56      0.71         9\n",
            "         347       1.00      1.00      1.00         2\n",
            "         348       1.00      1.00      1.00         1\n",
            "         349       0.00      0.00      0.00         3\n",
            "         350       1.00      0.14      0.25         7\n",
            "         351       0.90      1.00      0.95         9\n",
            "         352       0.77      1.00      0.87        10\n",
            "         353       0.89      0.89      0.89         9\n",
            "         354       0.00      0.00      0.00         7\n",
            "         355       1.00      1.00      1.00         5\n",
            "         357       0.00      0.00      0.00         4\n",
            "         358       1.00      0.20      0.33         5\n",
            "         359       0.60      0.75      0.67         4\n",
            "         361       0.60      0.60      0.60         5\n",
            "         362       1.00      0.29      0.44         7\n",
            "         363       1.00      0.29      0.44         7\n",
            "         364       1.00      1.00      1.00         3\n",
            "         365       0.60      0.60      0.60         5\n",
            "         366       1.00      1.00      1.00         1\n",
            "         367       1.00      1.00      1.00         3\n",
            "         370       1.00      0.50      0.67         4\n",
            "         371       0.85      0.85      0.85        13\n",
            "         373       0.29      0.29      0.29         7\n",
            "         374       1.00      0.50      0.67         4\n",
            "         375       0.00      0.00      0.00         7\n",
            "         376       0.80      1.00      0.89         4\n",
            "         377       0.00      0.00      0.00         1\n",
            "         378       0.00      0.00      0.00         1\n",
            "         379       0.57      1.00      0.73         4\n",
            "         381       0.71      0.50      0.59        10\n",
            "         382       0.00      0.00      0.00         7\n",
            "         383       0.83      1.00      0.91         5\n",
            "         384       0.83      0.83      0.83         6\n",
            "         385       1.00      0.67      0.80         3\n",
            "         386       1.00      1.00      1.00         5\n",
            "         387       0.86      0.60      0.71        10\n",
            "         388       1.00      1.00      1.00         1\n",
            "         389       0.67      0.75      0.71         8\n",
            "         390       0.00      0.00      0.00         3\n",
            "         391       0.25      0.40      0.31         5\n",
            "         392       0.50      0.20      0.29         5\n",
            "         393       0.83      0.71      0.77         7\n",
            "         394       0.00      0.00      0.00         3\n",
            "         395       0.00      0.00      0.00         6\n",
            "         396       0.00      0.00      0.00         1\n",
            "         397       1.00      1.00      1.00        11\n",
            "         398       0.55      1.00      0.71         6\n",
            "         399       1.00      1.00      1.00         9\n",
            "         400       0.33      1.00      0.50         5\n",
            "         401       0.25      0.12      0.17         8\n",
            "         402       1.00      1.00      1.00         1\n",
            "         403       0.56      1.00      0.71         5\n",
            "         404       0.67      0.25      0.36         8\n",
            "         405       0.80      1.00      0.89         4\n",
            "         406       1.00      1.00      1.00        10\n",
            "         407       1.00      0.60      0.75         5\n",
            "         408       1.00      1.00      1.00         8\n",
            "         409       0.73      1.00      0.84         8\n",
            "         410       0.62      1.00      0.77         5\n",
            "         411       1.00      1.00      1.00         2\n",
            "         412       0.50      0.60      0.55         5\n",
            "         413       0.39      1.00      0.56         7\n",
            "         414       0.47      0.78      0.58         9\n",
            "         415       0.00      0.00      0.00        11\n",
            "         419       1.00      0.50      0.67         4\n",
            "         421       0.25      1.00      0.40         3\n",
            "         422       0.83      0.62      0.71         8\n",
            "         423       0.83      0.62      0.71         8\n",
            "         424       0.80      1.00      0.89         4\n",
            "         425       0.00      0.00      0.00         1\n",
            "         426       1.00      0.25      0.40         4\n",
            "         427       1.00      0.30      0.46        10\n",
            "         428       0.00      0.00      0.00         4\n",
            "         429       0.00      0.00      0.00         1\n",
            "         430       0.25      1.00      0.40         3\n",
            "         431       1.00      1.00      1.00         9\n",
            "         432       0.00      0.00      0.00         1\n",
            "         433       1.00      1.00      1.00         8\n",
            "         434       1.00      0.20      0.33         5\n",
            "         436       0.00      0.00      0.00         5\n",
            "         437       0.00      0.00      0.00         6\n",
            "         438       0.00      0.00      0.00        12\n",
            "         439       0.56      0.62      0.59         8\n",
            "         441       1.00      1.00      1.00         5\n",
            "         443       0.00      0.00      0.00         5\n",
            "         444       1.00      0.56      0.71         9\n",
            "         445       1.00      1.00      1.00         7\n",
            "         446       0.80      0.67      0.73         6\n",
            "         447       0.57      1.00      0.73         4\n",
            "         448       0.00      0.00      0.00         1\n",
            "         449       1.00      0.78      0.88         9\n",
            "         450       0.75      0.75      0.75         4\n",
            "         451       1.00      0.25      0.40         8\n",
            "         452       1.00      1.00      1.00         7\n",
            "         453       1.00      0.50      0.67         2\n",
            "         455       0.00      0.00      0.00         1\n",
            "         457       0.35      1.00      0.52         6\n",
            "         458       0.62      1.00      0.77         5\n",
            "         459       1.00      0.71      0.83         7\n",
            "         461       0.86      1.00      0.92         6\n",
            "         464       0.00      0.00      0.00         1\n",
            "         465       0.83      1.00      0.91         5\n",
            "         467       0.58      1.00      0.74         7\n",
            "         468       1.00      1.00      1.00         7\n",
            "         469       0.80      1.00      0.89         4\n",
            "         470       0.80      1.00      0.89         4\n",
            "         471       1.00      0.20      0.33         5\n",
            "         472       0.43      1.00      0.60        13\n",
            "         473       1.00      0.50      0.67         4\n",
            "         474       1.00      1.00      1.00         7\n",
            "         475       0.33      1.00      0.50         1\n",
            "         476       1.00      0.71      0.83         7\n",
            "         479       1.00      1.00      1.00         6\n",
            "         481       1.00      1.00      1.00         1\n",
            "         482       0.00      0.00      0.00         2\n",
            "         483       0.83      1.00      0.91         5\n",
            "         484       0.86      0.55      0.67        11\n",
            "         487       0.00      0.00      0.00         5\n",
            "         488       1.00      0.31      0.47        13\n",
            "         489       1.00      1.00      1.00         2\n",
            "         490       0.89      1.00      0.94         8\n",
            "         491       0.83      0.71      0.77         7\n",
            "         492       1.00      0.17      0.29         6\n",
            "         494       1.00      0.86      0.92         7\n",
            "         495       0.88      1.00      0.93         7\n",
            "         496       0.75      1.00      0.86        12\n",
            "         497       1.00      1.00      1.00         6\n",
            "         498       1.00      0.25      0.40         4\n",
            "         499       0.70      1.00      0.82         7\n",
            "         500       1.00      1.00      1.00         8\n",
            "         503       0.80      1.00      0.89         4\n",
            "         504       1.00      1.00      1.00         3\n",
            "         505       0.42      1.00      0.59         8\n",
            "         506       0.41      1.00      0.58         7\n",
            "         507       0.00      0.00      0.00         4\n",
            "         508       1.00      0.45      0.62        11\n",
            "         509       0.73      1.00      0.84         8\n",
            "         510       1.00      1.00      1.00         1\n",
            "         511       0.56      1.00      0.71         5\n",
            "         512       0.43      0.75      0.55         4\n",
            "         513       0.67      1.00      0.80         2\n",
            "         514       1.00      1.00      1.00         3\n",
            "         515       0.00      0.00      0.00         1\n",
            "         516       0.33      1.00      0.50         3\n",
            "         518       1.00      1.00      1.00         1\n",
            "         520       1.00      0.50      0.67        12\n",
            "         521       1.00      0.50      0.67         2\n",
            "         522       1.00      0.29      0.44         7\n",
            "\n",
            "    accuracy                           0.68      2368\n",
            "   macro avg       0.65      0.66      0.61      2368\n",
            "weighted avg       0.71      0.68      0.64      2368\n",
            "\n",
            "\n",
            "Training and Evaluating for Text Tasks:\n",
            "\n",
            "Starting training for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=0.0721]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.0304\n",
            "Epoch 2/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=0.0263]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2641\n",
            "Epoch 3/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=0.00295]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1224\n",
            "Epoch 4/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=0.0869]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0731\n",
            "Epoch 5/5 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=0.00222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0423\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:09<00:00, 16.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for hazard-category: 0.9566\n",
            "Classification Report for hazard-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       710\n",
            "           1       0.98      0.98      0.98       703\n",
            "           2       1.00      0.99      0.99       269\n",
            "           3       1.00      0.89      0.94        18\n",
            "           4       0.97      0.95      0.96       258\n",
            "           5       0.94      0.94      0.94       248\n",
            "           7       0.92      0.92      0.92        39\n",
            "           8       0.92      0.95      0.93        75\n",
            "           9       0.96      0.96      0.96        48\n",
            "\n",
            "    accuracy                           0.97      2368\n",
            "   macro avg       0.96      0.95      0.96      2368\n",
            "weighted avg       0.97      0.97      0.97      2368\n",
            "\n",
            "\n",
            "Starting training for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=1.88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.4608\n",
            "Epoch 2/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=0.431]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.9814\n",
            "Epoch 3/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=0.292]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3720\n",
            "Epoch 4/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=0.298]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1873\n",
            "Epoch 5/5 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=0.428]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1125\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:09<00:00, 15.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for product-category: 0.9393\n",
            "Classification Report for product-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        51\n",
            "           1       0.88      0.88      0.88       254\n",
            "           2       0.90      0.93      0.91        74\n",
            "           3       0.88      0.97      0.92       103\n",
            "           4       0.78      0.88      0.83        69\n",
            "           5       0.97      0.97      0.97        29\n",
            "           6       1.00      1.00      1.00         6\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      1.00      1.00         4\n",
            "           9       0.93      0.94      0.93       281\n",
            "          10       0.91      0.95      0.93        78\n",
            "          11       1.00      1.00      1.00        10\n",
            "          12       0.90      0.83      0.86        42\n",
            "          13       0.94      0.92      0.93       610\n",
            "          14       0.97      0.97      0.97       116\n",
            "          15       0.91      0.93      0.92       108\n",
            "          16       1.00      0.98      0.99        42\n",
            "          17       1.00      0.77      0.87        13\n",
            "          18       0.87      0.85      0.86       178\n",
            "          19       0.98      0.97      0.97       143\n",
            "          20       0.93      0.91      0.92       142\n",
            "          21       1.00      1.00      1.00         7\n",
            "\n",
            "    accuracy                           0.92      2368\n",
            "   macro avg       0.94      0.94      0.94      2368\n",
            "weighted avg       0.92      0.92      0.92      2368\n",
            "\n",
            "\n",
            "Starting training for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=3.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.3663\n",
            "Epoch 2/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=2.67]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.0946\n",
            "Epoch 3/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=1.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.1151\n",
            "Epoch 4/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=0.855]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4896\n",
            "Epoch 5/5 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:40<00:00,  5.88it/s, loss=1.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1951\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:09<00:00, 15.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for hazard: 0.8370\n",
            "Classification Report for hazard:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "           3       1.00      1.00      1.00         5\n",
            "           4       0.88      0.78      0.82         9\n",
            "           5       0.80      1.00      0.89        12\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       0.88      1.00      0.93         7\n",
            "           8       1.00      1.00      1.00         8\n",
            "           9       0.47      0.78      0.58         9\n",
            "          10       1.00      1.00      1.00         3\n",
            "          11       0.73      0.73      0.73        11\n",
            "          12       1.00      1.00      1.00        10\n",
            "          13       0.53      0.75      0.62        12\n",
            "          14       1.00      0.62      0.77        16\n",
            "          15       0.91      1.00      0.95        10\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       0.84      0.83      0.83        58\n",
            "          18       0.67      1.00      0.80         4\n",
            "          19       0.75      1.00      0.86         6\n",
            "          20       1.00      1.00      1.00         3\n",
            "          21       1.00      1.00      1.00        11\n",
            "          22       0.94      0.94      0.94        17\n",
            "          23       0.60      1.00      0.75         3\n",
            "          24       0.67      1.00      0.80         6\n",
            "          25       0.40      1.00      0.57         2\n",
            "          26       1.00      1.00      1.00         4\n",
            "          27       0.70      1.00      0.82         7\n",
            "          28       1.00      0.92      0.96        13\n",
            "          29       1.00      1.00      1.00        16\n",
            "          30       0.82      1.00      0.90         9\n",
            "          31       0.81      1.00      0.90        13\n",
            "          32       1.00      1.00      1.00         5\n",
            "          33       1.00      0.92      0.96        12\n",
            "          34       0.92      0.76      0.84        80\n",
            "          35       1.00      1.00      1.00         7\n",
            "          36       0.90      0.79      0.84        84\n",
            "          37       1.00      1.00      1.00         9\n",
            "          38       0.93      0.87      0.90        15\n",
            "          39       0.65      0.59      0.62        22\n",
            "          40       0.87      0.81      0.84        32\n",
            "          41       1.00      1.00      1.00         8\n",
            "          42       0.75      0.86      0.80         7\n",
            "          43       0.89      1.00      0.94        16\n",
            "          44       1.00      1.00      1.00        10\n",
            "          45       0.94      0.94      0.94        16\n",
            "          46       1.00      1.00      1.00         6\n",
            "          47       0.33      0.25      0.29         8\n",
            "          48       0.73      1.00      0.85        11\n",
            "          49       0.83      1.00      0.91        10\n",
            "          50       0.80      1.00      0.89         8\n",
            "          51       0.72      1.00      0.84        13\n",
            "          52       0.28      0.92      0.43        38\n",
            "          53       1.00      0.43      0.60        14\n",
            "          54       0.68      0.68      0.68        22\n",
            "          55       0.97      0.79      0.87       220\n",
            "          56       0.67      1.00      0.80         6\n",
            "          57       0.89      0.85      0.87        55\n",
            "          58       0.93      0.76      0.84        17\n",
            "          59       0.95      0.85      0.89       203\n",
            "          60       1.00      0.60      0.75        10\n",
            "          61       0.62      0.62      0.62        13\n",
            "          62       0.90      1.00      0.95         9\n",
            "          63       0.91      1.00      0.95        10\n",
            "          64       0.95      0.86      0.90        21\n",
            "          65       0.80      1.00      0.89        16\n",
            "          66       1.00      1.00      1.00         6\n",
            "          67       0.67      1.00      0.80         8\n",
            "          68       0.92      1.00      0.96        12\n",
            "          69       0.83      1.00      0.91         5\n",
            "          70       0.60      0.21      0.32        14\n",
            "          71       1.00      1.00      1.00         9\n",
            "          72       1.00      1.00      1.00         6\n",
            "          73       1.00      0.26      0.41        43\n",
            "          74       1.00      1.00      1.00        11\n",
            "          75       0.90      0.90      0.90        10\n",
            "          76       0.90      1.00      0.95         9\n",
            "          77       0.58      1.00      0.74         7\n",
            "          78       0.75      0.43      0.55         7\n",
            "          79       1.00      1.00      1.00         7\n",
            "          80       0.89      0.67      0.76        12\n",
            "          81       0.67      0.67      0.67         3\n",
            "          82       0.58      1.00      0.74         7\n",
            "          83       0.60      0.86      0.71         7\n",
            "          84       0.72      1.00      0.84        13\n",
            "          85       0.88      0.92      0.90        63\n",
            "          86       1.00      0.67      0.80         9\n",
            "          87       0.86      1.00      0.92         6\n",
            "          88       0.83      0.71      0.77         7\n",
            "          89       0.90      1.00      0.95         9\n",
            "          90       0.88      0.51      0.64        83\n",
            "          91       0.86      1.00      0.92        12\n",
            "          92       0.53      1.00      0.70         8\n",
            "          93       0.83      0.83      0.83         6\n",
            "          94       0.57      0.80      0.67        15\n",
            "          95       0.86      1.00      0.92         6\n",
            "          96       0.65      1.00      0.79        15\n",
            "          97       0.40      0.12      0.19        16\n",
            "          98       0.96      0.82      0.88       202\n",
            "          99       0.86      1.00      0.93        19\n",
            "         100       0.73      0.54      0.62        65\n",
            "         101       0.80      1.00      0.89         4\n",
            "         102       1.00      0.67      0.80        15\n",
            "         103       0.19      1.00      0.32         4\n",
            "         104       1.00      1.00      1.00         7\n",
            "         105       0.85      1.00      0.92        11\n",
            "         106       0.78      1.00      0.88         7\n",
            "         107       0.67      0.86      0.75         7\n",
            "         108       0.67      0.80      0.73         5\n",
            "         109       0.89      0.93      0.91        45\n",
            "         110       0.71      1.00      0.83         5\n",
            "         111       0.67      1.00      0.80         6\n",
            "         112       0.60      1.00      0.75         3\n",
            "         113       1.00      1.00      1.00        12\n",
            "         114       0.67      0.83      0.74        12\n",
            "         115       0.64      1.00      0.78         7\n",
            "         116       0.77      1.00      0.87        17\n",
            "         117       1.00      1.00      1.00         4\n",
            "         118       1.00      1.00      1.00         5\n",
            "         119       0.87      1.00      0.93        27\n",
            "         120       0.75      1.00      0.86         6\n",
            "         121       0.92      0.69      0.79        16\n",
            "         122       0.62      1.00      0.76         8\n",
            "         123       0.33      1.00      0.50         3\n",
            "         124       1.00      1.00      1.00         6\n",
            "         125       1.00      1.00      1.00         4\n",
            "         126       0.62      1.00      0.77        15\n",
            "         127       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           0.82      2368\n",
            "   macro avg       0.82      0.89      0.84      2368\n",
            "weighted avg       0.87      0.82      0.83      2368\n",
            "\n",
            "\n",
            "Starting training for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-12-d56d50d070ea>:50: RuntimeWarning: divide by zero encountered in divide\n",
            "  class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=5.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 6.1497\n",
            "Epoch 2/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=5.75]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 5.5286\n",
            "Epoch 3/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=4.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.9850\n",
            "Epoch 4/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=4.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.5899\n",
            "Epoch 5/5 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 592/592 [01:40<00:00,  5.87it/s, loss=4.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.3606\n",
            "Final evaluation on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 148/148 [00:09<00:00, 15.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MACRO F1 for product: 0.5500\n",
            "Classification Report for product:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.67      0.80         3\n",
            "           2       0.33      0.33      0.33         6\n",
            "           4       0.67      0.67      0.67         6\n",
            "           6       1.00      0.44      0.62         9\n",
            "           7       1.00      0.25      0.40         4\n",
            "           8       0.92      1.00      0.96        12\n",
            "           9       1.00      0.17      0.29         6\n",
            "          10       1.00      0.67      0.80         6\n",
            "          11       0.85      1.00      0.92        11\n",
            "          12       0.78      1.00      0.88         7\n",
            "          13       1.00      0.75      0.86         4\n",
            "          14       1.00      0.20      0.33         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          17       1.00      0.67      0.80         3\n",
            "          18       0.00      0.00      0.00         6\n",
            "          19       1.00      0.75      0.86        16\n",
            "          20       1.00      1.00      1.00         1\n",
            "          21       0.00      0.00      0.00         3\n",
            "          22       1.00      0.20      0.33         5\n",
            "          23       1.00      1.00      1.00         8\n",
            "          24       1.00      1.00      1.00         2\n",
            "          25       1.00      0.10      0.18        10\n",
            "          26       1.00      0.38      0.55         8\n",
            "          27       0.00      0.00      0.00         2\n",
            "          29       0.00      0.00      0.00         1\n",
            "          31       0.50      1.00      0.67         7\n",
            "          32       0.33      0.50      0.40         2\n",
            "          33       0.70      1.00      0.82         7\n",
            "          34       0.00      0.00      0.00         4\n",
            "          35       0.42      1.00      0.59         5\n",
            "          36       1.00      1.00      1.00         4\n",
            "          38       0.33      0.18      0.24        11\n",
            "          39       1.00      1.00      1.00         7\n",
            "          40       0.00      0.00      0.00         6\n",
            "          41       1.00      1.00      1.00         1\n",
            "          42       0.29      1.00      0.44         2\n",
            "          43       1.00      0.50      0.67        10\n",
            "          44       1.00      1.00      1.00         1\n",
            "          45       0.00      0.00      0.00         3\n",
            "          46       0.55      1.00      0.71         6\n",
            "          47       0.00      0.00      0.00         1\n",
            "          48       1.00      1.00      1.00         2\n",
            "          49       1.00      0.14      0.25         7\n",
            "          50       1.00      1.00      1.00        12\n",
            "          51       0.00      0.00      0.00         0\n",
            "          52       0.00      0.00      0.00         1\n",
            "          53       0.62      0.83      0.71         6\n",
            "          54       1.00      1.00      1.00         1\n",
            "          55       0.42      0.71      0.53         7\n",
            "          56       0.00      0.00      0.00         3\n",
            "          57       0.00      0.00      0.00         4\n",
            "          58       0.00      0.00      0.00         5\n",
            "          59       1.00      0.75      0.86         4\n",
            "          60       1.00      0.20      0.33         5\n",
            "          62       0.64      1.00      0.78         7\n",
            "          63       1.00      0.33      0.50         3\n",
            "          64       1.00      0.60      0.75         5\n",
            "          65       0.50      1.00      0.67         2\n",
            "          66       1.00      1.00      1.00         6\n",
            "          67       0.21      1.00      0.35         4\n",
            "          68       0.67      0.11      0.18        19\n",
            "          69       0.00      0.00      0.00         3\n",
            "          71       1.00      1.00      1.00         6\n",
            "          72       0.00      0.00      0.00         1\n",
            "          73       0.60      1.00      0.75         3\n",
            "          74       0.50      1.00      0.67         6\n",
            "          75       0.00      0.00      0.00         1\n",
            "          77       0.08      0.50      0.13         2\n",
            "          78       1.00      0.50      0.67         4\n",
            "          79       0.47      1.00      0.64         7\n",
            "          80       0.25      0.33      0.29         6\n",
            "          81       1.00      0.67      0.80         3\n",
            "          82       0.00      0.00      0.00         4\n",
            "          84       1.00      0.25      0.40         8\n",
            "          85       0.00      0.00      0.00         8\n",
            "          86       0.62      1.00      0.76         8\n",
            "          87       0.31      1.00      0.47         4\n",
            "          88       1.00      0.60      0.75         5\n",
            "          89       1.00      1.00      1.00         2\n",
            "          90       0.15      1.00      0.27         2\n",
            "          91       0.00      0.00      0.00         9\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.67      0.17      0.28        23\n",
            "          94       0.00      0.00      0.00        15\n",
            "          95       0.24      1.00      0.38         8\n",
            "          96       1.00      1.00      1.00         8\n",
            "          97       0.11      0.40      0.17         5\n",
            "          98       1.00      0.20      0.33        10\n",
            "          99       0.00      0.00      0.00         3\n",
            "         100       0.00      0.00      0.00         8\n",
            "         101       0.50      0.80      0.62         5\n",
            "         104       0.56      1.00      0.71         5\n",
            "         107       0.28      1.00      0.43         5\n",
            "         109       0.62      1.00      0.77         5\n",
            "         110       1.00      0.86      0.92         7\n",
            "         111       0.00      0.00      0.00         0\n",
            "         112       0.00      0.00      0.00         4\n",
            "         113       0.50      0.09      0.15        11\n",
            "         114       1.00      0.11      0.20         9\n",
            "         116       0.27      0.75      0.40         4\n",
            "         117       0.00      0.00      0.00         1\n",
            "         118       0.00      0.00      0.00         1\n",
            "         119       0.50      1.00      0.67         1\n",
            "         120       0.50      0.38      0.43         8\n",
            "         121       0.00      0.00      0.00         1\n",
            "         122       0.60      1.00      0.75         6\n",
            "         123       1.00      0.56      0.71         9\n",
            "         124       1.00      0.40      0.57         5\n",
            "         126       1.00      0.20      0.33        10\n",
            "         127       0.57      1.00      0.73         4\n",
            "         129       0.80      1.00      0.89         8\n",
            "         130       0.00      0.00      0.00         2\n",
            "         131       0.20      1.00      0.33         3\n",
            "         132       0.45      1.00      0.62         5\n",
            "         133       0.54      1.00      0.70         7\n",
            "         134       0.00      0.00      0.00         2\n",
            "         135       1.00      1.00      1.00         6\n",
            "         136       1.00      0.50      0.67         6\n",
            "         138       0.00      0.00      0.00         1\n",
            "         139       0.27      1.00      0.43         3\n",
            "         141       0.86      1.00      0.92         6\n",
            "         142       0.67      0.40      0.50         5\n",
            "         143       0.54      1.00      0.70         7\n",
            "         145       1.00      0.91      0.95        11\n",
            "         146       0.50      0.12      0.20         8\n",
            "         147       1.00      0.75      0.86         4\n",
            "         148       0.00      0.00      0.00         7\n",
            "         150       0.25      0.33      0.29         3\n",
            "         151       1.00      1.00      1.00         4\n",
            "         152       0.00      0.00      0.00         4\n",
            "         153       0.00      0.00      0.00         1\n",
            "         154       0.60      1.00      0.75         3\n",
            "         155       1.00      0.67      0.80         9\n",
            "         156       1.00      0.83      0.91         6\n",
            "         158       0.83      1.00      0.91         5\n",
            "         159       1.00      1.00      1.00         4\n",
            "         160       1.00      0.50      0.67         8\n",
            "         162       1.00      0.50      0.67         4\n",
            "         163       0.40      1.00      0.57         2\n",
            "         164       1.00      0.12      0.22         8\n",
            "         166       1.00      0.22      0.36         9\n",
            "         168       1.00      1.00      1.00         1\n",
            "         169       1.00      0.56      0.72        16\n",
            "         170       0.67      1.00      0.80         2\n",
            "         171       0.00      0.00      0.00         6\n",
            "         172       1.00      1.00      1.00         1\n",
            "         173       1.00      1.00      1.00         7\n",
            "         174       1.00      0.80      0.89         5\n",
            "         175       0.00      0.00      0.00         0\n",
            "         176       0.00      0.00      0.00         1\n",
            "         177       0.00      0.00      0.00        12\n",
            "         178       0.00      0.00      0.00         4\n",
            "         180       0.33      1.00      0.50         6\n",
            "         181       0.58      1.00      0.73        11\n",
            "         182       1.00      0.75      0.86         4\n",
            "         183       1.00      1.00      1.00         4\n",
            "         184       0.42      1.00      0.59         5\n",
            "         187       1.00      1.00      1.00         1\n",
            "         189       1.00      0.71      0.83         7\n",
            "         190       0.00      0.00      0.00        12\n",
            "         192       0.47      0.70      0.56        10\n",
            "         193       0.62      1.00      0.77         5\n",
            "         194       0.60      1.00      0.75         3\n",
            "         195       1.00      1.00      1.00         8\n",
            "         197       0.43      1.00      0.60         6\n",
            "         198       1.00      0.60      0.75        10\n",
            "         199       0.00      0.00      0.00         4\n",
            "         200       1.00      1.00      1.00         6\n",
            "         201       1.00      0.33      0.50         6\n",
            "         202       0.43      1.00      0.60         3\n",
            "         204       1.00      1.00      1.00         1\n",
            "         206       0.37      1.00      0.54         7\n",
            "         207       0.00      0.00      0.00         7\n",
            "         208       1.00      0.83      0.91         6\n",
            "         209       0.00      0.00      0.00         1\n",
            "         210       0.50      1.00      0.67         3\n",
            "         211       0.00      0.00      0.00         1\n",
            "         212       0.00      0.00      0.00         6\n",
            "         213       0.71      1.00      0.83         5\n",
            "         215       1.00      1.00      1.00         5\n",
            "         216       0.29      1.00      0.44         6\n",
            "         217       0.50      0.75      0.60         4\n",
            "         218       0.00      0.00      0.00         6\n",
            "         219       0.62      0.62      0.62         8\n",
            "         220       0.77      1.00      0.87        10\n",
            "         221       1.00      1.00      1.00         3\n",
            "         223       0.50      1.00      0.67         5\n",
            "         224       1.00      0.73      0.84        11\n",
            "         225       1.00      1.00      1.00         7\n",
            "         226       0.00      0.00      0.00         1\n",
            "         227       1.00      1.00      1.00         1\n",
            "         228       0.00      0.00      0.00         6\n",
            "         229       0.00      0.00      0.00         4\n",
            "         230       0.16      1.00      0.28         5\n",
            "         231       0.21      1.00      0.35         7\n",
            "         232       0.75      1.00      0.86         3\n",
            "         233       1.00      1.00      1.00         8\n",
            "         236       1.00      1.00      1.00         2\n",
            "         237       0.50      1.00      0.67         2\n",
            "         238       0.26      1.00      0.41         8\n",
            "         239       0.86      1.00      0.92         6\n",
            "         240       0.71      1.00      0.83         5\n",
            "         241       1.00      0.25      0.40         4\n",
            "         242       0.88      1.00      0.93         7\n",
            "         243       1.00      1.00      1.00         2\n",
            "         244       0.62      1.00      0.77         5\n",
            "         245       0.67      1.00      0.80         6\n",
            "         247       1.00      0.60      0.75         5\n",
            "         248       0.00      0.00      0.00         4\n",
            "         249       0.00      0.00      0.00         5\n",
            "         250       1.00      1.00      1.00         1\n",
            "         251       1.00      1.00      1.00         4\n",
            "         253       1.00      1.00      1.00         4\n",
            "         255       0.82      1.00      0.90         9\n",
            "         256       0.00      0.00      0.00         1\n",
            "         257       1.00      1.00      1.00         7\n",
            "         258       1.00      0.22      0.36         9\n",
            "         259       0.91      1.00      0.95        10\n",
            "         260       0.80      1.00      0.89         4\n",
            "         261       1.00      0.14      0.25         7\n",
            "         262       0.80      0.57      0.67         7\n",
            "         263       0.40      0.40      0.40         5\n",
            "         264       0.67      1.00      0.80         2\n",
            "         266       0.86      0.86      0.86         7\n",
            "         267       1.00      0.83      0.91         6\n",
            "         268       0.33      1.00      0.50         2\n",
            "         270       0.35      1.00      0.52         7\n",
            "         271       0.60      0.21      0.31        29\n",
            "         273       0.69      0.82      0.75        11\n",
            "         274       0.33      1.00      0.50         4\n",
            "         275       1.00      0.50      0.67         4\n",
            "         276       1.00      0.43      0.60         7\n",
            "         277       0.92      0.73      0.81        15\n",
            "         278       1.00      1.00      1.00         7\n",
            "         279       1.00      1.00      1.00         4\n",
            "         280       1.00      0.44      0.62         9\n",
            "         281       1.00      0.80      0.89         5\n",
            "         282       0.00      0.00      0.00         5\n",
            "         283       1.00      1.00      1.00         1\n",
            "         284       1.00      0.75      0.86         4\n",
            "         285       0.80      1.00      0.89         4\n",
            "         287       0.00      0.00      0.00         2\n",
            "         288       0.78      1.00      0.88         7\n",
            "         289       1.00      1.00      1.00         2\n",
            "         290       1.00      1.00      1.00         2\n",
            "         291       0.75      0.75      0.75         4\n",
            "         292       0.09      1.00      0.17         3\n",
            "         293       0.67      1.00      0.80         6\n",
            "         294       0.50      1.00      0.67         1\n",
            "         295       0.00      0.00      0.00         1\n",
            "         296       0.62      1.00      0.76         8\n",
            "         297       0.38      1.00      0.55         3\n",
            "         298       1.00      0.67      0.80         3\n",
            "         299       0.00      0.00      0.00         1\n",
            "         300       1.00      0.67      0.80         6\n",
            "         301       1.00      0.67      0.80         6\n",
            "         303       0.30      1.00      0.46         3\n",
            "         304       0.16      1.00      0.27         5\n",
            "         305       0.70      0.50      0.58        14\n",
            "         307       0.50      0.29      0.36         7\n",
            "         308       1.00      0.57      0.73         7\n",
            "         309       1.00      1.00      1.00         2\n",
            "         310       1.00      0.60      0.75         5\n",
            "         311       0.31      1.00      0.47         4\n",
            "         312       1.00      0.50      0.67         2\n",
            "         313       1.00      1.00      1.00         8\n",
            "         314       1.00      1.00      1.00         5\n",
            "         316       1.00      0.25      0.40         8\n",
            "         318       1.00      0.83      0.91         6\n",
            "         320       1.00      0.60      0.75         5\n",
            "         321       1.00      0.20      0.33         5\n",
            "         322       0.00      0.00      0.00         3\n",
            "         323       0.83      0.71      0.77         7\n",
            "         324       0.50      1.00      0.67         1\n",
            "         325       0.57      1.00      0.73         4\n",
            "         326       1.00      1.00      1.00         4\n",
            "         327       0.00      0.00      0.00         1\n",
            "         328       1.00      0.60      0.75         5\n",
            "         329       1.00      1.00      1.00         4\n",
            "         330       0.80      1.00      0.89         4\n",
            "         331       1.00      0.17      0.29         6\n",
            "         335       1.00      0.80      0.89         5\n",
            "         336       0.00      0.00      0.00         1\n",
            "         337       0.33      1.00      0.50         1\n",
            "         338       0.64      1.00      0.78         9\n",
            "         339       1.00      0.80      0.89        10\n",
            "         340       0.00      0.00      0.00         5\n",
            "         341       0.00      0.00      0.00         0\n",
            "         342       1.00      1.00      1.00         2\n",
            "         343       0.14      1.00      0.24         3\n",
            "         346       1.00      0.33      0.50         9\n",
            "         347       1.00      1.00      1.00         2\n",
            "         348       1.00      1.00      1.00         1\n",
            "         349       0.00      0.00      0.00         3\n",
            "         350       1.00      0.14      0.25         7\n",
            "         351       1.00      0.67      0.80         9\n",
            "         352       1.00      0.80      0.89        10\n",
            "         353       0.82      1.00      0.90         9\n",
            "         354       0.00      0.00      0.00         7\n",
            "         355       0.62      1.00      0.77         5\n",
            "         357       1.00      0.25      0.40         4\n",
            "         358       1.00      0.20      0.33         5\n",
            "         359       1.00      0.25      0.40         4\n",
            "         361       1.00      0.20      0.33         5\n",
            "         362       0.00      0.00      0.00         7\n",
            "         363       1.00      0.29      0.44         7\n",
            "         364       1.00      1.00      1.00         3\n",
            "         365       0.00      0.00      0.00         5\n",
            "         366       0.00      0.00      0.00         1\n",
            "         367       0.75      1.00      0.86         3\n",
            "         370       1.00      0.50      0.67         4\n",
            "         371       0.45      0.77      0.57        13\n",
            "         373       1.00      0.14      0.25         7\n",
            "         374       1.00      0.25      0.40         4\n",
            "         375       0.00      0.00      0.00         7\n",
            "         376       1.00      1.00      1.00         4\n",
            "         377       1.00      1.00      1.00         1\n",
            "         378       0.00      0.00      0.00         1\n",
            "         379       0.57      1.00      0.73         4\n",
            "         381       1.00      0.10      0.18        10\n",
            "         382       0.00      0.00      0.00         7\n",
            "         383       0.00      0.00      0.00         5\n",
            "         384       0.62      0.83      0.71         6\n",
            "         385       0.00      0.00      0.00         3\n",
            "         386       0.26      1.00      0.42         5\n",
            "         387       0.71      1.00      0.83        10\n",
            "         388       0.50      1.00      0.67         1\n",
            "         389       0.14      0.50      0.22         8\n",
            "         390       0.00      0.00      0.00         3\n",
            "         391       0.60      0.60      0.60         5\n",
            "         392       0.67      0.40      0.50         5\n",
            "         393       0.47      1.00      0.64         7\n",
            "         394       1.00      0.33      0.50         3\n",
            "         395       0.00      0.00      0.00         6\n",
            "         396       1.00      1.00      1.00         1\n",
            "         397       1.00      1.00      1.00        11\n",
            "         398       0.67      1.00      0.80         6\n",
            "         399       0.69      1.00      0.82         9\n",
            "         400       0.24      1.00      0.38         5\n",
            "         401       1.00      0.12      0.22         8\n",
            "         402       0.25      1.00      0.40         1\n",
            "         403       0.80      0.80      0.80         5\n",
            "         404       0.50      0.25      0.33         8\n",
            "         405       1.00      1.00      1.00         4\n",
            "         406       1.00      1.00      1.00        10\n",
            "         407       1.00      0.20      0.33         5\n",
            "         408       0.57      1.00      0.73         8\n",
            "         409       1.00      0.50      0.67         8\n",
            "         410       0.83      1.00      0.91         5\n",
            "         411       0.67      1.00      0.80         2\n",
            "         412       0.67      0.40      0.50         5\n",
            "         413       0.88      1.00      0.93         7\n",
            "         414       0.67      0.89      0.76         9\n",
            "         415       0.00      0.00      0.00        11\n",
            "         419       1.00      0.75      0.86         4\n",
            "         421       0.33      0.67      0.44         3\n",
            "         422       1.00      0.50      0.67         8\n",
            "         423       0.33      0.25      0.29         8\n",
            "         424       1.00      1.00      1.00         4\n",
            "         425       0.00      0.00      0.00         1\n",
            "         426       1.00      0.25      0.40         4\n",
            "         427       1.00      0.10      0.18        10\n",
            "         428       0.00      0.00      0.00         4\n",
            "         429       1.00      1.00      1.00         1\n",
            "         430       1.00      1.00      1.00         3\n",
            "         431       0.50      0.11      0.18         9\n",
            "         432       0.00      0.00      0.00         1\n",
            "         433       0.67      0.75      0.71         8\n",
            "         434       0.33      0.40      0.36         5\n",
            "         436       0.00      0.00      0.00         5\n",
            "         437       0.00      0.00      0.00         6\n",
            "         438       0.67      0.33      0.44        12\n",
            "         439       0.50      0.38      0.43         8\n",
            "         440       0.00      0.00      0.00         0\n",
            "         441       1.00      0.80      0.89         5\n",
            "         443       1.00      0.40      0.57         5\n",
            "         444       1.00      0.22      0.36         9\n",
            "         445       1.00      0.14      0.25         7\n",
            "         446       0.75      0.50      0.60         6\n",
            "         447       0.80      1.00      0.89         4\n",
            "         448       0.00      0.00      0.00         1\n",
            "         449       0.36      0.44      0.40         9\n",
            "         450       1.00      0.50      0.67         4\n",
            "         451       0.83      0.62      0.71         8\n",
            "         452       0.88      1.00      0.93         7\n",
            "         453       0.00      0.00      0.00         2\n",
            "         455       0.00      0.00      0.00         1\n",
            "         457       1.00      1.00      1.00         6\n",
            "         458       0.71      1.00      0.83         5\n",
            "         459       0.00      0.00      0.00         7\n",
            "         461       0.44      0.67      0.53         6\n",
            "         464       0.00      0.00      0.00         1\n",
            "         465       1.00      1.00      1.00         5\n",
            "         467       0.70      1.00      0.82         7\n",
            "         468       0.38      0.43      0.40         7\n",
            "         469       0.80      1.00      0.89         4\n",
            "         470       0.67      1.00      0.80         4\n",
            "         471       0.33      0.20      0.25         5\n",
            "         472       0.65      1.00      0.79        13\n",
            "         473       1.00      0.75      0.86         4\n",
            "         474       1.00      1.00      1.00         7\n",
            "         475       0.50      1.00      0.67         1\n",
            "         476       0.71      0.71      0.71         7\n",
            "         479       1.00      1.00      1.00         6\n",
            "         481       1.00      1.00      1.00         1\n",
            "         482       0.00      0.00      0.00         2\n",
            "         483       0.71      1.00      0.83         5\n",
            "         484       1.00      0.45      0.62        11\n",
            "         487       0.00      0.00      0.00         5\n",
            "         488       0.32      0.54      0.40        13\n",
            "         489       1.00      1.00      1.00         2\n",
            "         490       1.00      0.50      0.67         8\n",
            "         491       1.00      0.43      0.60         7\n",
            "         492       1.00      1.00      1.00         6\n",
            "         494       1.00      0.71      0.83         7\n",
            "         495       0.78      1.00      0.88         7\n",
            "         496       0.75      0.25      0.38        12\n",
            "         497       0.86      1.00      0.92         6\n",
            "         498       0.33      0.25      0.29         4\n",
            "         499       1.00      1.00      1.00         7\n",
            "         500       0.89      1.00      0.94         8\n",
            "         503       0.22      1.00      0.36         4\n",
            "         504       1.00      1.00      1.00         3\n",
            "         505       0.80      1.00      0.89         8\n",
            "         506       1.00      1.00      1.00         7\n",
            "         507       0.00      0.00      0.00         4\n",
            "         508       1.00      0.73      0.84        11\n",
            "         509       0.67      1.00      0.80         8\n",
            "         510       1.00      1.00      1.00         1\n",
            "         511       0.50      1.00      0.67         5\n",
            "         512       0.33      1.00      0.50         4\n",
            "         513       0.67      1.00      0.80         2\n",
            "         514       1.00      1.00      1.00         3\n",
            "         515       1.00      1.00      1.00         1\n",
            "         516       0.33      0.33      0.33         3\n",
            "         518       1.00      1.00      1.00         1\n",
            "         520       0.83      0.42      0.56        12\n",
            "         521       0.00      0.00      0.00         2\n",
            "         522       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.60      2368\n",
            "   macro avg       0.61      0.60      0.55      2368\n",
            "weighted avg       0.67      0.60      0.57      2368\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate for both title and text\n",
        "print(\"\\nTraining and Evaluating for Title Tasks:\")\n",
        "title_f1_scores = train_and_evaluate_nn(title_splits, targets_subtask1 + targets_subtask2, model_type='title')\n",
        "\n",
        "print(\"\\nTraining and Evaluating for Text Tasks:\")\n",
        "text_f1_scores = train_and_evaluate_nn(text_splits, targets_subtask1 + targets_subtask2, model_type='text')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCXJKnU1O3mx"
      },
      "source": [
        "# Create DataFrames for F1 Scores\n",
        "\n",
        "This cell creates two DataFrames, one for the title-focused F1 scores and another for the text-focused F1 scores. These DataFrames will be used for plotting the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9aMV7x3O17b",
        "outputId": "1f47e477-8e42-4fd0-bc50-00d6d3cd0594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collected F1-Scores for Title-Focused Classification:\n",
            "               Task  F1-Score\n",
            "0   hazard-category  0.959760\n",
            "1  product-category  0.970981\n",
            "2            hazard  0.838082\n",
            "3           product  0.606639\n",
            "\n",
            "Collected F1-Scores for Text-Focused Classification::\n",
            "               Task  F1-Score\n",
            "0   hazard-category  0.956631\n",
            "1  product-category  0.939294\n",
            "2            hazard  0.837009\n",
            "3           product  0.550043\n"
          ]
        }
      ],
      "source": [
        "# Create DataFrames for F1 scores for title and text\n",
        "f1_scores_title_df = pd.DataFrame({\n",
        "    'Task': targets_subtask1 + targets_subtask2,\n",
        "    'F1-Score': title_f1_scores\n",
        "})\n",
        "\n",
        "f1_scores_text_df = pd.DataFrame({\n",
        "    'Task': targets_subtask1 + targets_subtask2,\n",
        "    'F1-Score': text_f1_scores\n",
        "})\n",
        "# Print the collected F1-scores for title\n",
        "print(\"\\nCollected F1-Scores for Title-Focused Classification:\")\n",
        "print(f1_scores_title_df)\n",
        "\n",
        "# Print the collected F1-scores for text\n",
        "print(\"\\nCollected F1-Scores for Text-Focused Classification::\")\n",
        "print(f1_scores_text_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1AkMuwMO6rz"
      },
      "source": [
        "# Plot F1 Scores for Title and Text Subtasks\n",
        "\n",
        "This cell visualizes the F1 scores for the title and text-based tasks by plotting them in a bar chart. It displays both sets of F1 scores in a combined chart for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "6YvqtXEhO5kz",
        "outputId": "f6324680-bb1f-4e33-f6b6-3c0d82ca1ee7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIO0lEQVR4nOzdd3xT1f/H8XdaOiiFslvKKhQtLciUJVvLEAQRkaHIkKUsGbJEtoKIICBDQRmiCMpUQDY4AEFZsvdSoMyWUWhpen5/8Gu+hJbdNKS8no9HHpCTe5PPzbnNJ5/cc8+1GGOMAAAAAABAsnNzdgAAAAAAAKRWFN0AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAOAi4uLi1LNnT+XOnVtubm6qV6+es0OSJFksFg0cOPC+lg0KClKLFi0cGs+TpEWLFgoKCnJ2GDZr166VxWLR2rVrnR0KHpKj+tDZf/tJ/a1cuXJFrVu3VkBAgCwWi7p06aKjR4/KYrFo2rRpKR5jlSpVVKVKlRR/XQCOR9ENIFlNmzZNFoslyVvv3r1tyy1fvlytWrVS4cKF5e7u/lCFw44dO9SgQQPlzZtX3t7eypkzp6pVq6bPP/88Gbfo8TFlyhSNGDFCDRo00PTp09W1a1eHvM7d+vDW2536bP369Ro4cKAiIyMdEt+tEr4gJ3UrW7asw18fD+5+9q3kLPpOnjypgQMHatu2bfe9TpUqVe4Y1969e5MlrtTi0KFDateunfLnzy9vb29lyJBB5cuX15gxY3Tt2jVnh3dXQ4cO1bRp0/TOO+9oxowZevPNNx3+mrt379bAgQN19OhRh78WgMdHGmcHACB1Gjx4sPLly2fXVrhwYdv/Z86cqdmzZ6tEiRIKDAx84Odfv369qlatqjx58qhNmzYKCAjQiRMn9Oeff2rMmDHq1KnTI2/D42b16tXKmTOnPvvsM4e+TqVKlTRjxgy7ttatW6t06dJq27atrc3X11eSdO3aNaVJ8790sn79eg0aNEgtWrRQxowZHRprgiZNmqhWrVp2bdmyZUuR18aDuX3f+uabb7RixYpE7aGhocnyeidPntSgQYMUFBSkYsWK3fd6uXLl0rBhwxK1P8znVWq1ePFivfbaa/Ly8lKzZs1UuHBhxcbG6o8//lCPHj20a9cuTZo0ydlhSpImT56s+Ph4u7bVq1erbNmyGjBggK3NGKNr167Jw8PDIXHs3r1bgwYNUpUqVRL9cLl8+XKHvCYA56PoBuAQL774op599tk7Pj506FBNnjxZHh4eeumll7Rz584Hev6PPvpIfn5++uuvvxIVdmfOnHmYkB9adHS0fHx8HP46Z86cSdYiNj4+XrGxsfL29rZrz58/v/Lnz2/X9vbbbyt//vxq2rRpoue5fX1nKFGiRJKx4fFzez/9+eefWrFixWPXf35+fo9dTI+TI0eOqHHjxsqbN69Wr16tHDly2B7r0KGDDh48qMWLFzsxQntJFdFnzpxRWFiYXZvFYnHaZ5qnp6dTXheA4zG8HIBTBAYGPtKRhEOHDqlQoUJJFqHZs2dP1Pbtt9+qdOnS8vHxUaZMmVSpUqVERxUmTJigQoUKycvLS4GBgerQoUOiIdJVqlRR4cKFtXnzZlWqVEk+Pj56//33JUkxMTEaMGCAChQoIC8vL+XOnVs9e/ZUTEyM3XOsWLFCFSpUUMaMGeXr66uQkBDbcyQlYQj1mjVrtGvXrkTDb69evaru3bsrd+7c8vLyUkhIiD799FMZY+yex2KxqGPHjvruu+9s27l06dI7vu79uvWc7oEDB6pHjx6SpHz58tlivdtQysjISHXp0sUWf4ECBTR8+PBER6UexeHDh/Xaa68pc+bM8vHxUdmyZZMsCK5fv66BAwfq6aeflre3t3LkyKH69evr0KFDku58vmtS54GePn1aLVu2VK5cueTl5aUcOXLo5ZdfTvRe/PLLL6pYsaLSpUun9OnTq3bt2tq1a1ei2BYsWKDChQvL29tbhQsX1vz58+97+xcuXKjatWsrMDBQXl5eCg4O1pAhQ2S1Wu2WS9i/d+/erapVq8rHx0c5c+bUJ598kug5//33X9WrV0/p0qVT9uzZ1bVr10T7+sOKj4/X6NGjVahQIXl7e8vf31/t2rXTxYsXbcsMGDBAbm5uWrVqld26bdu2laenp7Zv3661a9eqVKlSkqSWLVva9sfkOF83Li5OQ4YMUXBwsLy8vBQUFKT3338/yffgl19+UeXKlZU+fXplyJBBpUqV0syZM22P3+l856TO8f38889VqFAh22fZs88+a/dckvTff//prbfekr+/v7y8vFSoUCFNmTIl0fM/Sh9+8sknunLlir7++mu7gjtBgQIF9O67795x/QsXLui9997TM888I19fX2XIkEEvvviitm/fnmjZe23z5cuX1aVLFwUFBcnLy0vZs2dXtWrVtGXLFtsyt57TnfB3fOTIES1evNjuc+pO53Tv3btXDRs2VLZs2ZQ2bVqFhISob9++tsePHTum9u3bKyQkRGnTplWWLFn02muv2f29T5s2Ta+99pokqWrVqok+y5Pq7zNnzqhVq1by9/eXt7e3ihYtqunTp9stkxDzp59+qkmTJtn2yVKlSumvv/66Yx8ASDkc6QbgEFFRUTp37pxdW9asWZPt+fPmzasNGzZo586ddsPWkzJo0CANHDhQzz33nAYPHixPT09t3LhRq1evVvXq1SXdLBYHDRqk8PBwvfPOO9q3b58mTpyov/76S+vWrbP7geD8+fN68cUX1bhxYzVt2lT+/v6Kj49X3bp19ccff6ht27YKDQ3Vjh079Nlnn2n//v1asGCBJGnXrl166aWXVKRIEQ0ePFheXl46ePCg1q1bd8f4s2XLphkzZuijjz7SlStXbENeQ0NDZYxR3bp1tWbNGrVq1UrFihXTsmXL1KNHD/3333+JhqKvXr1aP/zwgzp27KisWbMm+yRc9evX1/79+/X999/rs88+s/X5nYZ6R0dHq3Llyvrvv//Url075cmTR+vXr1efPn106tQpjR49+r5eNzo6OtH+5ufnJw8PD0VEROi5555TdHS0OnfurCxZsmj69OmqW7eu5syZo1deeUWSZLVa9dJLL2nVqlVq3Lix3n33XV2+fFkrVqzQzp07FRwc/EDvxauvvqpdu3apU6dOCgoK0pkzZ7RixQodP37c9r7PmDFDzZs3V40aNTR8+HBFR0dr4sSJqlChgrZu3Wpbbvny5Xr11VcVFhamYcOG6fz587aC/n5MmzZNvr6+6tatm3x9fbV69Wr1799fly5d0ogRI+yWvXjxomrWrKn69eurYcOGmjNnjnr16qVnnnlGL774oqSbpxS88MILOn78uDp37qzAwEDNmDFDq1evfqD36E7atWunadOmqWXLlurcubOOHDmicePGaevWrba/xw8++EA///yzWrVqpR07dih9+vRatmyZJk+erCFDhqho0aKKiIjQ4MGD1b9/f7Vt21YVK1aUJD333HP3jMFqtSbap7y9vW2nVbRu3VrTp09XgwYN1L17d23cuFHDhg3Tnj177H4QmTZtmt566y0VKlRIffr0UcaMGbV161YtXbpUr7/++gO9L5MnT1bnzp3VoEEDvfvuu7p+/br++ecfbdy40fZcERERKlu2rO1HtmzZsumXX35Rq1atdOnSJXXp0kXSo/fhzz//rPz589/Xe5mUw4cPa8GCBXrttdeUL18+RURE6Msvv1TlypW1e/du2zD++9nmt99+W3PmzFHHjh0VFham8+fP648//tCePXtUokSJRK8dGhqqGTNmqGvXrsqVK5e6d+8u6ebn1NmzZxMt/88//6hixYry8PBQ27ZtFRQUpEOHDunnn3/WRx99JEn666+/tH79ejVu3Fi5cuXS0aNHNXHiRFWpUkW7d++Wj4+PKlWqpM6dO2vs2LF6//33badQ3OlUimvXrqlKlSo6ePCgOnbsqHz58unHH39UixYtFBkZmehHjZkzZ+ry5ctq166dLBaLPvnkE9WvX1+HDx922HB5APfJAEAymjp1qpGU5O1OateubfLmzftAr7N8+XLj7u5u3N3dTbly5UzPnj3NsmXLTGxsrN1yBw4cMG5ubuaVV14xVqvV7rH4+HhjjDFnzpwxnp6epnr16nbLjBs3zkgyU6ZMsbVVrlzZSDJffPGF3XPNmDHDuLm5md9//92u/YsvvjCSzLp164wxxnz22WdGkjl79uwDbW/CaxcqVMiubcGCBUaS+fDDD+3aGzRoYCwWizl48KCtTZJxc3Mzu3bteuDXTpcunWnevHmSj0kyAwYMsN0fMWKEkWSOHDmSaNm8efPaPc+QIUNMunTpzP79++2W6927t3F3dzfHjx+/a1xHjhy54/62Zs0aY4wxXbp0MZLs+uby5csmX758JigoyNbnU6ZMMZLMqFGjEr1Owr6yZs0au+e+PY6pU6caY4y5ePGikWRGjBhxx9gvX75sMmbMaNq0aWPXfvr0aePn52fXXqxYMZMjRw4TGRlpa1u+fLmRdF9/O9HR0Yna2rVrZ3x8fMz169dtbQn79zfffGNri4mJMQEBAebVV1+1tY0ePdpIMj/88IOt7erVq6ZAgQJJvj9306FDB7vPh99//91IMt99953dckuXLk3UvmPHDuPp6Wlat25tLl68aHLmzGmeffZZc+PGDdsyf/31l13f3I+E9+H2W8K+u23bNiPJtG7d2m699957z0gyq1evNsYYExkZadKnT2/KlCljrl27Zrdswj5lTOK/i1vjqFy5su3+yy+/nOgz4HatWrUyOXLkMOfOnbNrb9y4sfHz87PtC4/Sh1FRUUaSefnll+8ay61u38br168n+kw+cuSI8fLyMoMHD7a13c82+/n5mQ4dOtx1mebNmyf6W8mbN6+pXbt2ohhu318qVapk0qdPb44dO2a37K19mNTf2IYNGxL9Pf344493fH9v7++EPvr2229tbbGxsaZcuXLG19fXXLp0yS7mLFmymAsXLtiWXbhwoZFkfv7558RvCIAUxfByAA4xfvx4rVixwu6WnKpVq6YNGzaobt262r59uz755BPVqFFDOXPm1E8//WRbbsGCBYqPj1f//v3l5mb/kWexWCRJK1euVGxsrLp06WK3TJs2bZQhQ4ZEw5C9vLzUsmVLu7Yff/xRoaGhKliwoM6dO2e7Pf/885KkNWvWSJJtOPzChQuTZfj0kiVL5O7urs6dO9u1d+/eXcYY/fLLL3btlStXTnQOozP9+OOPqlixojJlymT3voWHh8tqteq33367r+dp27Ztov2taNGikm6+R6VLl1aFChVsy/v6+qpt27Y6evSodu/eLUmaO3eusmbNmuQkfAn7yv1KmzatPD09tXbtWrsh0bdasWKFIiMj1aRJE7ttd3d3V5kyZWz7zKlTp7Rt2zY1b95cfn5+tvWrVat2332ZNm1a2/8vX76sc+fOqWLFioqOjk40G7evr6/ducyenp4qXbq0Dh8+bGtbsmSJcuTIoQYNGtjafHx87Cbae1g//vij/Pz8VK1aNbv3pWTJkvL19bW9L9LNyRkHDRqkr776SjVq1NC5c+c0ffp0u4n9HlZQUFCifapnz56Sbm6/JHXr1s1unYQjpgmfGStWrNDly5fVu3fvROcJP+g+Jd38/Pj333/vOGTYGKO5c+eqTp06MsbYvX81atRQVFSUbcj1o/ThpUuXJEnp06d/4G1I4OXlZfu8tVqtOn/+vO10m1uHhd9rmxOW2bhxo06ePPnQ8dzJ2bNn9dtvv+mtt95Snjx57B67tQ9v/Ru7ceOGzp8/rwIFCihjxox22/MglixZooCAADVp0sTW5uHhoc6dO+vKlSv69ddf7ZZv1KiRMmXKZLufMLLj1r9dAM7B8HIADlG6dOm7TqR2P6xWa6KhfpkzZ7ZNNlOqVCnNmzdPsbGx2r59u+bPn6/PPvtMDRo00LZt2xQWFqZDhw7Jzc3trsXJsWPHJEkhISF27Z6ensqfP7/t8QQ5c+ZMNOHNgQMHtGfPnjsOo06Y3K1Ro0b66quv1Lp1a/Xu3VsvvPCC6tevrwYNGiT6UeB+HDt2TIGBgYm+/CYMV7w99ttnlHe2AwcO6J9//rnn+3b27Fm78499fX1tw3wl6amnnlJ4eHiSz3Hs2DGVKVMmUfut71HhwoV16NAhhYSEJEvB5uXlpeHDh6t79+7y9/dX2bJl9dJLL6lZs2YKCAiQdHPbJdl+mLldhgwZbPFJN7fxdrcXKHeya9cuffDBB1q9erWtYEoQFRVldz9XrlyJCsJMmTLpn3/+sd0/duyYChQokGi52/+GHsaBAwcUFRWV5NwMUuKJEnv06KFZs2Zp06ZNGjp06H3/EHHlyhVduXLFdt/d3d1uP0yXLt1d9yk3NzcVKFDArj0gIEAZM2a09VnCXAD3OgXmfvXq1UsrV65U6dKlVaBAAVWvXl2vv/66ypcvL+nm30lkZKQmTZp0x1nDE96/R+nDhH3z8uXLD70t8fHxGjNmjCZMmKAjR47Y/X1nyZLF9v97bbN08/zy5s2bK3fu3CpZsqRq1aqlZs2aJZoQ8mEkFKz36sNr165p2LBhmjp1qv777z+7OTVu/xu7X8eOHdNTTz2VKDfc6fP99h8FEgrwO/3wByDlUHQDeGydOHEiUZG4Zs2aRBPNeHp6qlSpUipVqpSefvpptWzZUj/++KPdZWCS061HNBLEx8frmWee0ahRo5JcJ3fu3LZ1f/vtN61Zs0aLFy/W0qVLNXv2bD3//PNavny53N3dHRLz3WJ3pvj4eFWrVs12BPF2Tz/9tKSbP7Dc+gVzwIABtsnbUtKdjk7ePiGZJHXp0kV16tTRggULtGzZMvXr10/Dhg3T6tWrVbx4cdtIhxkzZtgK8VslR/Ev3ZyornLlysqQIYMGDx6s4OBgeXt7a8uWLerVq1eiERd32gfNbRPzOUp8fLyyZ8+u7777LsnHb/+B5vDhw7YfMHbs2HHfr/Ppp59q0KBBtvt58+Z94GsnP8zR6gd5HqvVatcfoaGh2rdvnxYtWqSlS5dq7ty5mjBhgvr3769BgwbZ+rJp06Zq3rx5ks9ZpEiRR443Q4YMCgwMfOCrTtxq6NCh6tevn9566y0NGTJEmTNnlpubm7p06WK3T95rmyWpYcOGqlixoubPn6/ly5drxIgRGj58uObNm2ebh8DROnXqpKlTp6pLly4qV66c/Pz8ZLFY1Lhx42SdFPJunP23C+DOKLoBPLYCAgISDUtPGDJ8JwlH10+dOiVJCg4OVnx8vHbv3n3Ha/TmzZtXkrRv3z67IyOxsbE6cuTIHY923So4OFjbt2/XCy+8cM8v4m5ubnrhhRf0wgsvaNSoURo6dKj69u2rNWvW3Ndr3R77ypUrdfnyZbuj3QlDhhO2LSU9SCESHBysK1eu3HO7v/vuO127ds12/0GOYOXNm1f79u1L1H77exQcHKyNGzfqxo0bd5x0KOHI0e2z2t9+xClBcHCwunfvru7du+vAgQMqVqyYRo4cqW+//dY2MVv27Nnvuv0J8SUUlrdKartut3btWp0/f17z5s1TpUqVbO1Hjhy557p3i2nnzp0yxtj19/3Ecy/BwcFauXKlypcvf88fieLj49WiRQtlyJBBXbp00dChQ9WgQQPVr1/ftsyd9sdmzZrZnXLwID9I5c2bV/Hx8Tpw4IDdJFgRERGKjIy026ckaefOnYmOit8qU6ZMifYp6eZ+dfu+ni5dOjVq1EiNGjVSbGys6tevr48++kh9+vRRtmzZlD59elmt1nv+TT1qH7700kuaNGmSNmzYoHLlyt3XOreaM2eOqlatqq+//tquPTIyMtGkm3fb5oRh+zly5FD79u3Vvn17nTlzRiVKlNBHH330yEV3wvt/rx8Y5syZo+bNm2vkyJG2tuvXryfq1wf5fMybN6/++ecfxcfH2x3tdubnO4CHwzndAB5b3t7eCg8Pt7slFD1r1qxJ8tf7hHMtE4ZI1qtXT25ubho8eHCiow0J64eHh8vT01Njx461e86vv/5aUVFRql279j1jbdiwof777z9Nnjw50WPXrl3T1atXJd28TM7tEn4MeJjLLdWqVUtWq1Xjxo2za//ss89ksVhS7CjPrdKlSycpcWGalIYNG2rDhg1atmxZosciIyMVFxcnSSpfvrzdfvAgRXetWrW0adMmbdiwwdZ29epVTZo0SUFBQbbhyK+++qrOnTuX6L2U/rev5M2bV+7u7onONZ8wYYLd/ejoaF2/ft2uLTg4WOnTp7f1c40aNZQhQwYNHTpUN27cSPSaCadW5MiRQ8WKFdP06dPthqmuWLHCdj763SQc/bp1346NjU0U84OoVauWTp48qTlz5tjaoqOj7zik+UE0bNhQVqtVQ4YMSfRYXFyc3X41atQorV+/XpMmTdKQIUP03HPP6Z133rGbdfxO+2P+/Pnt9qlbhyvfS61atSQp0ez6CSNdEj4zqlevrvTp02vYsGGJ9odb+yM4OFh//vmnYmNjbW2LFi3SiRMn7NY5f/683X1PT0+FhYXJGKMbN27I3d1dr776qubOnZtkkXjr6TqP2oc9e/ZUunTp1Lp1a0VERCR6/NChQxozZswd13d3d0/0Gf7jjz/qv//+s2u71zZbrdZEw7ezZ8+uwMDAZLmEXbZs2VSpUiVNmTJFx48ft3vs1viT2p7PP/880SiYB/l8rFWrlk6fPq3Zs2fb2uLi4vT555/L19dXlStXftDNAeAkHOkG4BT//POPbcKzgwcPKioqSh9++KGkm0ez69Spc9f1O3XqpOjoaL3yyisqWLCgYmNjtX79es2ePVtBQUG2ic4KFCigvn37asiQIapYsaLq168vLy8v/fXXXwoMDNSwYcOULVs29enTR4MGDVLNmjVVt25d7du3TxMmTFCpUqXsJpW6kzfffFM//PCD3n77ba1Zs0bly5eX1WrV3r179cMPP2jZsmV69tlnNXjwYP3222+qXbu28ubNqzNnzmjChAnKlSuX3VG3+1WnTh1VrVpVffv21dGjR1W0aFEtX75cCxcuVJcuXR74MlfJoWTJkpKkvn37qnHjxvLw8FCdOnVsXzZv1aNHD/3000966aWX1KJFC5UsWVJXr17Vjh07NGfOHB09evSRLzXXu3dvff/993rxxRfVuXNnZc6cWdOnT9eRI0c0d+5c2xGkZs2a6ZtvvlG3bt20adMmVaxYUVevXtXKlSvVvn17vfzyy/Lz89Nrr72mzz//XBaLRcHBwVq0aFGi84z379+vF154QQ0bNlRYWJjSpEmj+fPnKyIiQo0bN5Z0c4juxIkT9eabb6pEiRJq3LixsmXLpuPHj2vx4sUqX7687QeAYcOGqXbt2qpQoYLeeustXbhwwXbt4lvPS07Kc889p0yZMql58+bq3LmzLBaLZsyY8UhDTtu0aaNx48apWbNm2rx5s3LkyKEZM2bIx8fnoZ8zQeXKldWuXTsNGzZM27ZtU/Xq1eXh4aEDBw7oxx9/1JgxY9SgQQPt2bNH/fr1U4sWLWyfF9OmTVOxYsXUvn17/fDDD5JuFrQZM2bUF198ofTp0ytdunQqU6bMI81vULRoUTVv3lyTJk2yDd/ftGmTpk+frnr16qlq1aqSbvbxZ599ptatW6tUqVJ6/fXXlSlTJm3fvl3R0dG26y23bt1ac+bMUc2aNdWwYUMdOnTIbjREgurVqysgIEDly5eXv7+/9uzZo3Hjxql27dq2kS4ff/yx1qxZozJlyqhNmzYKCwvThQsXtGXLFq1cudL2w9+j9mFwcLBmzpypRo0aKTQ0VM2aNVPhwoVtn8UJl7a6k5deekmDBw9Wy5Yt9dxzz2nHjh367rvvEv2gdq9tjoyMVK5cudSgQQMVLVpUvr6+Wrlypf766y+7o86PYuzYsapQoYJKlCihtm3bKl++fDp69KgWL16sbdu22bZnxowZ8vPzU1hYmDZs2KCVK1fanZ8u3fyR1d3dXcOHD1dUVJS8vLz0/PPPJzmHQdu2bfXll1+qRYsW2rx5s4KCgjRnzhytW7dOo0ePfqSJ7ACksJSdLB1AapdwybC//vrrvpZL6nanS1Pd6pdffjFvvfWWKViwoPH19TWenp6mQIECplOnTiYiIiLR8lOmTDHFixc3Xl5eJlOmTKZy5cpmxYoVdsuMGzfOFCxY0Hh4eBh/f3/zzjvvmIsXL9otk9RluxLExsaa4cOHm0KFCtlep2TJkmbQoEEmKirKGGPMqlWrzMsvv2wCAwONp6enCQwMNE2aNEl0yayk3Om1L1++bLp27WoCAwONh4eHeeqpp8yIESPsLmdjzM1Le93rsjp38iCXDDPm5qXAcubMadzc3OwuH5bUpZEuX75s+vTpYwoUKGA8PT1N1qxZzXPPPWc+/fTTRJeAu13CpXLudmkuY4w5dOiQadCggcmYMaPx9vY2pUuXNosWLUq0XHR0tOnbt6/Jly+f8fDwMAEBAaZBgwbm0KFDtmXOnj1rXn31VePj42MyZcpk2rVrZ3bu3Gl3maFz586ZDh06mIIFC5p06dIZPz8/U6ZMGbvLMyVYs2aNqVGjhvHz8zPe3t4mODjYtGjRwvz99992y82dO9eEhoYaLy8vExYWZubNm5fkZZCSsm7dOlO2bFmTNm1aExgYaLvEnm67dNGd9rGkXufYsWOmbt26xsfHx2TNmtW8++67tst6PcolwxJMmjTJlCxZ0qRNm9akT5/ePPPMM6Znz57m5MmTJi4uzpQqVcrkypXL7jJqxhgzZswYI8nMnj3b1rZw4UITFhZm0qRJc1+XD7vb33mCGzdumEGDBtn2ldy5c5s+ffrYXYItwU8//WSee+45kzZtWpMhQwZTunRp8/3339stM3LkSJMzZ07j5eVlypcvb/7+++9El5D68ssvTaVKlUyWLFmMl5eXCQ4ONj169LB9viSIiIgwHTp0MLlz57btxy+88IKZNGmS3XLJ0Yf79+83bdq0MUFBQcbT09OkT5/elC9f3nz++ed270VSlwzr3r27yZEjh0mbNq0pX7682bBhwwNvc0xMjOnRo4cpWrSoSZ8+vUmXLp0pWrSomTBhgl2cj3LJMGOM2blzp3nllVdsnyEhISGmX79+tscvXrxoWrZsabJmzWp8fX1NjRo1zN69e5P8zJs8ebLJnz+/cXd3t3uvb992Y272ZcLzenp6mmeeeSZRbHf7HEzq8xlAyrMYw+wKAAAAAAA4Aud0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAO4tSi+7ffflOdOnUUGBgoi8WiBQsW3HOdtWvXqkSJEvLy8lKBAgU0bdo0h8cJAAAAAMDDcGrRffXqVRUtWlTjx4+/r+WPHDmi2rVrq2rVqtq2bZu6dOmi1q1bJ3l9VwAAAAAAnO2xmb3cYrFo/vz5qlev3h2X6dWrlxYvXqydO3fa2ho3bqzIyEgtXbo0BaIEAAAAAOD+pXF2AA9iw4YNCg8Pt2urUaOGunTpcsd1YmJiFBMTY7sfHx+vCxcuKEuWLLJYLI4KFQAAAACQihljdPnyZQUGBsrN7c6DyF2q6D59+rT8/f3t2vz9/XXp0iVdu3ZNadOmTbTOsGHDNGjQoJQKEQAAAADwBDlx4oRy5cp1x8ddquh+GH369FG3bt1s96OiopQnTx6dOHFCGTJkcGJkAAAAAABXdenSJeXOnVvp06e/63IuVXQHBAQoIiLCri0iIkIZMmRI8ii3JHl5ecnLyytRe4YMGSi6AQAAAACP5F6nLbvUdbrLlSunVatW2bWtWLFC5cqVc1JEAAAAAADcmVOL7itXrmjbtm3atm2bpJuXBNu2bZuOHz8u6ebQ8GbNmtmWf/vtt3X48GH17NlTe/fu1YQJE/TDDz+oa9euzggfAAAAAIC7cmrR/ffff6t48eIqXry4JKlbt24qXry4+vfvL0k6deqUrQCXpHz58mnx4sVasWKFihYtqpEjR+qrr75SjRo1nBI/AAAAAAB389hcpzulXLp0SX5+foqKiuKcbgAAAOAJZLVadePGDWeHgcech4eH3N3d7/j4/daWLjWRGgAAAAA8LGOMTp8+rcjISGeHAheRMWNGBQQE3HOytLuh6AYAAADwREgouLNnzy4fH59HKqSQuhljFB0drTNnzkiScuTI8dDPRdENAAAAINWzWq22gjtLlizODgcuIOGy1GfOnFH27NnvOtT8blzqkmEAAAAA8DASzuH28fFxciRwJQn7y6PMAUDRDQAAAOCJwZByPIjk2F8ougEAAAAAcBDO6QaSQVDvxc4OIVU6+nFtZ4cAAADw2GvRooUiIyO1YMGCOy6zdu1aVa1aVRcvXlTGjBlTLDZXYLFYNH/+fNWrV88hz0/RDQAAAOCJltIHUB7kwMK9hjcPGDBAY8aMkTHG1lalShUVK1ZMo0ePftgQ7ygoKEjHjh2za8uZM6f+/fffZH+t1IKiGwAAAAAeU6dOnbL9f/bs2erfv7/27dtna/P19ZWvr2+KxjR48GC1adPGdv9hZ/V+UnBONwAAAAA8pgICAmw3Pz8/WSwWuzZfX1+1aNHCNjS6RYsW+vXXXzVmzBhZLBZZLBYdPXo0yef+448/VLFiRaVNm1a5c+dW586ddfXq1XvGlD59ersYsmXLZnts4sSJCg4Olqenp0JCQjRjxgy7dSMjI9WuXTv5+/vL29tbhQsX1qJFiyRJAwcOVLFixeyWHz16tIKCgmz3165dq9KlSytdunTKmDGjypcvb3fkfeHChSpRooS8vb2VP39+DRo0SHFxcbbHDxw4oEqVKsnb21thYWFasWLFPbf3UXGk+zHGecKOwXnCAAAASK3GjBmj/fv3q3Dhwho8eLAkKVu2bIkK70OHDqlmzZr68MMPNWXKFJ09e1YdO3ZUx44dNXXq1Id67fnz5+vdd9/V6NGjFR4erkWLFqlly5bKlSuXqlatqvj4eL344ou6fPmyvv32WwUHB2v37t33faQ8Li5O9erVU5s2bfT9998rNjZWmzZtsg3B//3339WsWTONHTtWFStW1KFDh9S2bVtJN4fhx8fHq379+vL399fGjRsVFRWlLl26PNS2PgiKbgAAAABIJfz8/OTp6SkfHx8FBATccblhw4bpjTfesBWdTz31lMaOHavKlStr4sSJ8vb2vuO6vXr10gcffGC7P3ToUHXu3FmffvqpWrRoofbt20uSunXrpj///FOffvqpqlatqpUrV2rTpk3as2ePnn76aUlS/vz573vbLl26pKioKL300ksKDg6WJIWGhtoeHzRokHr37q3mzZvbnnvIkCHq2bOnBgwYoJUrV2rv3r1atmyZAgMDbbG/+OKL9x3Dw6DoBgAAj2agn7MjSJ0GRjk7AgCp2Pbt2/XPP//ou+++s7UZYxQfH68jR45o/vz5Gjp0qO2x3bt3K0+ePJKkHj16qEWLFrbHsmbNKknas2eP7chygvLly2vMmDGSpG3btilXrly2gvtBZc6cWS1atFCNGjVUrVo1hYeHq2HDhsqRI4dtm9atW6ePPvrIto7VatX169cVHR2tPXv2KHfu3LaCW5LKlSv3ULE8CIpuAAAAAHjCXLlyRe3atVPnzp0TPZYnTx69/fbbatiwoa3t1kI1a9asKlCgwAO/Ztq0ae/6uJubm90s7JJ048YNu/tTp05V586dtXTpUs2ePVsffPCBVqxYobJly+rKlSsaNGiQ6tevn+i573bk3tEougEAAAAgFfH09JTVar3rMiVKlNDu3bvvWDxnzpxZmTNnfqDXDQ0N1bp162zDuyVp3bp1CgsLkyQVKVJE//77r/bv35/k0e5s2bLp9OnTMsbYztPetm1bouWKFy+u4sWLq0+fPipXrpxmzpypsmXLqkSJEtq3b98dtyk0NFQnTpzQqVOnbEfH//zzzwfaxodB0Q0AAAAAqUhQUJA2btyoo0ePytfXN8niuVevXipbtqw6duyo1q1bK126dNq9e7dWrFihcePGPdTr9ujRQw0bNlTx4sUVHh6un3/+WfPmzdPKlSslSZUrV1alSpX06quvatSoUSpQoID27t0ri8WimjVrqkqVKjp79qw++eQTNWjQQEuXLtUvv/yiDBkySJKOHDmiSZMmqW7dugoMDNS+fft04MABNWvWTJLUv39/vfTSS8qTJ48aNGggNzc3bd++XTt37tSHH36o8PBwPf3002revLlGjBihS5cuqW/fvg/5Lt8/LhkGAAAAAKnIe++9J3d3d4WFhSlbtmw6fvx4omWKFCmiX3/9Vfv371fFihVVvHhx9e/f324Y+YOqV6+exowZo08//VSFChXSl19+qalTp6pKlSq2ZebOnatSpUqpSZMmCgsLU8+ePW1H5UNDQzVhwgSNHz9eRYsW1aZNm/Tee+/Z1vXx8dHevXv16quv6umnn1bbtm3VoUMHtWvXTpJUo0YNLVq0SMuXL1epUqVUtmxZffbZZ8qbN6+km8PX58+fr2vXrql06dJq3bq13fnfjmIxtw+aT+UuXbokPz8/RUVF2X4xeVxxyTDHcMQlw+grx+DyboCLYCI1x2AiNSBZXb9+XUeOHFG+fPmcen4vXMvd9pv7rS050g0AAAAAgINwTjcAAMCTglEJjsGoBAB3wZFuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAT5wqVaqoS5cuDn+dNA5/BQAAAAB4nA30S+HXi7rvRS0Wy10fHzBggAYOHPhQYRw9elT58uXT1q1bVaxYsbsu26JFC02fPj1R+4EDB1SgQIGHev0nBUU3AAAAADymTp06Zfv/7Nmz1b9/f+3bt8/W5uvrm2Kx1KxZU1OnTrVry5YtW4q9vqtieDkAAAAAPKYCAgJsNz8/P1ksFru2WbNmKTQ0VN7e3ipYsKAmTJhgW/ett95SkSJFFBMTI0mKjY1V8eLF1axZM0lSvnz5JEnFixeXxWJRlSpV7hqLl5eX3WsHBATI3d1dkvTrr7+qdOnS8vLyUo4cOdS7d2/FxcXZ1o2Pj9cnn3yiAgUKyMvLS3ny5NFHH30kSVq7dq0sFosiIyNty2/btk0Wi0VHjx6VJB07dkx16tRRpkyZlC5dOhUqVEhLliyxLb9z5069+OKL8vX1lb+/v958802dO3fO9vjVq1fVrFkz+fr6KkeOHBo5cuQD9sTDo+gGAAAAABf03XffqX///vroo4+0Z88eDR06VP369bMNAx87dqyuXr2q3r17S5L69u2ryMhIjRs3TpK0adMmSdLKlSt16tQpzZs376Hi+O+//1SrVi2VKlVK27dv18SJE/X111/rww8/tC3Tp08fffzxx+rXr592796tmTNnyt/f/75fo0OHDoqJidFvv/2mHTt2aPjw4baj/JGRkXr++edVvHhx/f3331q6dKkiIiLUsGFD2/o9evTQr7/+qoULF2r58uVau3attmzZ8lDb+6AYXg4AAAAALmjAgAEaOXKk6tevL+nmkevdu3fryy+/VPPmzeXr66tvv/1WlStXVvr06TV69GitWbNGGTJkkPS/oeFZsmRRQEDAPV9v0aJFdsPZX3zxRf3444+aMGGCcufOrXHjxslisahgwYI6efKkevXqpf79++vq1asaM2aMxo0bp+bNm0uSgoODVaFChfve1uPHj+vVV1/VM888I0nKnz+/7bFx48apePHiGjp0qK1typQpyp07t/bv36/AwEB9/fXX+vbbb/XCCy9IkqZPn65cuXLd9+s/CopuAAAAAHAxV69e1aFDh9SqVSu1adPG1h4XFyc/v/9NDFeuXDm99957GjJkiHr16nXPQvf333/Xiy++aLv/5Zdf6o033pAkVa1aVRMnTrQ9li5dOknSnj17VK5cObtJ38qXL68rV67o33//1enTpxUTE2MreB9G586d9c4772j58uUKDw/Xq6++qiJFikiStm/frjVr1iR5fvuhQ4d07do1xcbGqkyZMrb2zJkzKyQk5KHjeRAU3QAAAADgYq5cuSJJmjx5sl0xKcl2nrV081zqdevWyd3dXQcPHrzn8z777LPatm2b7f6tQ8DTpUv3UDOVp02b9q6Pu7ndPOvZGGNru3Hjht0yrVu3Vo0aNbR48WItX75cw4YN08iRI9WpUydduXJFderU0fDhwxM9d44cOe5rux2Jc7oBAAAAwMX4+/srMDBQhw8fVoECBexuCROkSdKIESO0d+9e/frrr1q6dKnd7OOenp6SJKvVamtLmzat3XOlT5/+nrGEhoZqw4YNdkXzunXrlD59euXKlUtPPfWU0qZNq1WrViW5fsIw91tnar+18E+QO3duvf3225o3b566d++uyZMnS5JKlCihXbt2KSgoKNF7kS5dOgUHB8vDw0MbN260PdfFixe1f//+e25bcqDoBgAAAAAXNGjQIA0bNkxjx47V/v37tWPHDk2dOlWjRo2SJG3dulX9+/fXV199pfLly2vUqFF69913dfjwYUlS9uzZlTZtWtvEY1FR93/98Fu1b99eJ06cUKdOnbR3714tXLhQAwYMULdu3eTm5iZvb2/16tVLPXv21DfffKNDhw7pzz//1Ndffy1JKlCggHLnzq2BAwfqwIEDWrx4caLZxbt06aJly5bpyJEj2rJli9asWaPQ0FBJNydZu3Dhgpo0aaK//vpLhw4d0rJly9SyZUtZrVb5+vqqVatW6tGjh1avXq2dO3eqRYsWtiPsjkbRDQAAAAAuqHXr1vrqq680depUPfPMM6pcubKmTZumfPny6fr162ratKlatGihOnXqSJLatm2rqlWr6s0335TValWaNGk0duxYffnllwoMDNTLL7/8UHHkzJlTS5Ys0aZNm1S0aFG9/fbbatWqlT744APbMv369VP37t3Vv39/hYaGqlGjRjpz5owkycPDQ99//7327t2rIkWKaPjw4XYzn0s3j8Z36NBBoaGhqlmzpp5++mnb5dECAwO1bt06Wa1WVa9eXc8884y6dOmijBkz2grrESNGqGLFiqpTp47Cw8NVoUIFlSxZ8qG290FZzK1jAJ4Aly5dkp+fn6Kiomyz9j2ugnovdnYIqdLRj2sn+3PSV47hiL4C4AAD/e69DB7cwIc74nT356SvHMIRfYVkd/36dR05ckT58uWTt7e3s8OBi7jbfnO/tSVHugEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAE+M+Ph4Z4cAF5Ic+0uaZIgDAAAAAB5rnp6ecnNz08mTJ5UtWzZ5enrKYrE4Oyw8powxio2N1dmzZ+Xm5iZPT8+Hfi6KbgAAAACpnpubm/Lly6dTp07p5MmTzg4HLsLHx0d58uSRm9vDDxKn6AYAAADwRPD09FSePHkUFxcnq9Xq7HDwmHN3d1eaNGkeeUQERTcAAACAJ4bFYpGHh4c8PDycHQqeEEykBgAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgFN0AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg6RxdgAAkJKCei92dgip0tGPazs7BABIfQb6OTuC1GdglLMjwBOII90AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDOL3oHj9+vIKCguTt7a0yZcpo06ZNd11+9OjRCgkJUdq0aZU7d2517dpV169fT6FoAQAAAAC4f04tumfPnq1u3bppwIAB2rJli4oWLaoaNWrozJkzSS4/c+ZM9e7dWwMGDNCePXv09ddfa/bs2Xr//fdTOHIAAAAAAO7NqUX3qFGj1KZNG7Vs2VJhYWH64osv5OPjoylTpiS5/Pr161W+fHm9/vrrCgoKUvXq1dWkSZN7Hh0HAAAAAMAZnFZ0x8bGavPmzQoPD/9fMG5uCg8P14YNG5Jc57nnntPmzZttRfbhw4e1ZMkS1apV646vExMTo0uXLtndAAAAAABICWmc9cLnzp2T1WqVv7+/Xbu/v7/27t2b5Dqvv/66zp07pwoVKsgYo7i4OL399tt3HV4+bNgwDRo0KFljBwAAAADgfjh9IrUHsXbtWg0dOlQTJkzQli1bNG/ePC1evFhDhgy54zp9+vRRVFSU7XbixIkUjBgAAAAA8CRz2pHurFmzyt3dXREREXbtERERCggISHKdfv366c0331Tr1q0lSc8884yuXr2qtm3bqm/fvnJzS/wbgpeXl7y8vJJ/AwAAAAAAuAenHen29PRUyZIltWrVKltbfHy8Vq1apXLlyiW5TnR0dKLC2t3dXZJkjHFcsAAAAAAAPASnHemWpG7duql58+Z69tlnVbp0aY0ePVpXr15Vy5YtJUnNmjVTzpw5NWzYMElSnTp1NGrUKBUvXlxlypTRwYMH1a9fP9WpU8dWfAMAAAAA8LhwatHdqFEjnT17Vv3799fp06dVrFgxLV261Da52vHjx+2ObH/wwQeyWCz64IMP9N9//ylbtmyqU6eOPvroI2dtAgAAAAAAd+TUoluSOnbsqI4dOyb52Nq1a+3up0mTRgMGDNCAAQNSIDIAAAAAAB6NS81eDgAAAACAK6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB3F60T1+/HgFBQXJ29tbZcqU0aZNm+66fGRkpDp06KAcOXLIy8tLTz/9tJYsWZJC0QIAAAAAcP/SOPPFZ8+erW7duumLL75QmTJlNHr0aNWoUUP79u1T9uzZEy0fGxuratWqKXv27JozZ45y5sypY8eOKWPGjCkfPAAAAAAA9+DUonvUqFFq06aNWrZsKUn64osvtHjxYk2ZMkW9e/dOtPyUKVN04cIFrV+/Xh4eHpKkoKCglAwZAJBCgnovdnYIqdLRj2s7OwQAAJ4oThteHhsbq82bNys8PPx/wbi5KTw8XBs2bEhynZ9++knlypVThw4d5O/vr8KFC2vo0KGyWq0pFTYAAAAAAPfNaUe6z507J6vVKn9/f7t2f39/7d27N8l1Dh8+rNWrV+uNN97QkiVLdPDgQbVv3143btzQgAEDklwnJiZGMTExtvuXLl1Kvo0AAAAAAOAunD6R2oOIj49X9uzZNWnSJJUsWVKNGjVS37599cUXX9xxnWHDhsnPz892y507dwpGDAAAAAB4kjmt6M6aNavc3d0VERFh1x4REaGAgIAk18mRI4eefvppubu729pCQ0N1+vRpxcbGJrlOnz59FBUVZbudOHEi+TYCAAAAAIC7cFrR7enpqZIlS2rVqlW2tvj4eK1atUrlypVLcp3y5cvr4MGDio+Pt7Xt379fOXLkkKenZ5LreHl5KUOGDHY3AAAAAABSglOHl3fr1k2TJ0/W9OnTtWfPHr3zzju6evWqbTbzZs2aqU+fPrbl33nnHV24cEHvvvuu9u/fr8WLF2vo0KHq0KGDszYBAAAAAIA7cuolwxo1aqSzZ8+qf//+On36tIoVK6alS5faJlc7fvy43Nz+97tA7ty5tWzZMnXt2lVFihRRzpw59e6776pXr17O2gQAAAAAAO7IqUW3JHXs2FEdO3ZM8rG1a9cmaitXrpz+/PNPB0cFAAAAAMCjc6nZywEAAAAAcCUU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgD110z5gxQ+XLl1dgYKCOHTsmSRo9erQWLlyYbMEBAAAAAODKHqronjhxorp166ZatWopMjJSVqtVkpQxY0aNHj06OeMDAAAAAMBlPVTR/fnnn2vy5Mnq27ev3N3dbe3PPvusduzYkWzBAQAAAADgyh6q6D5y5IiKFy+eqN3Ly0tXr1595KAAAAAAAEgNHqrozpcvn7Zt25aofenSpQoNDX3UmAAAAAAASBXSPMxK3bp1U4cOHXT9+nUZY7Rp0yZ9//33GjZsmL766qvkjhEAAAAAAJf0UEV369atlTZtWn3wwQeKjo7W66+/rsDAQI0ZM0aNGzdO7hgBAAAA4PE10M/ZEaQ+A6OcHUGyeeCiOy4uTjNnzlSNGjX0xhtvKDo6WleuXFH27NkdER8AAAAAAC7rgc/pTpMmjd5++21dv35dkuTj40PBDQAAAABAEh5qIrXSpUtr69atyR0LAAAAAACpykOd092+fXt1795d//77r0qWLKl06dLZPV6kSJFkCQ4AAAAAAFf2UEV3wmRpnTt3trVZLBYZY2SxWGS1WpMnOgAAAAAAXNhDFd1HjhxJ7jgAAAAAAEh1Hqrozps3b3LHAQAAAABAqvNQRbckHTp0SKNHj9aePXskSWFhYXr33XcVHBycbMEBAAAAAODKHmr28mXLliksLEybNm1SkSJFVKRIEW3cuFGFChXSihUrkjtGAAAAAABc0kMd6e7du7e6du2qjz/+OFF7r169VK1atWQJDgAAAAAAV/ZQR7r37NmjVq1aJWp/6623tHv37kcOCgAAAACA1OChiu5s2bJp27Ztidq3bdum7NmzP2pMAAAAAACkCg81vLxNmzZq27atDh8+rOeee06StG7dOg0fPlzdunVL1gABAAAAAHBVD1V09+vXT+nTp9fIkSPVp08fSVJgYKAGDhyozp07J2uAAAAAAAC4qocqui0Wi7p27aquXbvq8uXLkqT06dMna2AAAAAAALi6hyq6jxw5ori4OD311FN2xfaBAwfk4eGhoKCg5IoPAAAAAACX9VATqbVo0ULr169P1L5x40a1aNHiUWMCAAAAACBVeKiie+vWrSpfvnyi9rJlyyY5qzkAAAAAAE+ihyq6LRaL7VzuW0VFRclqtT5yUAAAAAAApAYPVXRXqlRJw4YNsyuwrVarhg0bpgoVKiRbcAAAAAAAuLKHmkht+PDhqlSpkkJCQlSxYkVJ0u+//65Lly5p9erVyRogAAAAAACu6qGOdIeFhemff/5Rw4YNdebMGV2+fFnNmjXT3r17Vbhw4eSOEQAAAAAAl/RQR7olKTAwUEOHDk3OWAAAAAAASFUe6Ej3uXPndOzYMbu2Xbt2qWXLlmrYsKFmzpyZrMEBAAAAAODKHqjo7tSpk8aOHWu7f+bMGVWsWFF//fWXYmJi1KJFC82YMSPZgwQAAAAAwBU9UNH9559/qm7durb733zzjTJnzqxt27Zp4cKFGjp0qMaPH5/sQQIAAAAA4IoeqOg+ffq0goKCbPdXr16t+vXrK02am6eG161bVwcOHEjWAAEAAAAAcFUPVHRnyJBBkZGRtvubNm1SmTJlbPctFotiYmKSLTgAAAAAAFzZAxXdZcuW1dixYxUfH685c+bo8uXLev75522P79+/X7lz5072IAEAAAAAcEUPdMmwIUOG6IUXXtC3336ruLg4vf/++8qUKZPt8VmzZqly5crJHiQAAAAAAK7ogYruIkWKaM+ePVq3bp0CAgLshpZLUuPGjRUWFpasAQIAAAAA4KoeqOiWpKxZs+rll1+23f/3338VGBgoNzc31a5dO1mDAwAAAADAlT3QOd1JCQsL09GjR5MhFAAAAAAAUpdHLrqNMckRBwAAAAAAqc4jF90AAAAAACBpj1x0v//++8qcOXNyxAIAAAAAQKrywBOp3a5Pnz7JEQcAAAAAAKlOsg4vP3HihN56663kfEoAAAAAAFxWshbdFy5c0PTp05PzKQEAAAAAcFkPNLz8p59+uuvjhw8ffqRgAAAAAABITR6o6K5Xr54sFstdLxNmsVgeOSgAAAAAAFKDBxpeniNHDs2bN0/x8fFJ3rZs2eKoOAEAAAAAcDkPVHSXLFlSmzdvvuPj9zoKDgAAAADAk+SBhpf36NFDV69evePjBQoU0Jo1ax45KAAAAAAAUoMHKrpz5sypfPny3fHxdOnSqXLlyo8cFAAAAAAAqcEDDS9/6qmndPbsWdv9Ro0aKSIiItmDAgAAAAAgNXigovv287WXLFly1+HmAAAAAAA8yR6o6AYAAAAAAPfvgYpui8WS6DrcXJcbAAAAAICkPdBEasYYtWjRQl5eXpKk69ev6+2331a6dOnslps3b17yRQgAAAAAgIt6oKK7efPmdvebNm2arMEAAAAAAJCaPFDRPXXqVEfFAQAAAABAqsNEagAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAO8lgU3ePHj1dQUJC8vb1VpkwZbdq06b7WmzVrliwWi+rVq+fYAAEAAAAAeAhOL7pnz56tbt26acCAAdqyZYuKFi2qGjVq6MyZM3dd7+jRo3rvvfdUsWLFFIoUAAAAAIAH4/Sie9SoUWrTpo1atmypsLAwffHFF/Lx8dGUKVPuuI7VatUbb7yhQYMGKX/+/CkYLQAAAAAA98+pRXdsbKw2b96s8PBwW5ubm5vCw8O1YcOGO643ePBgZc+eXa1atbrna8TExOjSpUt2NwAAAAAAUoJTi+5z587JarXK39/frt3f31+nT59Ocp0//vhDX3/9tSZPnnxfrzFs2DD5+fnZbrlz537kuAEAAAAAuB9OH17+IC5fvqw333xTkydPVtasWe9rnT59+igqKsp2O3HihIOjBAAAAADgpjTOfPGsWbPK3d1dERERdu0REREKCAhItPyhQ4d09OhR1alTx9YWHx8vSUqTJo327dun4OBgu3W8vLzk5eXlgOgBAAAAALg7px7p9vT0VMmSJbVq1SpbW3x8vFatWqVy5colWr5gwYLasWOHtm3bZrvVrVtXVatW1bZt2xg6DgAAAAB4rDj1SLckdevWTc2bN9ezzz6r0qVLa/To0bp69apatmwpSWrWrJly5sypYcOGydvbW4ULF7ZbP2PGjJKUqB0AAAAAAGdzetHdqFEjnT17Vv3799fp06dVrFgxLV261Da52vHjx+Xm5lKnngMAAAAAIOkxKLolqWPHjurYsWOSj61du/au606bNi35AwIAAAAAIBlwCBkAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEougEAAAAAcBCKbgAAAAAAHISiGwAAAAAAB6HoBgAAAADAQSi6AQAAAABwEIpuAAAAAAAchKIbAAAAAAAHoegGAAAAAMBBKLoBAAAAAHAQim4AAAAAAByEohsAAAAAAAeh6AYAAAAAwEEei6J7/PjxCgoKkre3t8qUKaNNmzbdcdnJkyerYsWKypQpkzJlyqTw8PC7Lg8AAAAAgLM4veiePXu2unXrpgEDBmjLli0qWrSoatSooTNnziS5/Nq1a9WkSROtWbNGGzZsUO7cuVW9enX9999/KRw5AAAAAAB35/Sie9SoUWrTpo1atmypsLAwffHFF/Lx8dGUKVOSXP67775T+/btVaxYMRUsWFBfffWV4uPjtWrVqhSOHAAAAACAu3Nq0R0bG6vNmzcrPDzc1ubm5qbw8HBt2LDhvp4jOjpaN27cUObMmZN8PCYmRpcuXbK7AQAAAACQEpxadJ87d05Wq1X+/v527f7+/jp9+vR9PUevXr0UGBhoV7jfatiwYfLz87PdcufO/chxAwAAAABwP5w+vPxRfPzxx5o1a5bmz58vb2/vJJfp06ePoqKibLcTJ06kcJQAAAAAgCdVGme+eNasWeXu7q6IiAi79oiICAUEBNx13U8//VQff/yxVq5cqSJFitxxOS8vL3l5eSVLvAAAAAAAPAinHun29PRUyZIl7SZBS5gUrVy5cndc75NPPtGQIUO0dOlSPfvssykRKgAAAAAAD8ypR7olqVu3bmrevLmeffZZlS5dWqNHj9bVq1fVsmVLSVKzZs2UM2dODRs2TJI0fPhw9e/fXzNnzlRQUJDt3G9fX1/5+vo6bTsAAAAAALid04vuRo0a6ezZs+rfv79Onz6tYsWKaenSpbbJ1Y4fPy43t/8dkJ84caJiY2PVoEEDu+cZMGCABg4cmJKhAwAAAABwV04vuiWpY8eO6tixY5KPrV271u7+0aNHHR8QAAAAAADJwKVnLwcAAAAA4HFG0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgFN0AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgFN0AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgFN0AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgFN0AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgFN0AAAAAADgIRTcAAAAAAA5C0Q0AAAAAgINQdAMAAAAA4CAU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0AwAAAADgIBTdAAAAAAA4CEU3AAAAAAAOQtENAAAAAICDUHQDAAAAAOAgFN0AAAAAADgIRTcAAAAAAA7yWBTd48ePV1BQkLy9vVWmTBlt2rTprsv/+OOPKliwoLy9vfXMM89oyZIlKRQpAAAAAAD3z+lF9+zZs9WtWzcNGDBAW7ZsUdGiRVWjRg2dOXMmyeXXr1+vJk2aqFWrVtq6davq1aunevXqaefOnSkcOQAAAAAAd+f0onvUqFFq06aNWrZsqbCwMH3xxRfy8fHRlClTklx+zJgxqlmzpnr06KHQ0FANGTJEJUqU0Lhx41I4cgAAAAAA7s6pRXdsbKw2b96s8PBwW5ubm5vCw8O1YcOGJNfZsGGD3fKSVKNGjTsuDwAAAACAs6Rx5oufO3dOVqtV/v7+du3+/v7au3dvkuucPn06yeVPnz6d5PIxMTGKiYmx3Y+KipIkXbp06VFCTxHxMdHODiFVckTf01eOQV+5DvrKdTgk/8WY5H9OSPSV63DU90r6K/nRV67DBeq1hJxqzN3736lFd0oYNmyYBg0alKg9d+7cTogGjwO/0c6OAPeLvnId9JXroK9cyMd+zo4A94u+ch30letwob66fPmy/PzuHK9Ti+6sWbPK3d1dERERdu0REREKCAhIcp2AgIAHWr5Pnz7q1q2b7X58fLwuXLigLFmyyGKxPOIWQLr5C0/u3Ll14sQJZciQwdnh4C7oK9dBX7kO+sp10Feug75yHfSV66Cvkp8xRpcvX1ZgYOBdl3Nq0e3p6amSJUtq1apVqlevnqSbRfGqVavUsWPHJNcpV66cVq1apS5dutjaVqxYoXLlyiW5vJeXl7y8vOzaMmbMmBzh4zYZMmTgD9hF0Feug75yHfSV66CvXAd95TroK9dBXyWvux3hTuD04eXdunVT8+bN9eyzz6p06dIaPXq0rl69qpYtW0qSmjVrppw5c2rYsGGSpHfffVeVK1fWyJEjVbt2bc2aNUt///23Jk2a5MzNAAAAAAAgEacX3Y0aNdLZs2fVv39/nT59WsWKFdPSpUttk6UdP35cbm7/m2T9ueee08yZM/XBBx/o/fff11NPPaUFCxaocOHCztoEAAAAAACS5PSiW5I6dux4x+Hka9euTdT22muv6bXXXnNwVLhfXl5eGjBgQKJh/Hj80Feug75yHfSV66CvXAd95TroK9dBXzmPxdxrfnMAAAAAAPBQ3O69CAAAAAAAeBgU3QAAAAAAOAhFNwAAAAAADkLRDQAAAACAg1B0A4CLi42NdXYIAADcF3IWnkQU3UhV4uPj7e4zOf/j7/Y+w4P54osvNHHiRF26dMnZoQB4AOQr10O+enTkLDypKLqRqri53dylV65cqRs3bshisTg5ItxLQp8tXLhQBw8edHI0rmfNmjUaPXq0fvjhB77EuIidO3fa/j9x4kTt2rXLidHAWchXrod89ejIWa6HnJU8KLqR6vz+++/q0KGD9u/fL4lfph93xhjt2bNHzZs31+HDhyXRZ/cj4ajY7NmzVaNGDY0YMULff/+9oqKinBwZ7mbr1q1688039fHHH6tr167q0KGDvLy8nB0WnIR85VrIVw+PnOWayFnJx2IYz4RUJioqSs8884waNWqkESNGODsc3Kc333xTBw8e1KpVq+Tj4+PscFxCbGysPD09de3aNTVq1EjHjh1Thw4d9Prrr8vX19fZ4SEJFy9e1PDhwzVt2jRFR0frjz/+UJEiRRQXF6c0adI4OzykMPKVayJfPRxylushZyUfjnTDpSX8wpzw740bN+Tn56ehQ4dqxYoV2rFjhzPDQxJu/50vYUKVFi1aKC4uTps3b5bE0YN7McbI09NT33//verWrasbN27oxIkT6tWrl77//ntdvnzZ2SHiFsYYGWOUKVMmhYaGKiYmRnny5NGSJUtkjFGaNGlktVqdHSYciHzleshXyYec5VrIWcmPohsuyxhjO79q69atkiQPDw9JUqFChXTt2jVt375dEgnxcWGMsZ23uHDhQkVGRsrT01OSVLlyZUnShAkTJP3v3DkkzWKxaPPmzWrXrp2aNm2qr776SidOnFD16tU1aNAgzZo1S1euXHF2mNDNzx+LxSKLxaJ///1XRYsW1bp161SnTh3Nnz9fgwcPliS5u7s7OVI4CvnK9ZCvkhc5y3WQsxyDTwm4pIQPBEn6888/VaZMGb3yyiv6+uuvFRcXp+LFi+uNN97QBx98oFOnTpEQHwO39tkvv/yiESNGKDg4WMOHD9evv/6qNGnSaPjw4dq5c6dWrVrl5GgfT7cfdTlx4oSyZ8+u6tWrK2fOnEqXLp1mz56t5557Tr169dKsWbMUGRnpnGAh6eZ+n/D5079/f7355pu6dOmSwsLC1L17d1WoUEFLlizRhx9+aFunf//+tnN84frIV66HfJU8yFmuh5zlOHyywyUlfCC0bdtWX3/9tbZs2SJPT099+eWXCg0N1bfffquQkBCVKlVKf/zxhyQxDMaJbj3K895772ngwIGaP3++unfvrnXr1unll19W165dtWfPHmXNmlV79+6VxBGf2yV8CVy+fLkOHz6suLg4RUdH246+XL16VZI0btw4xcfHa/DgwVqwYAGXInKihP2+X79+mjRpkjp37qynnnpKkpQ1a1a9//77qlixohYuXKgGDRqodu3a+vLLLxUcHOzMsJGMyFeuhXyVfMhZroec5UAGcCHx8fG2///111+mQIECZunSpcYYY6Kjo82pU6dMhw4dTLVq1Uzu3LmNxWIxL7/8spOihTH2fbZ582ZTsWJFs27dOlvbuXPnzLJly8zzzz9vatWqZSwWi/H39zdHjhxxQrSPv3Xr1hmLxWIWLFhgrly5YoKDg029evXsltm/f7955ZVXzJtvvmkOHjzopEiRYN++faZgwYJm4cKFdu1xcXHGGGMuXLhgRo8ebRo0aGBef/11Exsba4wxxmq1pnisSD7kK9dDvkp+5CzXQ85yDGYvh0uaMmWKfv/9d2XOnFkjR46U1Wq1O7fk0KFD2rlzp0aNGqV9+/Zp/PjxevXVV50YMWbPnq1vvvlG6dKl06xZs3Tjxg15eXnZzps7f/68zp8/r4kTJ2rBggV677331KFDB7uhTk+6vXv3at++fTpw4IDee+89SdKKFSvUrFkzlSpVSp9//rmsVqumTZumLVu26Mcff1TatGmdHDU2bdqkOnXq6M8//1S+fPnszhWNiYmRp6dnoms0MzNs6kG+cj3kq+RBznJN5CzH4JMBLuf06dNaunSp5s+frzNnzki6OZmDMcY2vCs4OFgvv/yyvvvuOxUvXlwbN250ZshPvOvXr2vt2rX6559/tH//frm5ucnLy0tWq9X2wZ0lSxY9/fTT+uyzz1SzZk1NmTJFEhPUJPjvv/9UpUoVNWrUyDaDriRVqlRJ3333nfbv368yZcro+eef15dffqlBgwbx5cUJkvodO3PmzLp+/brtc8hisdiGD69du9Y2G+ytz8GXl9SBfOV6yFfJg5zlGshZKYdPBzz2bv9ACAgIUPfu3VWnTh3NmTNH8+bNkyTbTIsJ4uPjlStXLtWvX1/z58/XxYsXUzTuJ9nt57Z5e3vro48+UvPmzRUREaEePXooPj5e7u7udsvGxcVJkt555x1FR0fr0KFDKRr348zX11f9+/dXQECA7TI1kuTl5aXnn39eO3bs0PTp0zV16lT99ddfKlmypBOjfXIlfAaNHj1aS5culdVqVfbs2RUeHq7p06dr5cqVkm4WXlarVSNGjNDixYvtPrtuP4IA10G+cj3kK8cgZ7kGclbK4WcJPNZuHap16dIlWa1WZcqUSWXKlJGvr6+MMfrggw/k4eGhOnXqyGKx2IbBJKy3e/duZc6c2XZ5FjjWrX126NAheXl5yWKxKGfOnHrvvfd048YNrVmzRgMHDtTgwYPl5uZmG26Z8Evp5MmTdfHiRWXKlMmZm/JY8fPzU6NGjeTh4aFu3bqpY8eOGjdunKSb14719PRUjRo1nBwlEixevFiDBg3S3Llz9fzzz6tTp04aMmSI3n//fa1evVpZs2bVTz/9pAsXLmjp0qXODhfJgHzleshXjkPOci3kLMej6MZjy9wyg+jQoUO1aNEiRUZGKk+ePProo49UsmRJ9enTR8OHD1efPn1ksVj00ksv2X5xM8YoOjpaW7du1fjx4+Xr6+vMzXki3Npn/fr107x582zX3Xz//ffVrl079e3bV8YYLV++XG5ubho4cGCiaz1myZJFCxYsUObMmVN8Gx4HCV/Ed+zYoaNHjyo+Pl4vvPCCsmTJogYNGkiS+vbtK4vFos8//1yenp6JzhNFyknqPM4VK1aoYcOGatiwoWbPnq0XXnhB6dOn14IFC/Tdd98pX758ypMnj1asWKE0adJwPpyLI1+5HvJV8iFnuRZylpOkxGxtwKPo37+/yZIli/n888/NV199ZcqVK2fy589v5s6da4y5OcPoW2+9ZbJkyWJ+//1323oJs5DeuHHDKXE/yYYNG2ayZMliFi1aZBYuXGgGDRpkLBaL6devnzHm5gywPXv2NPnz5zeTJ0+2rXfrzLFPqoT3YN68eSYoKMiEhISY4sWLm0KFCpmTJ08aY27OHDpp0iSTI0cO06JFC2eGi1ucO3fOGGO/H9evX99kyZLFrFy50tZ2/fp1u88lPqNSD/KV6yFfPRpylusiZ6Usim481k6ePGkKFy5sZs6caddev359ky9fPvPvv/8aY25ekuLDDz+0Xc7gViTGlHX9+nVTrVo18/HHH9u1T58+3VgsFjN//nxjjDFnz54148ePT7LPnnSrVq0yGTNmNJMmTbLdt1gsJn/+/LbLqVy4cMGMHTvWFChQwJw+fdqZ4T6RvvzyS7Njxw7b/WnTppksWbKYXbt2GWPsP3deeuklkzt3brN69WoTExNj9zx8PqUe5CvXQ75KHuSsxx85y/kouvFYuf2P+dixYyZnzpxm+fLlxhhjrl27ZnusQIEC5r333kv0HCTFlHV7n124cMHkzZvXjBo1yhhzsz8S+uT111839erVs+vHhGVw0+XLl03nzp3Nhx9+aIwx5r///jN58uQxTZs2NeXLlzd58+Y1R48eNcYYc/HiRXPx4kUnRvtkWr9+vXF3dzft27c3e/fuNcbc7ItSpUqZ0NBQs3v3bmPM/65ZmvAF1Nvb2/z9999OixvJi3zleshXyY+c9fgjZz0emL0cjw1zy3UAL1++LEnKkyePfH19NXPmTEk3ZxVNuPREwYIFE806KolzhFJYQp9duHBBkpQpUybVqFFDU6dO1dGjR+36I2PGjLJYLPL29rZ7jie5z8zNHz8lSVarVb6+vqpevbpq1KihyMhI1a1bVy+++KJmzJih7t276/jx4ypevLiOHDmijBkzKmPGjM7dgCdQuXLlNGvWLP38888aO3asdu3apYwZM2rlypXKmDGjXn75Ze3evdt2zpynp6d69+6t9957T0WLFnVy9EgO5CvXRL56dOQs10POejxQdOOxEB8fb0uGEyZM0KBBg3T48GFJ0sCBA7V27Vr16dNH0s0PA0k6c+aM0qdP75yAYfcFcuzYsapTp47++ecfSVKTJk2UNWtWvffeezpx4oTc3d0VGxurffv2KSAgwFkhP3YSvrhYLBb99NNPeuONNyRJtWvX1rPPPquNGzfK09NTvXr1knRzwp46deooPDxcN27ccFrcT7KE971Bgwb68MMP9fPPP2vSpEnat2+fMmTIoKVLlypbtmyqU6eOFixYoK1bt+rTTz/VlStXNGTIEKVJk8Z2vVO4JvKV6yFfJQ9ylushZz0+mHYOTnfrLIr79+/X0qVLtXHjRqVLl06dOnVS3bp19d9//2nEiBH6888/FRISol27duny5cv64IMPnBz9k+nWPvvjjz8UFxenDRs2aPDgwRo+fLiqVKmiU6dOadKkSSpWrJhKliyps2fP6saNG7ZLTdx6pOhJdOuXlzlz5qhhw4aSpLfeekvVq1eXJJ04cUKbN2+Wv7+/JGn58uXKkCGDJk+enOjoCxzPGGO7lNOQIUN0/fp1Xbt2TePHj9fVq1fVo0cPhYSE6Ndff1W9evXUqlUr+fj4KEeOHPrxxx9tz/OkHylzZeQr10O+Sh7kLNdDznrMOGNMO5CULl26mKJFi5o333zTVKhQwbi5uZn333/fXLhwwcTGxprff//dvPLKK+aNN94wnTt3ts2eyPlVztOzZ0+TI0cO89FHH5lWrVqZjBkzmqpVq5ojR44YY4w5cOCA+fzzz817771nRowYYeszZr7837mFs2fPNu7u7ubzzz83ZcqUMQsWLLAtExERYYoVK2YyZcpknn/+eZMuXTqzfft2Z4WM//fJJ58YPz8/s2rVKvPHH3+YCRMmmPTp05u2bdvazpczxpgNGzaYTZs22T6j2O9TD/KV6yFfPRpylusiZz0eKLrxWFi0aJHJlCmT2bJli+2PfejQoSZjxoymT58+5tSpU0muxweC8/z9998ma9asdpeV2LVrl8mWLZt54YUXzL59+5Jcjy+d/zNv3jxjsVjMV199ZYwx5vnnnzfTpk0zxtz8ghMfH2/27dtn+vbta/r27Wv27NnjzHBhbk40U6tWLdOlSxe79pkzZxoPDw/Ttm1buxliE7Dfpx7kK9dDvkoe5CzXQ856fDC8HI+FmJgYZcmSRf7+/rYhXH369NGNGzc0ePBgeXp6qmXLlsqbN6/demnSsAs7i9Vqlaenp/LkySPp5nlDYWFh+uWXX1ShQgUNGDBA/fv3V2hoqN16DFO6yRij1atX69tvv9Xrr78u6eZ7s3XrVjVv3lzGGLm5uSl//vz68MMP7YZIwjni4+MVHx+vuLg42zlusbGxSpMmjZo0aaKNGzdq2rRpiomJ0cCBAxUUFGRbl/0+9SBfuR7y1aMjZ7kectbjhb8GpDjz/+cFJfwr3fxgiIiIUExMjNzc3HTt2jVJUps2beTn56epU6fqhx9+UGxsrN16SBlJ9VmOHDl08eJFrVq1SpLk4eGh+Ph4BQUFKTg4WHPnzlWfPn1sE9jQb/YsFovGjBmj119/3fYeBQQEKCoqSpLk5uam7t27q2XLlrYvM0hZt8827ebmpjRp0qhSpUr66quvtH//fnl6etr27UyZMik0NFRXrlyxfbmHayNfuR7ylWOQsx5/5KzHG38RSFG3zvoaFxdnN6ti8eLFVatWLV27dk1p06aVdPOIQuPGjdWkSRMNGDBABw8efOInM0lpt/bZuXPndPnyZV24cEG5c+dW9+7dNXToUH3//feSbn7Ae3t7q3Llylq2bJmWL1+u8ePHS9IT328JSS4iIkIXLlzQyZMnE30pCQkJ0dmzZyVJffv21fjx49WxY8cn/r1zhluP0mzbtk3r16/X7t27Jd3sm8qVK6ty5cratm2brl+/rpiYGG3ZskU9evTQjz/+KDc3tyQvEQXXQb5yPeSr5EPOci3krMcfY52QYm79QPj888+1Zs0aRUdHKzg4WOPHj9fYsWPVunVrFSpUSCNHjpSbm5smTpwoLy8vLVy4UDNmzNDChQsVFhbm5C15ctz6a/XQoUO1cuVKnTt3Tjly5NBHH32kbt26KTIyUh06dNDff/+tPHny6KefflJUVJQ+//xzlSlTRnv27HHyVjif+f+Zb3/++WcNHTpUkZGRSpcunbp3764mTZrY3mMfHx9dunRJgwYN0qeffqoNGzaoRIkSTo7+yXPrft+7d2/Nnz9fp0+fVu7cuZU/f3799NNPmjZtmt555x2VLVtWoaGhunr1qtzc3FS3bl1ZLBaO9Lg48pXrIV8lH3KWayFnuYiUOHEcuFWvXr1MQECA+fjjj823335rLBaLadiwobl8+bI5cOCAadiwocmVK5fJnz+/qVy5srl+/bq5ceOGKVy4sJk5c6azw38i9e3b12TJksXMmTPHrFy50pQuXdr4+fmZy5cvm2PHjpkvv/zShISEmHLlypk6deqY2NhYY4wxVapUMYMGDTLG/G/m0yfVzz//bNKlS2dGjhxpVq9ebbp162Y3IY0xxkyfPt1YLBaTOXNm8/fffzsxWhhjzOjRo03mzJnN2rVrzdatW82sWbNMSEiIee6552zL/PDDD2bcuHFm7NixzFCdCpGvXA/5KnmQs1wPOevxRtGNFLV9+3YTGhpq1qxZY4wx5pdffjG+vr5mwoQJdssdPHjQRERE2BJf3759Tb58+WyX9kDK+ffff025cuXMihUrjDE3E3HGjBnNuHHj7Ja7evWq3ReVnj17msDAQHPgwIEUjfdxdPz4cfPCCy+YMWPGGGOM+e+//0xQUJApVqyYsVgsZuLEicaYm+9h5cqVzbZt25wZ7hPLarXa3X/99ddN37597R7ftGmTeeqpp0ynTp2SfA6+vKQe5CvXQ75KHuQs10DOci0U3XCohA+EhH+XL19uQkJCjDHGLFy40Pj6+povvvjCGGNMZGSk+f777+3W3759u2nbtq3JkiWL2bJlSwpG/uS6/Rf+HTt2mEyZMplz586ZxYsXG19fX7uE+9lnn5n//vvPtvyWLVtM165dTWBgIH32/06ePGn69+9vTp06ZU6ePGlCQ0NN27ZtzYULF0yjRo2MxWIxI0eOdHaYT7Rb9/uVK1ea2NhYU716dVOnTp1Ey/bo0cM8//zz5vr16ykZIhyMfOV6yFeOQc56/JGzXA+D9+EwUVFRdpM6SFKBAgXk7++voUOHqmnTpho5cqTatWsnSdq7d69mzJihHTt22J7D19dX5cqV0/r161W8ePEU34Ynzfnz520ToMyYMUOSlC9fPlWqVEkjRoxQ48aNNXLkSL399tuSpCNHjmjt2rXat2+f7TmeeuopVatW7YntM2OM7dIc58+f19WrV5UjRw717t1bAQEBGj9+vPLly6fhw4crU6ZMyp8/v3LmzKmPPvpI58+fZ9ZcJzD/f/6iJPXv31/vvvuujh07ptq1a+vMmTNatmyZ3fL58+fX5cuXFRMT44xw4QDkK9dDvkoe5CzXQ85yUc6s+JF6/fTTT6Zdu3bm/PnzplOnTsbT09OcP3/enDx50tSsWdN4enqa3r1725a/du2aqVWrlmnQoEGi4TK334dj/PLLLyYkJMQcOXLEdOnSxaRLl84cO3bMGGNMy5YtjcViMV27drUtf+XKFfPiiy+amjVr2vroST4PbvHixXZD7ObNm2fKly9vnnrqKTNgwACzefNmY4wx9erVM2+88YZtuS5dupipU6eaqKioFI8Z9v755x/z0ksvmV9//dUYY8zhw4dNuXLlzMsvv2zmzp1rrFarOXfunKlWrZpp3LjxE72/pybkK9dDvnp05CzXR85yLRZj+IkKye/7779Xp06dlDdvXh07dky//fabbRbX9evXq3nz5goJCVH58uWVPXt2zZw5U2fOnNGWLVts189kFsWUFRcXp9DQUEVHR+vKlSv69ddfVaxYMdvj4eHhOnr0qCpWrCh/f3/9+eefunDhgjZv3vzE91lERITKlSunKlWqqG/fvrpx44bKlSun7t2769y5c/r9998VFBSkvn37atu2bXrnnXfUq1cvnThxQosWLdL69ev11FNPOXsznmgTJkzQ7NmzZbVaNW/ePGXPnl2StGvXLr377rv677//FBkZqRw5cshqtervv/+Wh4eH3REHuCbyleshXz0acpbrI2e5HopuOEyjRo00Z84cNWzYUCNHjlRgYKDtsV9//VXffvutli9frpCQEOXMmVOTJ09WmjRpFBcXpzRpuJpdSrpx44Y8PDzUt29fDRs2TE8//bQWLFigp556Su7u7rblBgwYoJ07d8oYo5CQEA0ZMoQ++39btmxRu3btVLZsWfn7+0uSPvjgA0nS4sWLNXLkSPn5+alJkyY6duyYZsyYoaxZs2rUqFF2XxaRMm7/0r169Wq1bNlSZ86c0dy5c1WrVi3bY6dPn9bx48e1bt06BQYGqkGDBnJ3d2e/T0XIV66DfJU8yFmuhZzl+ii6keysVqvc3d01bNgweXh4aOzYsapTp47effddPf3007Zf2axWq6Kjo+Xl5SVPT09J4gPByX777TelT59eb775ptzd3TVlyhSVKFEi0fUbb+2nhP7GzS8x77zzjiIiItS4cWN9/PHHtscWLVqkzz77TFmyZNG7776r8uXL6+rVq0qXLp0TI34y3frl5eDBg/Ly8lLu3Ll1+PBhVatWTWFhYRowYICeffbZOz4H+33qQL5yXeSrR0fOcg3krFTCGWPakfrc7Ty2GTNmmJw5c5r27dub/fv329oTLsOSgHNNUtbtfXbrZSOuXr1qChYsaIoUKWK2bt1qax8xYkRKheeytm/fbvLly2fKly9vdu7caffYokWLTLFixczrr7/OLKJOcuvnTK9evUzBggVNlixZTMWKFc38+fPN4cOHTf78+c1rr71md91ZPp9SD/KV6yFfOQ456/FGzko9ONKNR3brL3Dff/+9jh49KqvVqvr16ys0NFQWi0Xfffed+vTpo5o1a+rll1/WhAkT9M8//+j48eOSxPklKezWPpswYYJ27typI0eOqGfPnnrmmWeUNWtWXb9+XSVKlFCaNGn0zjvv6KefftK+fft04MABfi29h3/++UfNmzdX6dKl1blzZxUqVMj2WMIQ1bx58zoxwifTrfv9rFmz1LVrV33xxReKjIzUzp07NWrUKE2dOlUVKlRQ9erVbf1XtmxZJ0eO5EK+cj3kK8cjZz2eyFmpjLOrfqQePXv2NNmyZTOvvfaaefrpp03VqlXN1KlTbb+2zZo1yxQrVswUKlTIVKxY0cTGxjo5YvTu3dv4+/ubjh07mqZNm5qsWbOa4cOHm6NHjxpjjLl+/boJDw83lSpVMuHh4bY+Y4bee9uyZYspUaKEad26tdm1a5ezw8Et1qxZY1q3bm1GjRpla7t06ZIZM2aM8fb2NuvWrTNbtmwxPj4+pn///k6MFI5CvnI95CvHImc9vshZqQNFN5LFuHHjTJ48eWxDW2bNmmUsFospW7asmTx5su2LzMGDB83evXttSfDGjRtOi/lJN336dBMUFGS2bNlijDHmr7/+MhaLxQQEBJhBgwaZEydOGGNuDuM7deqUrQ/ps/u3ZcsWU7p0adO4cWOzZ88eZ4cDY8ypU6dMcHCwSZ8+vfnwww/tHrtw4YKpW7eu6dChgzHGmK1bt9oNY0XqQL5yPeSrlEHOevyQs1KPJ/d6CUg2165d0+nTp9W9e3eVLFlS8+bN09tvv62hQ4fKx8dHn3zyiaZOnSpjjIKDgxUSEiI3NzfFx8czCY2T3LhxQ/Hx8erevbuKFy+uBQsWKDw8XN98843efvttffjhh/rmm2905MgRubu7KyAgQBaLhT57QMWLF9e4ceN06tQp+fn5OTscSAoICLBdXmXevHnaunWr7bFMmTIpW7ZsOnjwoCSpWLFicnd3l9VqdVa4SGbkK9dDvko55KzHDzkrFXF21Q/Xc+tQrYRfkffu3WtOnz5t9u3bZ0JCQsxnn31mjDHmjz/+MOnTpzeFChUy8+fPd0K0MMa+zxImQzl69Kg5efKkOX78uClWrJgZOXKkMcaY06dPm4wZMxofHx8zbdo0p8Sb2ly7ds3ZIeA227dvN0WLFjXNmjWzTb506dIl89xzz5k2bdo4NzgkG/KV6yFfOR856/FDznJ9/ASIB3LrpA6TJk1S2rRpVbNmTYWEhEiS1q5dKx8fHzVu3FiSdPHiRdWoUUMFCxZU3bp1nRb3k+zWPvv0008VFRWldu3a2SZF2bRpk2JjY1WxYkVJ0rlz59S0aVOFhYWpadOmTos7NfH29nZ2CLhNkSJFNHXqVDVt2lQvvviinn32WXl6euratWsaN26cJNkuFwXXRL5yPeSrxwM56/FDznJ9DC/HA0lIhj179lT//v0VHR2t+Ph42+PXrl1TTEyMtmzZovPnz2vSpEkKCwvTkCFD5ObmxpAXJ7i1z0aOHKkcOXLYDbmLiopSRESEduzYoU2bNql37946e/as3nnnHYYpIVUrXry4Zs+erbRp0yoqKkrVqlXTli1b5OnpqRs3bvDlxcWRr1wP+Qq4M3KWa+OSYXhg3333nXr06KFFixapRIkSdo/9+++/atiwoU6ePKm4uDhly5ZNmzZtkoeHB7/AOdGSJUvUpk0bLViwQKVKlUr0+DvvvKNZs2YpQ4YM8vf317p16+Th4eGESIGUt23bNr399tsqUqSIevbsqQIFCjg7JCQT8pXrIV8Bd0fOck0U3Xhg/fr1086dO/Xjjz/Kzc1Nbm5udl9QIiIi9Ndff+nq1atq0KCB3N3dFRcXx4QmTvTVV19p2rRpWr16tdzc3JQmTZpEXyr//vtvSTd/SaXP8KTZunWr3n77beXPn18DBgxQwYIFnR0SkgH5yvWQr4B7I2e5Hj6hcN8Skt7OnTt15coVW4JLOAcrLi5OGzZsUJEiRfTSSy/Z1rNarSRDJzt37pwOHDggDw8PWSwWWa1W21C8NWvWqFChQnr22Wdty9NneNIkzNrbo0cPZu1NBchXrot8BdwbOcv1cE437lvCr8yNGzfW9u3bNWvWLEn/OwcrIiJCo0ePtrucgSS5u7unbKCwSRjIUqVKFWXOnFkDBw7U5cuXbX1y9epVDR06VL/88ovdevQZnkSlSpXS0qVLlSNHDmeHgkdEvnI95CvgwZCzXAs/DSKRa9euKW3atHZtCcnQYrGoQoUKqlq1qsaOHatr167pjTfe0JEjR9SjRw9FRETYZhWFc9165kipUqX08ssva/ny5Tp79qw6d+6siIgIffLJJ7p06ZKaNWvmxEiBxwez9roW8lXqQL4CHg45y3VwTjfsdOjQQQUKFFCrVq2UIUMGSfZfYObPn698+fLJ29tb48eP1zfffCNvb29lzJhRmTNn1m+//SYPDw/bcDA4T8Lwyjlz5mj//v3q2bOnPv74Yy1atEibNm1S4cKFlTVrVi1btow+A+ByyFepB/kKQGpH0Q07r776qnbu3Kn33ntPjRo1UoYMGWznwM2fP1+vvvqqvvjiC7Vt21aXLl3SmTNntGXLFvn7+6tChQpMaOIEO3fuVOHChSVJEydOVKVKlRQWFiaLxaJ58+apefPmGj58uNq3b6/4+HgZY7R582Zly5ZNefPmtZ3fSJ8BcCXkK9dDvgLwpKLohqT/TS4jSW3bttVvv/2mrl27qkmTJsqQIYP++OMP1alTR8OHD1fbtm0lKclLqvDrc8raunWr3nrrLTVq1EgREREaM2aM9u/frwIFCmjLli2qXbu2Bg4cqHbt2t3xEji39j0APO7IV66JfAXgSUbRDUn2iezUqVNq0qSJzp8/r06dOqlZs2batWuXTp8+rdq1azs5Utzq4sWLGj58uKZNm6bo6Gj98ccfKlKkiCTp0qVL2rJli6pUqeLcIAEgGZGvXBP5CsCTjPE5kPS/GV3fffdd7dq1S2nSpFFUVJR69eolNzc3NW3aVCVLlrzjr89IWQm/lWXKlEmhoaGKiYlRnjx5tGTJEj3zzDOyWCzKkCEDX2AApDrkK9dCvgIAim7cYtasWfrmm2+0du1a5cuXTxkyZFCTJk00YMAAGWPUpEkT+fr68kXGyW49yvPvv/+qaNGiWrdunWbMmKH58+crJiZGAwYMcHKUAOA45CvXQL4CgJs4MeYJZrVa7e6fP39eQUFByp8/v3x9fSVJ33//vcqUKaP3339f33//vSIjI/kC40S3foHp37+/3nzzTV26dElhYWHq3r27KlSooCVLlujDDz+0rdO/f3/t37/fWSEDwCMjX7ke8hUA/A9F9xMsYQKZkSNH6ujRo7Jarbpw4YK8vb3l5uam6OhoSTeT4JUrV9SvXz/9+uuvzgz5iZfwBaZfv36aNGmSOnfurKeeekqSlDVrVr3//vuqWLGiFi5cqAYNGqh27dr68ssvFRwc7MywAeCRkK9cD/kKAP6HovsJFB8fb/v/119/rR49eujatWtq1qyZbVieJPn4+EiSYmJi1LRpU7Vr104vvfSSU2LG/+zfv19z5szRpEmT9MorryhHjhySbh4JypIli/r27aumTZvKYrEoY8aM+vfff+Xu7m7X7wDgCshXro18BQA3MXv5E2zZsmU6cuSIMmfOrIYNG0qSlixZorfeekslSpTQwIEDZYzRoEGDlCtXLk2aNEkSl1lxtk2bNqlOnTr6888/lS9fPrtzFmNiYuTp6ZloSCXXNQXgyshXrol8BQA3caT7CbVt2za98sor6tixo27cuGFrf+GFFzRnzhydOnVK9erV02uvvaYLFy5o/PjxtmX4ApNykvpNLHPmzLp+/bo2btwoSbJYLLbzHdeuXaslS5bYrWeM4QsMAJdFvnIN5CsAuDOK7idU3rx5NXr0aPn7+2vJkiW2di8vL1WoUEFbt27VL7/8osWLF2v9+vXy8PBQXFycEyN+MiUcARg9erSWLl0qq9Wq7NmzKzw8XNOnT9fKlSsl3fxiabVaNWLECC1evNjuyAETCQFwZeQr10C+AoA7Y3j5E+DWGURvdeHCBc2dO1fdu3dXs2bNNG7cOEk3h3x5eXnZLcsQPeeqVq2a/v77b82dO1fPP/+81q5dqyFDhujy5csKDw9X1qxZ9dNPP+nChQvasmULRwoAuCTylesjXwFAYnzSpXK3foH56aefdOrUKVksFjVu3FiZM2dWgwYNJEl9+/aVm5ubxo4dKy8vr0RffPgCk3KS+tK5YsUKNWzYUA0bNtTs2bP1wgsvKH369FqwYIG+++475cuXT3ny5NGKFSuUJk0azokD4HLIV66HfAUA94cj3anYrROW9O7dWz/88IMyZ84sLy8vXbhwQatWrVJgYKAuXryouXPnql+/fqpWrZq++eYbJ0cO6eZ1aLNkyWLXj6+++qp+/fVX2xcZ6eaRHnd3d9uXFr7AAHA15CvXRr4CgLvjnO5ULCHxjRkzRt98841++OEH/f3332rRooX27dun5557TocOHVKmTJn06quvqnfv3jp//jyX6nCCSZMmaefOnbb706dPV0hIiHbv3i2LxWKbaGbu3LkqV66cWrZsqTVr1ig2NlZeXl62Ly1MQgPAFZGvXAf5CgAeHEe6U6Fbf2k+c+aM3n//fVWrVk2NGjXSokWL9Prrr6tXr1765ZdfdOrUKa1evVp58+bV5cuX5evrK4vFcsfz6pD8NmzYoIoVK6pdu3bq3LmzQkJCFBkZqerVq+vKlSuaO3euQkNDbX2yevVqhYeHy8vLS3/88YdKlizp7E0AgIdCvnIt5CsAeDgU3anMrV8+YmNj5enpqaVLl6pQoUK6cOGC6tWrp549e+qdd97RF198ofbt28vLy0sHDx5Uzpw5Jdl/CULKmDNnjrp166Y6deqoffv2KlSokC5duqSaNWvq3LlzWrBggcLCwiRJf/zxh5YsWSJ3d3cNGDCAIwUAXBL5yjWRrwDgwfHpl4rc+gWmb9++2r59uxYtWqSaNWtKkpYuXaqQkBA1bdpUkpQtWza1atVK2bJlU0BAgO15+AKTcm7cuCEPDw81aNBA0dHR+uCDD5QmTRq1b99eISEhWrp0qV588UXVqVNHI0eOVN68efXpp58qT548Gjt2rCRm6gXgeshXrod8BQAPj6I7lbj1C0z37t312WefydPTU7t27VKhQoUkSadOndKff/4pDw8PXblyRTNmzFBISIiGDh0qiWSY0owx8vDwkCQNGTJE169f17Vr1zR+/HhdvXpVPXr0UEhIiH799VfVq1dPrVq1ko+Pj3LkyKEff/zR9jz0GQBXQr5yPeQrAHg0FN2pgDHG9gWma9eu+vbbb/XLL7+oU6dOioqKsi3XrFkzzZs3T1myZFHevHllsVg0Z84c2+Mkw5SVcIRmxIgRGjlypObNm6datWrpn3/+Ua9eveTu7q5u3bopJCREixYt0p9//il3d3eVKFFC7u7uzPoKwOWQr1wT+QoAHg2fgKlAQjLs06ePvv76a/36668qXry40qZNq9jYWNtyefPm1c8//6yFCxfK09NTb731ltKkScMRAyeKj4/X2rVr1bJlSz3//POSpPLlyytjxoxq3ry5JKlTp04qXLiwypYta1vParXyBQaAyyFfuS7yFQA8PD4FUwmr1Sqr1aoNGzbYhud5eHho27ZtqlKliqSbX3b8/f3VsWNHu/X4AuMc8fHxio+PV1xcnKxWq6SbkwmlSZNGTZo00caNGzVt2jTFxMRo4MCBCgoKsq1LnwFwVeQr10O+AoBHwzU2XNTt1yZ1d3fX8OHDVahQIVmtVsXHx8vHx0cRERG2ZcqXL69evXolWg8p4/Y+c3NzU5o0aVSpUiV99dVX2r9/vzw9PW3XOM2UKZNCQ0N15coV5cmTxxkhA8AjI1+5HvIVACQvLhnmgm6dhGbXrl3y9PSUxWJRgQIF7B5v3bq1vL29NW7cONWsWVOHDx/Wzp075fl/7d1/SF31H8fx19Wrbqmo0NKtzHLZSjOzqUWa+ccMtyZNGFFZbLaFW7SpYNOwqSuoEDY2MLeItghsk9aV/NUPpguWEWXeO/cLYb8SQmerrLkfzV3P94/Y/WpbwfdL55578vkA0evnePwcD9zX5+3nfM4JDbWy+zPS1HPm8Xh04cIFRUdH+x6rsnjxYnk8Hn3yySdKSkqS0+nUE088oRUrVqioqIhn0QKwJfLKfsgrAPjnUXTbzNRnktbX12vv3r26cOGCQkNDtXHjRhUXF/u2ra6ulsfjUVhYmI4cOaJjx44pJCSEG5r42dRzVl1drdbWVo2MjCg+Pl6JiYlqa2vTmTNntHbtWnV1denuu+/W+fPnFRQUpMOHD8vpdPIsWgC2Q17ZD3kFAOYgyWxm6gCmqalJzc3Nuu2227Rp0yY9++yzGh8fV2lpqSQpIiJCn3/+ubKyshjAWOjqOdu2bZveeecduVwuRUVFaXBwUHV1dcrOzlZvb69cLpc+/PBDjY6OanJyUmvXruXGQQBsi7yyH/IKAExiwHb6+vqMvLw8o7u72zAMw+jo6DCio6ONpUuXGg6Hw3j77bcNwzCM8fFxY8WKFcbExIRhGIbvM/zD6/VOe/30008bNTU109q/+eYbIykpyVi3bt1193HlyhVT+wgAZiKv7IG8AgBzseDGBow/rQCIjY1VQUGBsrOz1dPTo+eff15vvPGG9uzZo0WLFmnNmjV68803FR4ervfee09Op5MZAz8zpjyLtru7WxMTEzp79qwGBgZ82wQFBSkzM1PLli3TkSNH9Pvvv1+zH2YMANgJeWU/5BUAmI+iO8B5vV7f5V4nTpzQmTNndMstt+ill15SWFiY3n//fS1btkyrVq1SeHi4EhMTtXDhQnV2dsowDN8AiAGM/xhT1rPV1taqrKxM33//vR577DGNjo7qs88+m7Z9YmKizp07d91BDADYBXllP+QVAPgHRXeA2r59uzwej+8/xy+//LIef/xxpaSkaMOGDfruu+8kSQcPHlR4eLhCQkJ08eJF/fjjj6qvr9eBAwe4kYlFrv7dDx06JLfbraamJt1xxx0qLCxUUFCQtm/fLpfLpcnJSf30009yuVyaP3++IiMjLe45APzvyCv7Iq8AwD+4e3kAOnXqlHJzc7V48WJt2LBBR48e1QsvvKDGxkYNDAyoq6tL8+bN0yuvvKIvv/xSlZWVKikpkcfj0cTEhL799lsFBwdzB1ELNTU1qaWlRV6vVy6XSzfddJOkPx6ZU1ZWph9++EFjY2OaO3euvF6v+vr6FBISwjkDYCvklf2RVwBgPoruAOXxeLR69Wo9/PDDCgoKUnJyslatWiVJ6ujo0ObNmxUTE6Mnn3xSZ8+eVVtbm26++Wbt2LFDISEh3EHUz/78TNKenh6VlJRodHRUH330kZYsWeJrGxkZ0dDQkHp7ezVv3jwtX75cwcHBrGMEYEvklb2QVwDgfxTdAay/v1+lpaU6ceKEamtrVV5e7mtrb2/Xtm3bFB0drYqKCmVnZ/vaCEP/mjqAOX78uMLCwhQfH6+TJ08qPz9fycnJqqurU0ZGxl/ug0EnADsjr+yBvAIAa7CmO4Ddf//92rlzp2JiYtTV1aVDhw752goLC1VRUaHBwUG1t7f7vm8YBgMYP5p619fq6moVFhYqPT1dubm5GhgY0L59+3T06FE1NDT41jVe/bmpGMAAsDPyKvCRVwBgHWa6beDgwYMqKSlRRkaGysrKlJKS4mv76quv9MADDxCCFpg6Y7Bnzx5VVFRox44dGhsb0+HDh7Vlyxbt2rVLOTk5evTRR5WVlaX169frwQcftLjnAGAO8iowkVcAYC2Kbptwu91avXq1Fi5cqPLyciUnJ09r53Iv63zxxRdqbm5WcnKyKioqJEnnzp3Trl27VFVVpe7ubs2ePVs5OTmqrKzUpk2bLO4xAJiHvApc5BUAWIOi20bcbrdKS0uVkJCghoYG3X777VZ3acYbGRlRTk6ORkdHVVVVpZqaGl/bL7/8opUrVyo+Pl6NjY3yeDxKTU1lsAngX4+8CjzkFQBYhzXdNpKenq7GxkZFRkYqISHB6u5AUlxcnO8RKy6XS26329cWExOjOXPm6Pjx45Kk++67T8HBwfJ6vVZ1FwD8grwKPOQVAFiHottmsrKy9O677yooKEiTk5NWdweS7r33XrlcLnm9Xm3dulUej0fSH5fsHTt2TLfeeuu07Zk5ADATkFeBh7wCAGtweblNGYYhh8NhdTcwhdvt1jPPPKOff/5ZGRkZCg0N1alTp/T1118rNDSUcwZgRuK9L/CQVwDgX8x02xRhGHjS09PV0tKi2bNn69dff1V+fr76+/sVGhqqiYkJzhmAGYn3vsBDXgGAf1F0A/+ge+65Ry6XS5cvX1Z/f79vfVxISIjFPQMA4L/IKwDwHy4vB0zgdru1Zs0aJSYmqq6uTnfddZfVXQIA4BrkFQCYj5luwARX79w7PDysqKgoq7sDAMB1kVcAYD5mugETXbp0SbNmzbK6GwAA/C3yCgDMQ9ENAAAAAIBJuLwcAAAAAACTUHQDAAAAAGASim4AAAAAAExC0Q0AAAAAgEkougEAwP8lLy9P5eXlVncDAICARtENAMC/mMPh+NuP+vp6q7sIAMC/mtPqDgAAAPMMDw/7vm5paVFtba0GBwd934uIiLCiWwAAzBjMdAMA8C8WFxfn+4iKipLD4fC9Pn/+vIqLixUbG6uIiAhlZmZq3759036+qalJSUlJmjVrlmJjY7V8+fK//F2dnZ2KiopSc3Oz2YcFAIBtUHQDADBDjY+Pa8mSJeru7pbb7VZBQYEKCws1NDQkSerr69P69ev16quvanBwUJ9++qlyc3Ovu68PPvhATz31lJqbm1VcXOzPwwAAIKBxeTkAADNUWlqa0tLSfK9fe+01tba2qq2tTS+++KKGhoYUHh6upUuXKjIyUgkJCUpPT79mP2+99ZZqamrU3t6uRx55xJ+HAABAwKPoBgBghhofH1d9fb06Ozs1PDysK1eu6OLFi76Z7vz8fCUkJCgxMVEFBQUqKChQUVGRbrjhBt8+9u7dq9HRUfX29iozM9OqQwEAIGBxeTkAADNUZWWlWltb9frrr+vAgQPyeDxKTU3V5cuXJUmRkZHq7+/X7t27NXfuXNXW1iotLU1jY2O+faSnp2vOnDnauXOnDMOw6EgAAAhcFN0AAMxQvb29WrlypYqKipSamqq4uDidPn162jZOp1OLFi1SQ0ODBgYGdPr0afX09Pja58+fr/379+vjjz/WunXr/HwEAAAEPi4vBwBghkpKSpLL5VJhYaEcDoc2btyoyclJX3tHR4dOnjyp3NxcxcTEqKurS5OTk1qwYMG0/dx5553av3+/8vLy5HQ6tXXrVj8fCQAAgYuiGwCAGWrLli167rnn9NBDD+nGG29UVVWVfvvtN197dHS0XC6X6uvrdenSJSUlJWn37t1KSUm5Zl8LFixQT0+P8vLyFBwcrM2bN/vzUAAACFgOgwVYAAAAAACYgjXdAAAAAACYhKIbAAAAAACTUHQDAAAAAGASim4AAAAAAExC0Q0AAAAAgEkougEAAAAAMAlFNwAAAAAAJqHoBgAAAADAJBTdAAAAAACYhKIbAAAAAACTUHQDAAAAAGASim4AAAAAAEzyH5StNrne4gUnAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "index_title = range(len(f1_scores_title_df))  # Position for Title-Focused\n",
        "index_text = [i + len(f1_scores_title_df) for i in range(len(f1_scores_text_df))]  # Position for Text-Focused\n",
        "\n",
        "# Plotting all Title-Focused F1-scores\n",
        "plt.bar(index_title, f1_scores_title_df['F1-Score'], label='Title-Focused')\n",
        "\n",
        "# Plotting all Text-Focused F1-scores (shifted on the x-axis after the Title-Focused bars)\n",
        "plt.bar(index_text, f1_scores_text_df['F1-Score'], label='Text-Focused')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Task')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.title('F1-Scores for Title-Focused and Text-Focused Classification')\n",
        "\n",
        "# Adjusting x-ticks to show all tasks\n",
        "plt.xticks(range(len(f1_scores_title_df) + len(f1_scores_text_df)),\n",
        "           list(f1_scores_title_df['Task']) + list(f1_scores_text_df['Task']),\n",
        "           rotation=45)\n",
        "\n",
        "# Setting y-axis limit from 0 to 1\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Adding legend\n",
        "plt.legend()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqV9R8UN9_8F"
      },
      "source": [
        "## Justification for Using F1-Macro for Model Comparison:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixwbIPs9_8F"
      },
      "source": [
        "# Why F1-Macro is Ideal for Evaluating Augmented and Balanced Datasets\n",
        "\n",
        "## 1. Balanced Measure of Precision and Recall\n",
        "The **F1-Macro** score is a robust metric for evaluating classification performance, especially for datasets with augmented and balanced distributions. Unlike accuracy, which can be skewed by dominant classes, F1-Macro considers both **precision** (correct positive predictions) and **recall** (capturing actual positives) equally for each class. This ensures a fair assessment of all categories, regardless of their frequencies in the dataset.\n",
        "\n",
        "## 2. Addressing Class Imbalance with Augmentation\n",
        "The dataset was **augmented and rebalanced** to mitigate class imbalances. While frequent categories like allergens or ice cream were overrepresented, augmentation ensured underrepresented classes were adequately represented in the training set. F1-Macro evaluates the performance **per class** and takes the **unweighted mean** across all classes, ensuring that every class, including rare ones, contributes equally to the final score.\n",
        "\n",
        "## 3. Avoiding the \"Accuracy Paradox\"\n",
        "Imbalanced datasets often lead to misleading accuracy scores, where the model predicts majority classes well but fails on minority classes. F1-Macro avoids this \"accuracy paradox\" by equally weighting the F1 score of each class, ensuring the model's performance on **minority classes** like rare food hazards or products is fairly evaluated.\n",
        "\n",
        "## 4. Handling Undefined F1 Scores\n",
        "In real-world datasets, some classes may have no true or predicted examples, especially after augmentation. **F1-Macro**, when used with `zero_division=0`, avoids undefined metrics by assigning meaningful scores to missing classes. This ensures fair evaluation and reliable model comparisons, even when dealing with rare or newly introduced classes.\n",
        "\n",
        "---\n",
        "\n",
        "# Improvements in Training Function and Their Correlation with F1-Macro\n",
        "\n",
        "The training function was enhanced with several techniques to improve learning and ensure better generalization across all classes, aligning with the strengths of **F1-Macro**:\n",
        "\n",
        "## 1. Class-Weighted Loss\n",
        "- **What was added**: A weighted loss function was introduced to address class imbalance by giving more weight to underrepresented classes.\n",
        "- **Correlation with F1-Macro**: This improvement ensures the model learns effectively from all classes, contributing to a higher F1-Macro score by improving precision and recall for rare categories.\n",
        "\n",
        "## 2. Gradient Clipping\n",
        "- **What was added**: Gradient clipping prevents exploding gradients, stabilizing the training process.\n",
        "- **Correlation with F1-Macro**: A stable training process results in consistent improvements across all classes, reducing the risk of overfitting or poor performance on specific categories.\n",
        "\n",
        "## 3. Learning Rate Scheduler\n",
        "- **What was added**: A linear decay scheduler was implemented to adjust the learning rate dynamically during training.\n",
        "- **Correlation with F1-Macro**: Optimizing the learning rate helps the model converge better, ensuring improved recall and precision, especially for difficult or rare classes.\n",
        "\n",
        "## 4. Dataset Augmentation and Balancing\n",
        "- **What was done**: The dataset was augmented and balanced to address class imbalances, creating a more representative training set.\n",
        "- **Correlation with F1-Macro**: Augmentation and balancing directly complement the F1-Macro metric, as they improve the model's ability to generalize across all classes, reducing the likelihood of bias toward dominant categories.\n",
        "\n",
        "---\n",
        "\n",
        "# Why F1-Macro and Training Improvements Align\n",
        "- **Comprehensive Class Coverage**: The F1-Macro metric evaluates performance equally across all classes, while the training function's enhancements ensure the model learns effectively from all categories, even underrepresented ones.\n",
        "- **Improved Generalization**: Techniques like weighted loss, gradient clipping, and a learning rate scheduler enhance the model's generalization capability, leading to higher F1-Macro scores.\n",
        "- **Better Representation**: The dataset augmentation and balancing efforts align directly with F1-Macro’s strength in evaluating performance in skewed distributions.\n",
        "\n",
        "By combining these training improvements with the F1-Macro metric, the model's performance can be evaluated holistically, ensuring reliable predictions even for rare and critical categories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb6A-WG39_8F"
      },
      "source": [
        "## Analysis of F1-Scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfDXhjd_aEp2"
      },
      "source": [
        "# Collected F1-Scores for Title-Focused Classification\n",
        "\n",
        "| Task               | F1-Score |\n",
        "|--------------------|----------|\n",
        "| hazard-category    | 0.9598   |\n",
        "| product-category   | 0.9710   |\n",
        "| hazard             | 0.8381   |\n",
        "| product            | 0.6066   |\n",
        "\n",
        "### Analysis:\n",
        "- **hazard-category**: F1-Score = 0.9598  \n",
        "  Excellent performance, indicating the model effectively identifies hazard categories from titles.\n",
        "\n",
        "- **product-category**: F1-Score = 0.9710  \n",
        "  Very strong results in classifying product categories from titles, showcasing the model's ability to generalize well.\n",
        "\n",
        "- **hazard**: F1-Score = 0.8381  \n",
        "  Good performance in predicting specific hazards from titles. While solid, further optimization might improve granularity.\n",
        "\n",
        "- **product**: F1-Score = 0.6066  \n",
        "  Moderate performance, indicating some difficulty in extracting product-related information from titles. This task shows room for improvement.\n",
        "\n",
        "---\n",
        "\n",
        "# Collected F1-Scores for Text-Focused Classification\n",
        "\n",
        "| Task               | F1-Score |\n",
        "|--------------------|----------|\n",
        "| hazard-category    | 0.9566   |\n",
        "| product-category   | 0.9393   |\n",
        "| hazard             | 0.8370   |\n",
        "| product            | 0.5500   |\n",
        "\n",
        "### Analysis:\n",
        "- **hazard-category**: F1-Score = 0.9566  \n",
        "  Great performance, showing the model's ability to extract hazard categories from text descriptions with high accuracy.\n",
        "\n",
        "- **product-category**: F1-Score = 0.9393  \n",
        "  Strong classification results for product categories in text-focused tasks, although slightly lower than title-based classification.\n",
        "\n",
        "- **hazard**: F1-Score = 0.8370  \n",
        "  Good results in identifying hazards from full-text descriptions. This shows the advantage of richer context provided by longer texts.\n",
        "\n",
        "- **product**: F1-Score = 0.5500  \n",
        "  Performance is still lower compared to other categories, showing that product classification remains a challenging area.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Insights:\n",
        "- **Enhanced Performance with Text**:  \n",
        "  Text-focused classification generally outperforms title-focused classification, highlighting the importance of contextual richness for better model predictions.\n",
        "\n",
        "- **Strength in Hazard-Related Tasks**:  \n",
        "  Both **hazard-category** and **hazard** classifications show high F1-scores, particularly with text-based inputs, suggesting the model performs well with these tasks.\n",
        "\n",
        "- **Challenges in Product Classification**:  \n",
        "  Despite some improvement in the text-based task, **product** classification continues to be the weakest, pointing to the need for more targeted model improvements.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion:\n",
        "Since the text-focused classification approach provides richer context and outperforms title-focused classification in most cases, we will proceed with text-based classification for more detailed insights. This approach will be essential for improving **hazard-related tasks** and addressing the challenges in **product-related classifications** moving forward.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
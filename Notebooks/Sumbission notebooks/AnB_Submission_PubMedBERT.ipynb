{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/steliosg23/PDS-A2/blob/main/SUBMISSION%20Finetuned%20PubMedBERT%20PDS%20A2%20Food%20Hazard%20Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Install necessary packages and import libraries\n","This section includes all the necessary imports for data manipulation, model training, and evaluation.\n","It also imports libraries for handling tokenization, model configuration, and metrics.\n"],"metadata":{"id":"Up2bFjloyvaZ"}},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","import torch\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, classification_report\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","import os\n","from shutil import make_archive\n","import numpy as np\n"],"metadata":{"id":"ySpgDtU5ywQM","executionInfo":{"status":"ok","timestamp":1732392632324,"user_tz":-120,"elapsed":3407,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Mount Google Drive\n"],"metadata":{"id":"MIYvD2xxyzL8"}},{"cell_type":"code","source":["drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YiM_6Z68yxJ6","outputId":"3e3d8d9b-98da-4b7f-a7b3-1d83296e1c8c","executionInfo":{"status":"ok","timestamp":1732392635283,"user_tz":-120,"elapsed":2961,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Load and preview the training dataset\n","The dataset containing incident reports is loaded from Google Drive.\n","We remove any unnecessary columns like 'Unnamed: 0'.\n"],"metadata":{"id":"g2q92Do-y2W9"}},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","\n","\n","# Define the path to the file on Google Drive\n","train_path = '/content/drive/MyDrive/Data/augmented_incidents_train.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(train_path)\n","\n","# Keep only the specified columns\n","columns_to_keep = ['year', 'month', 'day', 'country', 'title', 'text', 'hazard-category', 'product-category', 'hazard', 'product']\n","df = df[columns_to_keep]\n","df"],"metadata":{"id":"ncSsiInty1G9","colab":{"base_uri":"https://localhost:8080/","height":788},"outputId":"4205c7db-8c72-475c-c4e1-c504d4a5a739","executionInfo":{"status":"ok","timestamp":1732392635835,"user_tz":-120,"elapsed":555,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       year  month  day country  \\\n","0      2008     11    7      au   \n","1      2011      7   11      au   \n","2      2012      2   21      au   \n","3      2012     12    4      us   \n","4      2014      4   10      au   \n","...     ...    ...  ...     ...   \n","11834  2016      9   29      au   \n","11835  2016      8   18      au   \n","11836  2018     11   30      us   \n","11837  2005      9   27      au   \n","11838  2007     11    1      au   \n","\n","                                                   title  \\\n","0           Country Cuisine Pty Ltd—Malouf’s Spice Mezza   \n","1      Haigh's Manufacturing Pty Ltd—Haigh’s Aprichoc...   \n","2      Coles Supermarkets Limited—Coles Deli 200g Spi...   \n","3      2012 - Price Chopper Supermarkets Recalls Cent...   \n","4                                      Coles Easter Eggs   \n","...                                                  ...   \n","11834  Quality Bakers Australia Pty Limited — Various...   \n","11835  Gluten Free Bakehouse Pty Ltd — Various Zehnde...   \n","11836  Tres Hermanos Bakery Issues Allergy Alert on U...   \n","11837  Gold Coast Bakery Queensland Pty Ltd—Vogels—Fr...   \n","11838  Woolworths Limited—Home brand white sliced san...   \n","\n","                                                    text hazard-category  \\\n","0      PRA No. 2008/10424 Date Published Nov 7, 2008 ...       allergens   \n","1      PRA No. 2011/12730 Publication Date Jul 11, 20...       allergens   \n","2      PRA No. 2012/13032 Publication Date Feb 21, 20...       allergens   \n","3      FOR IMMEDIATE RELEASE - October 21, 2012 - (Sc...       allergens   \n","4      Coles Supermarkets Australia Pty Ltd recalled ...       allergens   \n","...                                                  ...             ...   \n","11834  PRA No. 2016/15657 Date published 29 Sep 2016 ...  foreign bodies   \n","11835  PRA No. 2016/15603 Date published 18 Aug 2016 ...       allergens   \n","11836  Wyoming, MI - Tres Hermanos Bakery of Wyoming,...       allergens   \n","11837  PRA No. 2005/8073 Date published 27 Sep 2005 P...  foreign bodies   \n","11838  PRA No. 2007/9622 Date published 1 Nov 2007 Pr...  foreign bodies   \n","\n","                                   product-category  \\\n","0                                  herbs and spices   \n","1      cocoa and cocoa preparations, coffee and tea   \n","2              soups, broths, sauces and condiments   \n","3                       cereals and bakery products   \n","4      cocoa and cocoa preparations, coffee and tea   \n","...                                             ...   \n","11834                   cereals and bakery products   \n","11835                   cereals and bakery products   \n","11836                   cereals and bakery products   \n","11837                   cereals and bakery products   \n","11838                   cereals and bakery products   \n","\n","                              hazard         product  \n","0                               nuts       spice mix  \n","1                               nuts       chocolate  \n","2                               nuts           sauce  \n","3                               nuts           cakes  \n","4                               nuts  chocolate eggs  \n","...                              ...             ...  \n","11834                 metal fragment           bread  \n","11835  soybeans and products thereof           bread  \n","11836      milk and products thereof           bread  \n","11837               plastic fragment           bread  \n","11838                 glass fragment           bread  \n","\n","[11839 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-8140970f-9333-48e7-8125-20da143e56c9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>country</th>\n","      <th>title</th>\n","      <th>text</th>\n","      <th>hazard-category</th>\n","      <th>product-category</th>\n","      <th>hazard</th>\n","      <th>product</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2008</td>\n","      <td>11</td>\n","      <td>7</td>\n","      <td>au</td>\n","      <td>Country Cuisine Pty Ltd—Malouf’s Spice Mezza</td>\n","      <td>PRA No. 2008/10424 Date Published Nov 7, 2008 ...</td>\n","      <td>allergens</td>\n","      <td>herbs and spices</td>\n","      <td>nuts</td>\n","      <td>spice mix</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2011</td>\n","      <td>7</td>\n","      <td>11</td>\n","      <td>au</td>\n","      <td>Haigh's Manufacturing Pty Ltd—Haigh’s Aprichoc...</td>\n","      <td>PRA No. 2011/12730 Publication Date Jul 11, 20...</td>\n","      <td>allergens</td>\n","      <td>cocoa and cocoa preparations, coffee and tea</td>\n","      <td>nuts</td>\n","      <td>chocolate</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2012</td>\n","      <td>2</td>\n","      <td>21</td>\n","      <td>au</td>\n","      <td>Coles Supermarkets Limited—Coles Deli 200g Spi...</td>\n","      <td>PRA No. 2012/13032 Publication Date Feb 21, 20...</td>\n","      <td>allergens</td>\n","      <td>soups, broths, sauces and condiments</td>\n","      <td>nuts</td>\n","      <td>sauce</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2012</td>\n","      <td>12</td>\n","      <td>4</td>\n","      <td>us</td>\n","      <td>2012 - Price Chopper Supermarkets Recalls Cent...</td>\n","      <td>FOR IMMEDIATE RELEASE - October 21, 2012 - (Sc...</td>\n","      <td>allergens</td>\n","      <td>cereals and bakery products</td>\n","      <td>nuts</td>\n","      <td>cakes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2014</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>au</td>\n","      <td>Coles Easter Eggs</td>\n","      <td>Coles Supermarkets Australia Pty Ltd recalled ...</td>\n","      <td>allergens</td>\n","      <td>cocoa and cocoa preparations, coffee and tea</td>\n","      <td>nuts</td>\n","      <td>chocolate eggs</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11834</th>\n","      <td>2016</td>\n","      <td>9</td>\n","      <td>29</td>\n","      <td>au</td>\n","      <td>Quality Bakers Australia Pty Limited — Various...</td>\n","      <td>PRA No. 2016/15657 Date published 29 Sep 2016 ...</td>\n","      <td>foreign bodies</td>\n","      <td>cereals and bakery products</td>\n","      <td>metal fragment</td>\n","      <td>bread</td>\n","    </tr>\n","    <tr>\n","      <th>11835</th>\n","      <td>2016</td>\n","      <td>8</td>\n","      <td>18</td>\n","      <td>au</td>\n","      <td>Gluten Free Bakehouse Pty Ltd — Various Zehnde...</td>\n","      <td>PRA No. 2016/15603 Date published 18 Aug 2016 ...</td>\n","      <td>allergens</td>\n","      <td>cereals and bakery products</td>\n","      <td>soybeans and products thereof</td>\n","      <td>bread</td>\n","    </tr>\n","    <tr>\n","      <th>11836</th>\n","      <td>2018</td>\n","      <td>11</td>\n","      <td>30</td>\n","      <td>us</td>\n","      <td>Tres Hermanos Bakery Issues Allergy Alert on U...</td>\n","      <td>Wyoming, MI - Tres Hermanos Bakery of Wyoming,...</td>\n","      <td>allergens</td>\n","      <td>cereals and bakery products</td>\n","      <td>milk and products thereof</td>\n","      <td>bread</td>\n","    </tr>\n","    <tr>\n","      <th>11837</th>\n","      <td>2005</td>\n","      <td>9</td>\n","      <td>27</td>\n","      <td>au</td>\n","      <td>Gold Coast Bakery Queensland Pty Ltd—Vogels—Fr...</td>\n","      <td>PRA No. 2005/8073 Date published 27 Sep 2005 P...</td>\n","      <td>foreign bodies</td>\n","      <td>cereals and bakery products</td>\n","      <td>plastic fragment</td>\n","      <td>bread</td>\n","    </tr>\n","    <tr>\n","      <th>11838</th>\n","      <td>2007</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>au</td>\n","      <td>Woolworths Limited—Home brand white sliced san...</td>\n","      <td>PRA No. 2007/9622 Date published 1 Nov 2007 Pr...</td>\n","      <td>foreign bodies</td>\n","      <td>cereals and bakery products</td>\n","      <td>glass fragment</td>\n","      <td>bread</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11839 rows × 10 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8140970f-9333-48e7-8125-20da143e56c9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8140970f-9333-48e7-8125-20da143e56c9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8140970f-9333-48e7-8125-20da143e56c9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e6b976a7-bcc3-4c87-8f09-5a316cedea85\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6b976a7-bcc3-4c87-8f09-5a316cedea85')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e6b976a7-bcc3-4c87-8f09-5a316cedea85 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_2ea13f69-a400-417c-9ee0-ba314760a261\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_2ea13f69-a400-417c-9ee0-ba314760a261 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 11839,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1994,\n        \"max\": 2022,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          1996,\n          2010,\n          2013\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          6,\n          5,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          18,\n          3,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"scot\",\n          \"us\",\n          \"ie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3732,\n        \"samples\": [\n          \"Giant/Martin\\u2019s Alerts Customers to Voluntary Recall of Store Brand Frozen Whole Kernel Sweet Corn\",\n          \"Flowers Foods Issues Voluntary Recall on Certain Tastykake Multi-Pack Cupcakes Sold in Eight States Due to Possible Presence of Tiny Fragments of Metal Mesh Wire\",\n          \"2015 - Gilster - Mary Lee Corp. Issues an Allergen Alert for Undeclared Almonds in Market Pantry Honey & Oat Mixers Ready to Eat Cereal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3900,\n        \"samples\": [\n          \"The presence of a small battery makes this product dangerous to eat and poses a health risk. Product: Coop Hollow Milk Chocolate Bunny Package size: 120g Lot codes: All date codes No other Co-op products are known to be affected. The product below may contain a small battery that can be a choking risk and can cause burns in the event of ingestion. If you purchased the product do not eat it. Instead, return it to a Co-op store for a full refund, or contact their customer relationship team at 0800 0686 727. Co-op recalls the product above. Point-of-sale notices will be displayed in all retail stores that sell the product. These reviews explain to customers why the product is recalled and tell them what to do if they bought the product.\",\n          \"Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.Product information is available on the website.\",\n          \"The company Double Cola recalls about 200 cases of its product Cherry SKI because they are mislabeled. Only Cherry SKI products with the new design are recalled, which have been delivered south of Illinois, Evansville, IN and Winchester, OH markets. No other products from the company Double Cola are recalled. New Cherry SKI cans are recalled because the nutritional facts are incorrect. The cans show \\\"Red #4\\\" under ingredients, which is incorrect. The ingredients must include \\\"Red #40\\\" and \\\"Yellow #5.\\\" These dyes can cause a reaction to any allergic person. Double Cola company has reported that no medical illness or harmful effects have resulted from this product so far. The affected products are coded A8023EV10:25 to A8023EV11:40. \\\"We have taken immediate action once we have known the situation and recalled the product\\\" Gina Dhanani, vice-president of SKI.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hazard-category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"organoleptic aspects\",\n          \"foreign bodies\",\n          \"packaging defect\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product-category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"herbs and spices\",\n          \"seafood\",\n          \"ices and desserts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hazard\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 128,\n        \"samples\": [\n          \"chemical\",\n          \"coliforms\",\n          \"hazelnut\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 523,\n        \"samples\": [\n          \"horseradish in jars\",\n          \"liquorice\",\n          \"bakery products\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Define a function to clean text data\n","This function removes special characters, converts text to lowercase, and strips extra whitespace.\n","It is essential to clean the text data for better model performance.\n"],"metadata":{"id":"KDS28Uk-zNPU"}},{"cell_type":"code","source":["import re\n","\n","def clean_text(text):\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n","    text = text.lower()  # Convert text to lowercase\n","    text = ' '.join(text.split())  # Remove extra spaces\n","    return text\n"],"metadata":{"id":"DxuoSFEUy3rX","executionInfo":{"status":"ok","timestamp":1732392635835,"user_tz":-120,"elapsed":4,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Clean the text data and load the tokenizer\n","We apply the `clean_text` function to clean the 'text' column of the dataset.\n","Then, we initialize the PubMedBERT tokenizer to prepare for tokenization.\n"],"metadata":{"id":"jBAtfWSgzROT"}},{"cell_type":"code","source":["# Load the tokenizer for the PubMedBERT model, specifically fine-tuned for biomedical text\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n","\n","# Apply the text cleaning function to the 'text' column in the DataFrame\n","# This function will preprocess each text entry by removing unwanted characters, stopwords, etc.\n","df['text'] = df['text'].apply(clean_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0Gy9-3ezQVb","outputId":"f2b7462b-19bd-46c3-ec3a-23a3725fa55c","executionInfo":{"status":"ok","timestamp":1732392637733,"user_tz":-120,"elapsed":1901,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# Define features and targets for classification tasks\n","We specify the input features like date and country and set the classification targets.\n","\n"],"metadata":{"id":"p6hPAUKozWMw"}},{"cell_type":"code","source":["# Define the features for the model, which include the year, month, day, and country information\n","features = ['year', 'month', 'day', 'country']\n","\n","# Define the target variables for Subtask 1, which are the hazard-category and product-category\n","targets_subtask1 = ['hazard-category', 'product-category']\n","\n","# Define the target variables for Subtask 2, which are hazard and product\n","# Add other targets if necessary depending on the task\n","targets_subtask2 = ['hazard', 'product']\n"],"metadata":{"id":"kvkxIfuvzU3Z","executionInfo":{"status":"ok","timestamp":1732392637733,"user_tz":-120,"elapsed":6,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Encode target labels\n","For classification, target labels need to be encoded as numeric values.\n","We use `LabelEncoder` to convert categorical labels into integers.\n"],"metadata":{"id":"-hn5LTh_zjI2"}},{"cell_type":"code","source":["# Create an empty dictionary to store label encoders for each target\n","label_encoders = {}\n","\n","# Iterate over both sets of targets (Subtask 1 and Subtask 2)\n","for target in targets_subtask1 + targets_subtask2:\n","    # Initialize a LabelEncoder for each target\n","    le = LabelEncoder()\n","\n","    # Transform the target column values into numeric labels and update the DataFrame\n","    df[target] = le.fit_transform(df[target])\n","\n","    # Store the fitted LabelEncoder in the dictionary for future use (e.g., inverse transformation)\n","    label_encoders[target] = le\n"],"metadata":{"id":"8Wd6N94wziDV","executionInfo":{"status":"ok","timestamp":1732392637733,"user_tz":-120,"elapsed":4,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Define a custom PyTorch dataset for text classification\n","This dataset class will handle text tokenization and label processing.\n","It ensures the text is properly encoded, padded, and truncated to a fixed length for the model.\n"],"metadata":{"id":"kdnDHxmlzpCm"}},{"cell_type":"code","source":["# Define a custom Dataset class for text data\n","class TextDataset(Dataset):\n","    # Initialize the dataset with texts, labels, tokenizer, and maximum sequence length\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts  # List of input texts\n","        self.labels = labels  # List of corresponding labels\n","        self.tokenizer = tokenizer  # Tokenizer for encoding the text\n","        self.max_len = max_len  # Maximum length for padding/truncation\n","\n","    # Define the length of the dataset (number of samples)\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    # Define how to retrieve a single item from the dataset\n","    def __getitem__(self, item):\n","        text = str(self.texts[item])  # Get the text for the given index\n","        label = self.labels[item]  # Get the label for the given index\n","\n","        # Use the tokenizer to encode the text (add special tokens, padding, truncation)\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,  # Add special tokens (e.g., [CLS], [SEP])\n","            max_length=self.max_len,  # Limit the sequence length\n","            padding='max_length',  # Pad sequences to max_length\n","            truncation=True,  # Truncate longer sequences\n","            return_tensors='pt'  # Return PyTorch tensors\n","        )\n","\n","        # Return a dictionary with input_ids, attention_mask, and label\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),  # Flatten the tensor\n","            'attention_mask': encoding['attention_mask'].flatten(),  # Flatten the attention mask\n","            'label': torch.tensor(label, dtype=torch.long)  # Convert label to a tensor\n","        }\n"],"metadata":{"id":"wkzzsS2Gzk9L","executionInfo":{"status":"ok","timestamp":1732392637733,"user_tz":-120,"elapsed":4,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Split the data into training and testing sets\n","We split the dataset into training and testing sets for each target.\n","This ensures that the model is trained on one set and evaluated on a separate, unseen set.\n"],"metadata":{"id":"SbkXM6pdzsmb"}},{"cell_type":"code","source":["# Define a function to prepare data for model training and testing\n","def prepare_data(text_column):\n","    # Extract features and text column from the DataFrame\n","    X = df[features + [text_column]]  # Features include specified columns plus the text column\n","    # Extract target variables for Subtask 1 and Subtask 2\n","    y_subtask1 = df[targets_subtask1]\n","    y_subtask2 = df[targets_subtask2]\n","\n","    # Initialize a dictionary to store data splits for each target\n","    data_splits = {}\n","\n","    # Iterate over both sets of target variables (Subtask 1 and Subtask 2)\n","    for target in targets_subtask1 + targets_subtask2:\n","        # Split the data into training and testing sets (80% train, 20% test)\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            X, df[target], test_size=0.2, random_state=42\n","        )\n","\n","        # Reset the indices for the train and test sets (important for maintaining order after split)\n","        X_train = X_train.reset_index(drop=True)\n","        y_train = y_train.reset_index(drop=True)\n","        X_test = X_test.reset_index(drop=True)\n","        y_test = y_test.reset_index(drop=True)\n","\n","        # Store the splits for the current target in the dictionary\n","        data_splits[target] = (X_train, X_test, y_train, y_test)\n","\n","    # Return the dictionary containing data splits for each target\n","    return data_splits\n"],"metadata":{"id":"dSuyInwzzrYc","executionInfo":{"status":"ok","timestamp":1732392637733,"user_tz":-120,"elapsed":3,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Prepare the data splits for text-based tasks\n","We apply the `prepare_data` function specifically for text tasks and save the splits for later use.\n"],"metadata":{"id":"gFhciBb6zvnJ"}},{"cell_type":"code","source":["# Prepare the data splits for the 'text' column using the prepare_data function\n","text_splits = prepare_data('text')\n"],"metadata":{"id":"Y7o-Gi7vzuZR","executionInfo":{"status":"ok","timestamp":1732392637733,"user_tz":-120,"elapsed":3,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Set model configuration and define the device\n","Here, we configure key parameters for training like maximum sequence length, batch size, and learning rate.\n","We also determine whether to use GPU or CPU for training based on availability.\n"],"metadata":{"id":"hEJgXRNIzybH"}},{"cell_type":"code","source":["# Define configuration settings for the model training\n","config = {\n","    'max_len': 512,  # Maximum sequence length for input texts\n","    'batch_size': 16,  # Batch size for training\n","    'learning_rate': 2e-5,  # Learning rate for the optimizer\n","    'epochs': 5,  # Number of training epochs\n","    'model_name': \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"  # Pre-trained model to use\n","}\n","\n","# Determine the device to use for training (GPU if available, otherwise CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TetF1XIzxU8","outputId":"14686afa-1fb1-4628-f12f-22d97ee3c36d","executionInfo":{"status":"ok","timestamp":1732392637733,"user_tz":-120,"elapsed":3,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["# Train and evaluate the model for each task\n","This function performs model training and evaluation for each target.\n","It uses a neural network to predict labels and calculates the F1 score for evaluation.\n"],"metadata":{"id":"u22f8LM6z26I"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torch import nn, optim\n","from transformers import AutoModelForSequenceClassification, get_scheduler\n","from sklearn.metrics import f1_score, classification_report\n","from tqdm import tqdm\n","\n","# Training and evaluation function\n","def train_and_evaluate_bert(data_splits, targets):\n","    f1_scores = []  # Store F1 scores for each task\n","\n","    for target in targets:\n","        print(f\"\\nStarting training for task: {target}\")\n","\n","        # Retrieve data splits\n","        X_train, X_test, y_train, y_test = data_splits[target]\n","\n","        # Extract the 'text' column for training and testing\n","        texts_train = X_train['text'].values\n","        texts_test = X_test['text'].values\n","\n","        # Create datasets\n","        train_dataset = TextDataset(texts_train, y_train, tokenizer, config['max_len'])\n","        test_dataset = TextDataset(texts_test, y_test, tokenizer, config['max_len'])\n","\n","        # Create DataLoaders\n","        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n","\n","        # Number of labels for classification\n","        num_labels = len(label_encoders[target].classes_)\n","\n","        # Load pre-trained model\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            config['model_name'], num_labels=num_labels\n","        ).to(device)\n","\n","        # Optimizer and scheduler\n","        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n","        scheduler = get_scheduler(\n","            \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * config['epochs']\n","        )\n","\n","        # Class-weighted loss\n","        class_counts = np.bincount(y_train)\n","        class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float).to(device)\n","        criterion = nn.CrossEntropyLoss(weight=class_weights)\n","\n","        # Training loop\n","        for epoch in range(config['epochs']):\n","            print(f\"Epoch {epoch+1}/{config['epochs']} - Training: {target}\")\n","            model.train()\n","            train_loss = 0\n","            progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=True)\n","\n","            for batch in progress_bar:\n","                optimizer.zero_grad()\n","\n","                input_ids = batch['input_ids'].squeeze(1).to(device)\n","                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n","                labels = batch['label'].to(device)\n","\n","                outputs = model(input_ids, attention_mask=attention_mask)\n","                loss = criterion(outputs.logits, labels)\n","                loss.backward()\n","\n","                # Gradient clipping to prevent exploding gradients\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                optimizer.step()\n","                scheduler.step()\n","\n","                train_loss += loss.item()\n","                progress_bar.set_postfix(loss=loss.item())\n","\n","            print(f\"Training Loss: {train_loss / len(train_loader):.4f}\")\n","\n","        # Final evaluation on the test set\n","        print(\"Final evaluation on test set...\")\n","        model.eval()\n","        test_preds, test_labels = [], []\n","\n","        with torch.no_grad():\n","            for batch in tqdm(test_loader, desc=\"Testing\", leave=True):\n","                input_ids = batch['input_ids'].squeeze(1).to(device)\n","                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n","                labels = batch['label'].to(device)\n","\n","                outputs = model(input_ids, attention_mask=attention_mask)\n","                _, preds = torch.max(outputs.logits, dim=1)\n","                test_preds.extend(preds.cpu().numpy())\n","                test_labels.extend(labels.cpu().numpy())\n","\n","        # Calculate F1 score\n","        test_f1 = f1_score(test_labels, test_preds, average='macro')\n","        f1_scores.append(test_f1)\n","        print(f\"Final Test MACRO F1 for {target}: {test_f1:.4f}\")\n","\n","        # Classification report\n","        print(f\"Classification Report for {target}:\\n\")\n","        print(classification_report(test_labels, test_preds, zero_division=0))\n","\n","        # Save the model and tokenizer\n","        os.makedirs(f'./model_{target}', exist_ok=True)\n","        model.save_pretrained(f'./model_{target}')\n","        tokenizer.save_pretrained(f'./model_{target}')\n","\n","        # Save the label encoder\n","        np.save(f'./model_{target}/{target}_label_encoder.npy', label_encoders[target].classes_)\n","        print(f\"Model and LabelEncoder for {target} saved in './model_{target}'\")\n","\n","    return f1_scores\n","\n","\n","\n","text_f1_scores = train_and_evaluate_bert(text_splits, targets_subtask1 + targets_subtask2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imN-jiGRz1xV","outputId":"40c94eb1-e309-41cd-8939-6bee12d08b92","executionInfo":{"status":"ok","timestamp":1732396536678,"user_tz":-120,"elapsed":3898483,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting training for task: hazard-category\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5 - Training: hazard-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1: 100%|██████████| 592/592 [03:11<00:00,  3.09it/s, loss=0.163]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.0411\n","Epoch 2/5 - Training: hazard-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2: 100%|██████████| 592/592 [03:10<00:00,  3.10it/s, loss=0.0174]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.2128\n","Epoch 3/5 - Training: hazard-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3: 100%|██████████| 592/592 [03:10<00:00,  3.10it/s, loss=0.00358]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0856\n","Epoch 4/5 - Training: hazard-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4: 100%|██████████| 592/592 [03:10<00:00,  3.10it/s, loss=0.00431]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0595\n","Epoch 5/5 - Training: hazard-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=0.00152]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0363\n","Final evaluation on test set...\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 148/148 [00:16<00:00,  8.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Final Test MACRO F1 for hazard-category: 0.9568\n","Classification Report for hazard-category:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.98      0.97       710\n","           1       0.99      0.98      0.98       703\n","           2       0.99      0.99      0.99       269\n","           3       1.00      0.89      0.94        18\n","           4       0.97      0.96      0.96       258\n","           5       0.95      0.94      0.94       248\n","           7       0.93      0.95      0.94        39\n","           8       0.92      0.93      0.93        75\n","           9       0.96      0.96      0.96        48\n","\n","    accuracy                           0.97      2368\n","   macro avg       0.96      0.95      0.96      2368\n","weighted avg       0.97      0.97      0.97      2368\n","\n","Model and LabelEncoder for hazard-category saved in './model_hazard-category'\n","\n","Starting training for task: product-category\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5 - Training: product-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1: 100%|██████████| 592/592 [03:11<00:00,  3.09it/s, loss=1.43]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 2.1730\n","Epoch 2/5 - Training: product-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=0.685]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.6330\n","Epoch 3/5 - Training: product-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=0.357]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.2434\n","Epoch 4/5 - Training: product-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=0.0396]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.1207\n","Epoch 5/5 - Training: product-category\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5: 100%|██████████| 592/592 [03:10<00:00,  3.10it/s, loss=0.0414]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0764\n","Final evaluation on test set...\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 148/148 [00:16<00:00,  8.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Final Test MACRO F1 for product-category: 0.9466\n","Classification Report for product-category:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99        51\n","           1       0.89      0.89      0.89       254\n","           2       0.97      0.81      0.88        74\n","           3       0.89      0.97      0.93       103\n","           4       1.00      0.87      0.93        69\n","           5       1.00      0.97      0.98        29\n","           6       1.00      1.00      1.00         6\n","           7       1.00      1.00      1.00         8\n","           8       1.00      1.00      1.00         4\n","           9       0.93      0.96      0.94       281\n","          10       0.99      0.94      0.96        78\n","          11       1.00      1.00      1.00        10\n","          12       0.90      0.83      0.86        42\n","          13       0.95      0.95      0.95       610\n","          14       0.93      0.97      0.95       116\n","          15       0.95      0.94      0.94       108\n","          16       0.98      1.00      0.99        42\n","          17       1.00      0.77      0.87        13\n","          18       0.83      0.88      0.85       178\n","          19       0.98      0.97      0.98       143\n","          20       0.91      0.92      0.92       142\n","          21       1.00      1.00      1.00         7\n","\n","    accuracy                           0.93      2368\n","   macro avg       0.96      0.94      0.95      2368\n","weighted avg       0.93      0.93      0.93      2368\n","\n","Model and LabelEncoder for product-category saved in './model_product-category'\n","\n","Starting training for task: hazard\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5 - Training: hazard\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=3.97]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 4.3100\n","Epoch 2/5 - Training: hazard\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=2.48]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 2.9756\n","Epoch 3/5 - Training: hazard\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3: 100%|██████████| 592/592 [03:10<00:00,  3.10it/s, loss=1.28]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.9392\n","Epoch 4/5 - Training: hazard\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=0.756]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.3166\n","Epoch 5/5 - Training: hazard\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=1.54]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.0287\n","Final evaluation on test set...\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 148/148 [00:16<00:00,  8.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Final Test MACRO F1 for hazard: 0.8681\n","Classification Report for hazard:\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       0.75      1.00      0.86         9\n","           2       1.00      1.00      1.00        11\n","           3       0.71      1.00      0.83         5\n","           4       0.71      0.56      0.62         9\n","           5       0.86      1.00      0.92        12\n","           6       1.00      1.00      1.00         5\n","           7       0.88      1.00      0.93         7\n","           8       1.00      1.00      1.00         8\n","           9       0.56      1.00      0.72         9\n","          10       0.75      1.00      0.86         3\n","          11       0.89      0.73      0.80        11\n","          12       1.00      1.00      1.00        10\n","          13       0.90      0.75      0.82        12\n","          14       1.00      0.69      0.81        16\n","          15       1.00      1.00      1.00        10\n","          16       1.00      1.00      1.00         8\n","          17       0.90      0.76      0.82        58\n","          18       0.44      1.00      0.62         4\n","          19       0.86      1.00      0.92         6\n","          20       0.60      1.00      0.75         3\n","          21       1.00      1.00      1.00        11\n","          22       0.73      0.94      0.82        17\n","          23       0.75      1.00      0.86         3\n","          24       1.00      1.00      1.00         6\n","          25       0.67      1.00      0.80         2\n","          26       1.00      1.00      1.00         4\n","          27       1.00      1.00      1.00         7\n","          28       0.92      0.92      0.92        13\n","          29       1.00      1.00      1.00        16\n","          30       1.00      1.00      1.00         9\n","          31       0.93      1.00      0.96        13\n","          32       1.00      1.00      1.00         5\n","          33       1.00      0.92      0.96        12\n","          34       0.94      0.81      0.87        80\n","          35       1.00      1.00      1.00         7\n","          36       0.95      0.83      0.89        84\n","          37       1.00      1.00      1.00         9\n","          38       0.87      0.87      0.87        15\n","          39       0.81      0.59      0.68        22\n","          40       0.93      0.88      0.90        32\n","          41       1.00      1.00      1.00         8\n","          42       0.75      0.86      0.80         7\n","          43       1.00      1.00      1.00        16\n","          44       1.00      1.00      1.00        10\n","          45       1.00      0.94      0.97        16\n","          46       1.00      1.00      1.00         6\n","          47       1.00      0.75      0.86         8\n","          48       1.00      1.00      1.00        11\n","          49       1.00      0.80      0.89        10\n","          50       0.89      1.00      0.94         8\n","          51       0.62      1.00      0.76        13\n","          52       0.48      0.89      0.62        38\n","          53       0.58      0.50      0.54        14\n","          54       0.79      0.68      0.73        22\n","          55       0.99      0.80      0.89       220\n","          56       0.67      1.00      0.80         6\n","          57       0.89      0.87      0.88        55\n","          58       1.00      0.76      0.87        17\n","          59       0.97      0.85      0.91       203\n","          60       0.86      0.60      0.71        10\n","          61       0.43      0.77      0.56        13\n","          62       0.69      1.00      0.82         9\n","          63       1.00      1.00      1.00        10\n","          64       1.00      0.95      0.98        21\n","          65       0.94      1.00      0.97        16\n","          66       1.00      1.00      1.00         6\n","          67       0.67      1.00      0.80         8\n","          68       1.00      1.00      1.00        12\n","          69       0.83      1.00      0.91         5\n","          70       1.00      0.14      0.25        14\n","          71       1.00      1.00      1.00         9\n","          72       1.00      1.00      1.00         6\n","          73       1.00      0.30      0.46        43\n","          74       0.92      1.00      0.96        11\n","          75       0.82      0.90      0.86        10\n","          76       1.00      1.00      1.00         9\n","          77       0.88      1.00      0.93         7\n","          78       0.50      0.29      0.36         7\n","          79       1.00      1.00      1.00         7\n","          80       0.82      0.75      0.78        12\n","          81       1.00      1.00      1.00         3\n","          82       0.58      1.00      0.74         7\n","          83       0.86      0.86      0.86         7\n","          84       1.00      1.00      1.00        13\n","          85       0.86      0.90      0.88        63\n","          86       1.00      0.78      0.88         9\n","          87       0.55      1.00      0.71         6\n","          88       1.00      0.71      0.83         7\n","          89       0.90      1.00      0.95         9\n","          90       0.84      0.88      0.86        83\n","          91       0.86      1.00      0.92        12\n","          92       0.58      0.88      0.70         8\n","          93       1.00      1.00      1.00         6\n","          94       0.92      0.80      0.86        15\n","          95       0.86      1.00      0.92         6\n","          96       1.00      1.00      1.00        15\n","          97       0.74      0.88      0.80        16\n","          98       0.97      0.87      0.91       202\n","          99       0.76      1.00      0.86        19\n","         100       0.82      0.75      0.78        65\n","         101       0.67      1.00      0.80         4\n","         102       1.00      0.67      0.80        15\n","         103       0.19      1.00      0.32         4\n","         104       0.88      1.00      0.93         7\n","         105       0.73      1.00      0.85        11\n","         106       0.88      1.00      0.93         7\n","         107       0.64      1.00      0.78         7\n","         108       0.62      1.00      0.77         5\n","         109       0.98      1.00      0.99        45\n","         110       0.83      1.00      0.91         5\n","         111       0.75      1.00      0.86         6\n","         112       1.00      1.00      1.00         3\n","         113       1.00      1.00      1.00        12\n","         114       0.71      1.00      0.83        12\n","         115       0.58      1.00      0.74         7\n","         116       0.85      1.00      0.92        17\n","         117       0.80      1.00      0.89         4\n","         118       1.00      1.00      1.00         5\n","         119       1.00      0.96      0.98        27\n","         120       0.67      1.00      0.80         6\n","         121       0.93      0.88      0.90        16\n","         122       0.28      1.00      0.43         8\n","         123       1.00      1.00      1.00         3\n","         124       1.00      1.00      1.00         6\n","         125       0.80      1.00      0.89         4\n","         126       0.62      1.00      0.77        15\n","         127       0.91      1.00      0.95        10\n","\n","    accuracy                           0.86      2368\n","   macro avg       0.86      0.92      0.87      2368\n","weighted avg       0.90      0.86      0.87      2368\n","\n","Model and LabelEncoder for hazard saved in './model_hazard'\n","\n","Starting training for task: product\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","<ipython-input-12-e6cd23c59eba>:48: RuntimeWarning: divide by zero encountered in divide\n","  class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5 - Training: product\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1: 100%|██████████| 592/592 [03:11<00:00,  3.09it/s, loss=5.73]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 6.1077\n","Epoch 2/5 - Training: product\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=4.83]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 5.4392\n","Epoch 3/5 - Training: product\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=5.32]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 4.8731\n","Epoch 4/5 - Training: product\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=4.67]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 4.4698\n","Epoch 5/5 - Training: product\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5: 100%|██████████| 592/592 [03:11<00:00,  3.10it/s, loss=4.29]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 4.2427\n","Final evaluation on test set...\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 148/148 [00:16<00:00,  8.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Final Test MACRO F1 for product: 0.5706\n","Classification Report for product:\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         3\n","           2       1.00      0.33      0.50         6\n","           4       1.00      0.50      0.67         6\n","           5       0.00      0.00      0.00         0\n","           6       1.00      0.56      0.71         9\n","           7       0.00      0.00      0.00         4\n","           8       0.75      0.75      0.75        12\n","           9       0.50      0.17      0.25         6\n","          10       0.67      0.67      0.67         6\n","          11       0.92      1.00      0.96        11\n","          12       1.00      1.00      1.00         7\n","          13       0.80      1.00      0.89         4\n","          14       0.67      0.40      0.50         5\n","          15       0.00      0.00      0.00         1\n","          17       1.00      1.00      1.00         3\n","          18       1.00      0.67      0.80         6\n","          19       0.91      0.62      0.74        16\n","          20       1.00      1.00      1.00         1\n","          21       0.00      0.00      0.00         3\n","          22       1.00      0.40      0.57         5\n","          23       0.62      1.00      0.76         8\n","          24       1.00      1.00      1.00         2\n","          25       1.00      0.20      0.33        10\n","          26       0.67      1.00      0.80         8\n","          27       1.00      1.00      1.00         2\n","          29       0.00      0.00      0.00         1\n","          31       0.64      1.00      0.78         7\n","          32       0.00      0.00      0.00         2\n","          33       0.58      1.00      0.74         7\n","          34       0.25      0.25      0.25         4\n","          35       0.56      1.00      0.71         5\n","          36       0.57      1.00      0.73         4\n","          38       0.33      0.09      0.14        11\n","          39       0.88      1.00      0.93         7\n","          40       0.00      0.00      0.00         6\n","          41       0.50      1.00      0.67         1\n","          42       0.20      1.00      0.33         2\n","          43       0.33      0.10      0.15        10\n","          44       1.00      1.00      1.00         1\n","          45       0.00      0.00      0.00         3\n","          46       0.38      1.00      0.55         6\n","          47       0.00      0.00      0.00         1\n","          48       0.29      1.00      0.44         2\n","          49       1.00      0.14      0.25         7\n","          50       1.00      0.75      0.86        12\n","          52       0.00      0.00      0.00         1\n","          53       1.00      0.83      0.91         6\n","          54       0.50      1.00      0.67         1\n","          55       0.88      1.00      0.93         7\n","          56       0.00      0.00      0.00         3\n","          57       0.67      0.50      0.57         4\n","          58       0.00      0.00      0.00         5\n","          59       0.75      0.75      0.75         4\n","          60       1.00      0.20      0.33         5\n","          62       1.00      0.86      0.92         7\n","          63       0.50      0.33      0.40         3\n","          64       0.45      1.00      0.62         5\n","          65       1.00      1.00      1.00         2\n","          66       0.55      1.00      0.71         6\n","          67       0.57      1.00      0.73         4\n","          68       0.38      0.32      0.34        19\n","          69       0.00      0.00      0.00         3\n","          71       1.00      1.00      1.00         6\n","          72       0.00      0.00      0.00         1\n","          73       0.60      1.00      0.75         3\n","          74       1.00      1.00      1.00         6\n","          75       0.00      0.00      0.00         1\n","          77       0.33      0.50      0.40         2\n","          78       0.57      1.00      0.73         4\n","          79       0.64      1.00      0.78         7\n","          80       0.86      1.00      0.92         6\n","          81       0.33      1.00      0.50         3\n","          82       0.00      0.00      0.00         4\n","          83       0.00      0.00      0.00         0\n","          84       1.00      0.25      0.40         8\n","          85       1.00      0.25      0.40         8\n","          86       0.62      1.00      0.76         8\n","          87       0.00      0.00      0.00         4\n","          88       1.00      1.00      1.00         5\n","          89       1.00      1.00      1.00         2\n","          90       0.22      1.00      0.36         2\n","          91       0.00      0.00      0.00         9\n","          92       0.00      0.00      0.00         1\n","          93       0.38      0.26      0.31        23\n","          94       1.00      0.20      0.33        15\n","          95       0.44      1.00      0.62         8\n","          96       0.80      1.00      0.89         8\n","          97       0.10      0.40      0.15         5\n","          98       0.50      0.20      0.29        10\n","          99       1.00      1.00      1.00         3\n","         100       0.00      0.00      0.00         8\n","         101       0.75      0.60      0.67         5\n","         104       0.71      1.00      0.83         5\n","         107       0.25      0.20      0.22         5\n","         109       0.75      0.60      0.67         5\n","         110       0.58      1.00      0.74         7\n","         111       0.00      0.00      0.00         0\n","         112       0.00      0.00      0.00         4\n","         113       0.50      0.36      0.42        11\n","         114       0.86      0.67      0.75         9\n","         116       0.00      0.00      0.00         4\n","         117       0.00      0.00      0.00         1\n","         118       0.00      0.00      0.00         1\n","         119       0.17      1.00      0.29         1\n","         120       0.23      0.38      0.29         8\n","         121       0.00      0.00      0.00         1\n","         122       1.00      1.00      1.00         6\n","         123       0.53      1.00      0.69         9\n","         124       1.00      1.00      1.00         5\n","         126       0.33      0.20      0.25        10\n","         127       0.50      1.00      0.67         4\n","         129       1.00      0.25      0.40         8\n","         130       0.40      1.00      0.57         2\n","         131       0.50      0.33      0.40         3\n","         132       0.83      1.00      0.91         5\n","         133       0.78      1.00      0.88         7\n","         134       0.00      0.00      0.00         2\n","         135       1.00      1.00      1.00         6\n","         136       0.62      0.83      0.71         6\n","         138       0.00      0.00      0.00         1\n","         139       1.00      1.00      1.00         3\n","         141       0.75      0.50      0.60         6\n","         142       1.00      0.40      0.57         5\n","         143       1.00      1.00      1.00         7\n","         145       0.92      1.00      0.96        11\n","         146       1.00      0.25      0.40         8\n","         147       1.00      0.25      0.40         4\n","         148       0.00      0.00      0.00         7\n","         150       1.00      0.33      0.50         3\n","         151       1.00      0.50      0.67         4\n","         152       0.00      0.00      0.00         4\n","         153       0.00      0.00      0.00         1\n","         154       0.33      0.33      0.33         3\n","         155       1.00      0.67      0.80         9\n","         156       0.86      1.00      0.92         6\n","         158       1.00      0.80      0.89         5\n","         159       0.44      1.00      0.62         4\n","         160       1.00      0.38      0.55         8\n","         162       0.20      0.50      0.29         4\n","         163       1.00      1.00      1.00         2\n","         164       1.00      0.12      0.22         8\n","         166       0.40      0.22      0.29         9\n","         168       0.00      0.00      0.00         1\n","         169       1.00      0.81      0.90        16\n","         170       0.40      1.00      0.57         2\n","         171       0.00      0.00      0.00         6\n","         172       0.00      0.00      0.00         1\n","         173       1.00      1.00      1.00         7\n","         174       0.00      0.00      0.00         5\n","         176       0.00      0.00      0.00         1\n","         177       1.00      0.33      0.50        12\n","         178       0.00      0.00      0.00         4\n","         180       0.55      1.00      0.71         6\n","         181       1.00      0.91      0.95        11\n","         182       1.00      0.75      0.86         4\n","         183       1.00      1.00      1.00         4\n","         184       0.38      1.00      0.56         5\n","         187       1.00      1.00      1.00         1\n","         189       1.00      0.43      0.60         7\n","         190       0.00      0.00      0.00        12\n","         192       0.58      0.70      0.64        10\n","         193       0.83      1.00      0.91         5\n","         194       0.43      1.00      0.60         3\n","         195       0.73      1.00      0.84         8\n","         197       1.00      1.00      1.00         6\n","         198       0.50      1.00      0.67        10\n","         199       0.50      0.50      0.50         4\n","         200       0.60      1.00      0.75         6\n","         201       0.55      1.00      0.71         6\n","         202       0.75      1.00      0.86         3\n","         204       1.00      1.00      1.00         1\n","         206       0.88      1.00      0.93         7\n","         207       0.00      0.00      0.00         7\n","         208       1.00      0.50      0.67         6\n","         209       0.00      0.00      0.00         1\n","         210       0.50      0.67      0.57         3\n","         211       0.00      0.00      0.00         1\n","         212       0.00      0.00      0.00         6\n","         213       0.15      1.00      0.26         5\n","         215       0.71      1.00      0.83         5\n","         216       1.00      1.00      1.00         6\n","         217       1.00      1.00      1.00         4\n","         218       1.00      0.17      0.29         6\n","         219       1.00      0.50      0.67         8\n","         220       0.75      0.60      0.67        10\n","         221       1.00      1.00      1.00         3\n","         223       0.33      1.00      0.50         5\n","         224       1.00      1.00      1.00        11\n","         225       1.00      1.00      1.00         7\n","         226       0.00      0.00      0.00         1\n","         227       1.00      1.00      1.00         1\n","         228       0.50      0.17      0.25         6\n","         229       0.00      0.00      0.00         4\n","         230       0.09      1.00      0.16         5\n","         231       0.78      1.00      0.88         7\n","         232       0.75      1.00      0.86         3\n","         233       1.00      1.00      1.00         8\n","         236       0.40      1.00      0.57         2\n","         237       1.00      1.00      1.00         2\n","         238       1.00      0.75      0.86         8\n","         239       1.00      1.00      1.00         6\n","         240       1.00      1.00      1.00         5\n","         241       1.00      0.50      0.67         4\n","         242       0.88      1.00      0.93         7\n","         243       0.29      1.00      0.44         2\n","         244       0.36      1.00      0.53         5\n","         245       0.86      1.00      0.92         6\n","         246       0.00      0.00      0.00         0\n","         247       0.75      0.60      0.67         5\n","         248       0.67      0.50      0.57         4\n","         249       0.71      1.00      0.83         5\n","         250       1.00      1.00      1.00         1\n","         251       1.00      0.75      0.86         4\n","         253       0.40      1.00      0.57         4\n","         255       0.45      1.00      0.62         9\n","         256       0.00      0.00      0.00         1\n","         257       1.00      1.00      1.00         7\n","         258       0.24      0.56      0.33         9\n","         259       0.67      1.00      0.80        10\n","         260       1.00      0.75      0.86         4\n","         261       1.00      0.14      0.25         7\n","         262       0.41      1.00      0.58         7\n","         263       1.00      0.20      0.33         5\n","         264       1.00      1.00      1.00         2\n","         266       0.00      0.00      0.00         7\n","         267       1.00      0.83      0.91         6\n","         268       0.25      1.00      0.40         2\n","         270       1.00      1.00      1.00         7\n","         271       0.56      0.52      0.54        29\n","         273       0.91      0.91      0.91        11\n","         274       0.25      1.00      0.40         4\n","         275       0.67      1.00      0.80         4\n","         276       1.00      0.43      0.60         7\n","         277       1.00      0.73      0.85        15\n","         278       0.88      1.00      0.93         7\n","         279       1.00      0.75      0.86         4\n","         280       1.00      0.44      0.62         9\n","         281       0.50      0.60      0.55         5\n","         282       1.00      0.20      0.33         5\n","         283       1.00      1.00      1.00         1\n","         284       1.00      0.50      0.67         4\n","         285       0.50      1.00      0.67         4\n","         286       0.00      0.00      0.00         0\n","         287       1.00      1.00      1.00         2\n","         288       0.18      1.00      0.31         7\n","         289       0.50      1.00      0.67         2\n","         290       1.00      1.00      1.00         2\n","         291       0.00      0.00      0.00         4\n","         292       0.50      1.00      0.67         3\n","         293       0.50      1.00      0.67         6\n","         294       0.50      1.00      0.67         1\n","         295       0.00      0.00      0.00         1\n","         296       1.00      0.50      0.67         8\n","         297       1.00      1.00      1.00         3\n","         298       0.00      0.00      0.00         3\n","         299       0.00      0.00      0.00         1\n","         300       1.00      1.00      1.00         6\n","         301       1.00      1.00      1.00         6\n","         303       1.00      1.00      1.00         3\n","         304       1.00      1.00      1.00         5\n","         305       1.00      0.07      0.13        14\n","         307       1.00      0.57      0.73         7\n","         308       0.38      0.43      0.40         7\n","         309       1.00      0.50      0.67         2\n","         310       0.75      0.60      0.67         5\n","         311       1.00      1.00      1.00         4\n","         312       1.00      1.00      1.00         2\n","         313       0.73      1.00      0.84         8\n","         314       0.42      1.00      0.59         5\n","         316       1.00      0.25      0.40         8\n","         318       0.30      1.00      0.46         6\n","         320       1.00      1.00      1.00         5\n","         321       0.20      0.20      0.20         5\n","         322       0.67      0.67      0.67         3\n","         323       1.00      0.86      0.92         7\n","         324       1.00      1.00      1.00         1\n","         325       0.57      1.00      0.73         4\n","         326       0.67      1.00      0.80         4\n","         327       0.00      0.00      0.00         1\n","         328       1.00      0.60      0.75         5\n","         329       1.00      1.00      1.00         4\n","         330       0.80      1.00      0.89         4\n","         331       1.00      0.17      0.29         6\n","         335       1.00      0.60      0.75         5\n","         336       0.00      0.00      0.00         1\n","         337       0.20      1.00      0.33         1\n","         338       0.41      1.00      0.58         9\n","         339       1.00      0.60      0.75        10\n","         340       0.00      0.00      0.00         5\n","         342       0.67      1.00      0.80         2\n","         343       1.00      1.00      1.00         3\n","         346       1.00      0.44      0.62         9\n","         347       0.18      1.00      0.31         2\n","         348       1.00      1.00      1.00         1\n","         349       0.00      0.00      0.00         3\n","         350       1.00      0.14      0.25         7\n","         351       0.82      1.00      0.90         9\n","         352       0.50      1.00      0.67        10\n","         353       1.00      1.00      1.00         9\n","         354       0.00      0.00      0.00         7\n","         355       1.00      1.00      1.00         5\n","         357       1.00      0.25      0.40         4\n","         358       0.33      0.40      0.36         5\n","         359       1.00      0.50      0.67         4\n","         361       1.00      0.60      0.75         5\n","         362       1.00      0.14      0.25         7\n","         363       1.00      0.29      0.44         7\n","         364       0.00      0.00      0.00         3\n","         365       0.00      0.00      0.00         5\n","         366       0.50      1.00      0.67         1\n","         367       1.00      1.00      1.00         3\n","         370       1.00      1.00      1.00         4\n","         371       1.00      0.69      0.82        13\n","         373       1.00      0.29      0.44         7\n","         374       1.00      0.75      0.86         4\n","         375       1.00      0.14      0.25         7\n","         376       1.00      1.00      1.00         4\n","         377       0.00      0.00      0.00         1\n","         378       0.00      0.00      0.00         1\n","         379       0.57      1.00      0.73         4\n","         381       1.00      0.10      0.18        10\n","         382       0.00      0.00      0.00         7\n","         383       0.00      0.00      0.00         5\n","         384       0.50      0.67      0.57         6\n","         385       1.00      0.33      0.50         3\n","         386       0.56      1.00      0.71         5\n","         387       0.77      1.00      0.87        10\n","         388       0.33      1.00      0.50         1\n","         389       0.11      0.50      0.18         8\n","         390       0.00      0.00      0.00         3\n","         391       0.71      1.00      0.83         5\n","         392       1.00      0.40      0.57         5\n","         393       0.88      1.00      0.93         7\n","         394       1.00      0.33      0.50         3\n","         395       0.00      0.00      0.00         6\n","         396       1.00      1.00      1.00         1\n","         397       0.73      1.00      0.85        11\n","         398       0.46      1.00      0.63         6\n","         399       1.00      1.00      1.00         9\n","         400       1.00      1.00      1.00         5\n","         401       0.67      0.25      0.36         8\n","         402       1.00      1.00      1.00         1\n","         403       1.00      0.80      0.89         5\n","         404       1.00      0.62      0.77         8\n","         405       0.67      1.00      0.80         4\n","         406       0.67      1.00      0.80        10\n","         407       1.00      0.80      0.89         5\n","         408       1.00      0.50      0.67         8\n","         409       1.00      0.50      0.67         8\n","         410       0.83      1.00      0.91         5\n","         411       0.67      1.00      0.80         2\n","         412       1.00      0.60      0.75         5\n","         413       1.00      0.71      0.83         7\n","         414       1.00      0.44      0.62         9\n","         415       0.00      0.00      0.00        11\n","         419       0.75      0.75      0.75         4\n","         421       0.40      0.67      0.50         3\n","         422       1.00      0.62      0.77         8\n","         423       1.00      0.12      0.22         8\n","         424       0.80      1.00      0.89         4\n","         425       0.00      0.00      0.00         1\n","         426       0.80      1.00      0.89         4\n","         427       0.50      0.20      0.29        10\n","         428       0.00      0.00      0.00         4\n","         429       0.50      1.00      0.67         1\n","         430       0.75      1.00      0.86         3\n","         431       0.67      0.89      0.76         9\n","         432       0.00      0.00      0.00         1\n","         433       1.00      1.00      1.00         8\n","         434       1.00      0.60      0.75         5\n","         436       1.00      0.20      0.33         5\n","         437       0.00      0.00      0.00         6\n","         438       1.00      0.25      0.40        12\n","         439       0.62      0.62      0.62         8\n","         441       1.00      1.00      1.00         5\n","         443       1.00      0.40      0.57         5\n","         444       1.00      0.56      0.71         9\n","         445       1.00      0.14      0.25         7\n","         446       0.75      1.00      0.86         6\n","         447       0.31      1.00      0.47         4\n","         448       0.00      0.00      0.00         1\n","         449       0.80      0.44      0.57         9\n","         450       1.00      0.50      0.67         4\n","         451       0.86      0.75      0.80         8\n","         452       0.70      1.00      0.82         7\n","         453       1.00      0.50      0.67         2\n","         455       0.00      0.00      0.00         1\n","         456       0.00      0.00      0.00         0\n","         457       0.67      1.00      0.80         6\n","         458       0.42      1.00      0.59         5\n","         459       0.44      0.57      0.50         7\n","         461       0.40      1.00      0.57         6\n","         464       0.00      0.00      0.00         1\n","         465       0.62      1.00      0.77         5\n","         467       1.00      0.57      0.73         7\n","         468       0.78      1.00      0.88         7\n","         469       1.00      0.25      0.40         4\n","         470       0.67      1.00      0.80         4\n","         471       1.00      0.20      0.33         5\n","         472       0.72      1.00      0.84        13\n","         473       0.75      0.75      0.75         4\n","         474       0.88      1.00      0.93         7\n","         475       1.00      1.00      1.00         1\n","         476       0.80      0.57      0.67         7\n","         479       1.00      1.00      1.00         6\n","         481       1.00      1.00      1.00         1\n","         482       0.00      0.00      0.00         2\n","         483       0.71      1.00      0.83         5\n","         484       0.38      0.55      0.44        11\n","         487       0.00      0.00      0.00         5\n","         488       1.00      0.31      0.47        13\n","         489       1.00      1.00      1.00         2\n","         490       0.80      0.50      0.62         8\n","         491       1.00      0.14      0.25         7\n","         492       0.50      1.00      0.67         6\n","         494       1.00      0.57      0.73         7\n","         495       0.70      1.00      0.82         7\n","         496       0.83      0.42      0.56        12\n","         497       0.75      1.00      0.86         6\n","         498       0.00      0.00      0.00         4\n","         499       0.75      0.43      0.55         7\n","         500       0.67      1.00      0.80         8\n","         503       0.67      1.00      0.80         4\n","         504       1.00      1.00      1.00         3\n","         505       1.00      1.00      1.00         8\n","         506       0.88      1.00      0.93         7\n","         507       1.00      0.75      0.86         4\n","         508       1.00      0.45      0.62        11\n","         509       1.00      0.38      0.55         8\n","         510       0.50      1.00      0.67         1\n","         511       1.00      1.00      1.00         5\n","         512       0.36      1.00      0.53         4\n","         513       0.50      0.50      0.50         2\n","         514       1.00      1.00      1.00         3\n","         515       0.00      0.00      0.00         1\n","         516       0.75      1.00      0.86         3\n","         518       0.50      1.00      0.67         1\n","         520       1.00      0.58      0.74        12\n","         521       0.00      0.00      0.00         2\n","         522       1.00      0.14      0.25         7\n","\n","    accuracy                           0.63      2368\n","   macro avg       0.63      0.62      0.57      2368\n","weighted avg       0.70      0.63      0.60      2368\n","\n","Model and LabelEncoder for product saved in './model_product'\n"]}]},{"cell_type":"markdown","source":["# Generate predictions on the test data and print the predictions DataFrame\n","Here, we load the test dataset, use the trained model to generate predictions, and display the results.\n"],"metadata":{"id":"HLUojbN91Oon"}},{"cell_type":"code","source":["# Import necessary libraries\n","import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Load the test data for predictions (CSV containing validation data)\n","test_path = '/content/drive/MyDrive/Data/validation_data/incidents.csv'\n","test_df = pd.read_csv(test_path, index_col=0)\n","\n","# Define the predict function\n","def predict(texts, model_base_path, target):\n","    # Load the tokenizer for the specified pre-trained model\n","    tokenizer = AutoTokenizer.from_pretrained(model_base_path)\n","\n","    # Load the correct label encoder for the given target\n","    label_encoder_path = f'{model_base_path}/{target}_label_encoder.npy'\n","    label_encoder = LabelEncoder()\n","\n","    # Check if the label encoder file exists and load it\n","    if os.path.exists(label_encoder_path):\n","        label_encoder.classes_ = np.load(label_encoder_path, allow_pickle=True)\n","    else:\n","        # Print a warning if the label encoder is not found\n","        print(f\"Warning: Label encoder not found for {target} at {label_encoder_path}\")\n","        return None\n","\n","    # Load the pre-trained model for sequence classification\n","    model = AutoModelForSequenceClassification.from_pretrained(model_base_path).to(device)\n","\n","    # Tokenize the input texts\n","    inputs = tokenizer(\n","        texts,\n","        padding=True,  # Pad sequences to the max length\n","        truncation=True,  # Truncate sequences to the max length\n","        max_length=512,  # Limit sequence length to 512 tokens\n","        return_tensors=\"pt\"  # Return PyTorch tensors\n","    ).to(device)\n","\n","    # Put the model in evaluation mode\n","    model.eval()\n","\n","    # Make predictions with no gradient calculation\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)  # Get the predicted class for each input\n","\n","    # Decode the predictions using the label encoder\n","    decoded_predictions = label_encoder.inverse_transform(predictions.cpu().numpy())\n","\n","    # Return the decoded predictions\n","    return decoded_predictions\n","\n","# Define device for model prediction (use GPU if available, else use CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Prepare an empty dataframe to store the predictions\n","predictions = pd.DataFrame()\n","\n","# Run predictions for all targets using the correct saved model\n","for column in targets_subtask1 + targets_subtask2:\n","    # Define the model path dynamically based on the target column\n","    model_path = f'./model_{column}'\n","\n","    # Get the decoded predictions for the current target\n","    decoded_preds = predict(test_df['text'].tolist(), model_path, column)\n","\n","    # If predictions were successfully made, store them in the dataframe\n","    if decoded_preds is not None:\n","        predictions[column] = decoded_preds\n","\n","# Display the final predictions\n","print(\"\\nFinal Predictions:\\n\")\n","print(predictions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1asDnwc-1QIb","outputId":"84bd8044-4335-4f43-b7d0-ba198fb7cc12","executionInfo":{"status":"ok","timestamp":1732396551344,"user_tz":-120,"elapsed":14668,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","Final Predictions:\n","\n","    hazard-category                                   product-category  \\\n","0        biological                       meat, egg and dairy products   \n","1        biological                       meat, egg and dairy products   \n","2        biological                       meat, egg and dairy products   \n","3         allergens                                  ices and desserts   \n","4    foreign bodies                         prepared dishes and snacks   \n","..              ...                                                ...   \n","560       allergens                        cereals and bakery products   \n","561       allergens  dietetic foods, food supplements, fortified foods   \n","562  foreign bodies                        cereals and bakery products   \n","563       allergens                        cereals and bakery products   \n","564       allergens                                      confectionery   \n","\n","                           hazard                      product  \n","0                    listeria spp  thermal processed beef meat  \n","1                escherichia coli                  ground beef  \n","2                   enteroviruses                   chia seeds  \n","3                       pecan nut                    ice cream  \n","4                    listeria spp                 tomato sauce  \n","..                            ...                          ...  \n","560     milk and products thereof                        cakes  \n","561     milk and products thereof                    lollipops  \n","562              plastic fragment                        cakes  \n","563  peanuts and products thereof                    ice cream  \n","564                        almond                    ice cream  \n","\n","[565 rows x 4 columns]\n"]}]},{"cell_type":"code","source":["predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"v6gfxEZFM2Kq","outputId":"a8635d4b-5545-432a-8b1d-6ac5a10e318c","executionInfo":{"status":"ok","timestamp":1732396551344,"user_tz":-120,"elapsed":7,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    hazard-category                                   product-category  \\\n","0        biological                       meat, egg and dairy products   \n","1        biological                       meat, egg and dairy products   \n","2        biological                       meat, egg and dairy products   \n","3         allergens                                  ices and desserts   \n","4    foreign bodies                         prepared dishes and snacks   \n","..              ...                                                ...   \n","560       allergens                        cereals and bakery products   \n","561       allergens  dietetic foods, food supplements, fortified foods   \n","562  foreign bodies                        cereals and bakery products   \n","563       allergens                        cereals and bakery products   \n","564       allergens                                      confectionery   \n","\n","                           hazard                      product  \n","0                    listeria spp  thermal processed beef meat  \n","1                escherichia coli                  ground beef  \n","2                   enteroviruses                   chia seeds  \n","3                       pecan nut                    ice cream  \n","4                    listeria spp                 tomato sauce  \n","..                            ...                          ...  \n","560     milk and products thereof                        cakes  \n","561     milk and products thereof                    lollipops  \n","562              plastic fragment                        cakes  \n","563  peanuts and products thereof                    ice cream  \n","564                        almond                    ice cream  \n","\n","[565 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-eea353cc-ffec-4821-8f6f-960366237ef4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hazard-category</th>\n","      <th>product-category</th>\n","      <th>hazard</th>\n","      <th>product</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>biological</td>\n","      <td>meat, egg and dairy products</td>\n","      <td>listeria spp</td>\n","      <td>thermal processed beef meat</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>biological</td>\n","      <td>meat, egg and dairy products</td>\n","      <td>escherichia coli</td>\n","      <td>ground beef</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>biological</td>\n","      <td>meat, egg and dairy products</td>\n","      <td>enteroviruses</td>\n","      <td>chia seeds</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>allergens</td>\n","      <td>ices and desserts</td>\n","      <td>pecan nut</td>\n","      <td>ice cream</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>foreign bodies</td>\n","      <td>prepared dishes and snacks</td>\n","      <td>listeria spp</td>\n","      <td>tomato sauce</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>560</th>\n","      <td>allergens</td>\n","      <td>cereals and bakery products</td>\n","      <td>milk and products thereof</td>\n","      <td>cakes</td>\n","    </tr>\n","    <tr>\n","      <th>561</th>\n","      <td>allergens</td>\n","      <td>dietetic foods, food supplements, fortified foods</td>\n","      <td>milk and products thereof</td>\n","      <td>lollipops</td>\n","    </tr>\n","    <tr>\n","      <th>562</th>\n","      <td>foreign bodies</td>\n","      <td>cereals and bakery products</td>\n","      <td>plastic fragment</td>\n","      <td>cakes</td>\n","    </tr>\n","    <tr>\n","      <th>563</th>\n","      <td>allergens</td>\n","      <td>cereals and bakery products</td>\n","      <td>peanuts and products thereof</td>\n","      <td>ice cream</td>\n","    </tr>\n","    <tr>\n","      <th>564</th>\n","      <td>allergens</td>\n","      <td>confectionery</td>\n","      <td>almond</td>\n","      <td>ice cream</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>565 rows × 4 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eea353cc-ffec-4821-8f6f-960366237ef4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-eea353cc-ffec-4821-8f6f-960366237ef4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-eea353cc-ffec-4821-8f6f-960366237ef4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9bed80e4-132f-4449-9eb9-70023c54e004\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bed80e4-132f-4449-9eb9-70023c54e004')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9bed80e4-132f-4449-9eb9-70023c54e004 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_c109c892-6e6e-49b4-9b08-17d5e864e761\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('predictions')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_c109c892-6e6e-49b4-9b08-17d5e864e761 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('predictions');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"predictions","summary":"{\n  \"name\": \"predictions\",\n  \"rows\": 565,\n  \"fields\": [\n    {\n      \"column\": \"hazard-category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"allergens\",\n          \"fraud\",\n          \"biological\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product-category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"meat, egg and dairy products\",\n          \"ices and desserts\",\n          \"seafood\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hazard\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 72,\n        \"samples\": [\n          \"plastic fragment\",\n          \"allergens\",\n          \"walnut\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 78,\n        \"samples\": [\n          \"cbd oil\",\n          \"thermal processed beef meat\",\n          \"jellies\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["# Create the submission folder and archive the results\n","Finally, predictions and models are saved into a submission directory for easy sharing or evaluation.\n"],"metadata":{"id":"z5p7qVnA0Gc4"}},{"cell_type":"code","source":["import os\n","from shutil import make_archive\n","import pandas as pd\n","from google.colab import drive\n","\n","# Define the Google Drive path where you want to save the files\n","output_folder = '/content/drive/MyDrive/submission_augmented_train_set_finetunedPUBMEDBERT/'\n","\n","# Create the folder in Google Drive if it doesn't exist\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Save predictions to a CSV file named 'submission.csv' inside the folder\n","predictions.to_csv(f'{output_folder}submission.csv', index=False)\n","\n","# Zip the folder for submission\n","make_archive(output_folder, 'zip', output_folder)\n","\n","# Print confirmation message\n","print(f\"Submission saved to Google Drive at {output_folder}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"REr-ySES0FHE","outputId":"2ace4d04-826d-434b-a944-4e4087bd69c9","executionInfo":{"status":"ok","timestamp":1732396551845,"user_tz":-120,"elapsed":505,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Submission saved to Google Drive at /content/drive/MyDrive/submission_augmented_train_set_finetunedPUBMEDBERT/\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"88VTsOGTdMiW","executionInfo":{"status":"ok","timestamp":1732396551845,"user_tz":-120,"elapsed":2,"user":{"displayName":"Stelios Giagkos","userId":"06441279652746263838"}}},"execution_count":15,"outputs":[]}]}